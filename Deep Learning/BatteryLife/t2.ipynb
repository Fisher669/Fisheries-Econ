{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型效果\n",
    "深度不够\n",
    "## 修改模型为减少卷积层数量\n",
    "r2_train: 0.780303955078125\n",
    "r2_val: 0.757173478603363\n",
    "r2_a: 0.7901904334717177\n",
    "r2_b: 0.38925433189972136\n",
    "\n",
    "对比（非lstm架构）\n",
    "r2_train: 0.9947811961174011\n",
    "r2_val: 0.9749422669410706\n",
    "r2_a: 0.9464100454957816\n",
    "r2_b: 0.6836000724611211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 修改模型需要改层数，回传部分，以及lstm部分\n",
    "'''Define the CNN model''' \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(8, 16, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(32, 32, kernel_size=(2, 2), stride=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(32, 16, batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(4032, 100)\n",
    "        self.drop_layer6 = torch.nn.Dropout(p=0.2)\n",
    "        self.fc2 = torch.nn.Linear(100, 1)   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            x = self.layers(x) \n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.drop_layer6(x)                     \n",
    "            x = torch.sigmoid(self.fc2(x))        \n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during forward pass: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (12): Dropout(p=0.2, inplace=False)\n",
      "    (13): Conv2d(32, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (14): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (16): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(32, 16, batch_first=True)\n",
      "  (fc1): Linear(in_features=4032, out_features=100, bias=True)\n",
      "  (drop_layer6): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 11, 24]          20,008\n",
      "       BatchNorm2d-2            [-1, 8, 11, 24]              16\n",
      "         LeakyReLU-3            [-1, 8, 11, 24]               0\n",
      "         MaxPool2d-4            [-1, 8, 10, 23]               0\n",
      "           Dropout-5            [-1, 8, 10, 23]               0\n",
      "            Conv2d-6            [-1, 16, 9, 21]             784\n",
      "       BatchNorm2d-7            [-1, 16, 9, 21]              32\n",
      "         LeakyReLU-8            [-1, 16, 9, 21]               0\n",
      "           Dropout-9            [-1, 16, 9, 21]               0\n",
      "           Conv2d-10            [-1, 32, 8, 19]           3,104\n",
      "      BatchNorm2d-11            [-1, 32, 8, 19]              64\n",
      "        LeakyReLU-12            [-1, 32, 8, 19]               0\n",
      "          Dropout-13            [-1, 32, 8, 19]               0\n",
      "           Conv2d-14            [-1, 32, 7, 18]           4,128\n",
      "      BatchNorm2d-15            [-1, 32, 7, 18]              64\n",
      "        LeakyReLU-16            [-1, 32, 7, 18]               0\n",
      "          Dropout-17            [-1, 32, 7, 18]               0\n",
      "           Linear-18                  [-1, 100]         403,300\n",
      "          Dropout-19                  [-1, 100]               0\n",
      "           Linear-20                    [-1, 1]             101\n",
      "================================================================\n",
      "Total params: 431,601\n",
      "Trainable params: 431,601\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 0.44\n",
      "Params size (MB): 1.65\n",
      "Estimated Total Size (MB): 2.47\n",
      "----------------------------------------------------------------\n",
      "Step = 0 train_loss: 0.09159426 val_loss: 0.06969446\n",
      "Step = 1 train_loss: 0.08699191 val_loss: 0.06960723\n",
      "Step = 2 train_loss: 0.076461904 val_loss: 0.06931349\n",
      "Step = 3 train_loss: 0.076072976 val_loss: 0.069147974\n",
      "Step = 4 train_loss: 0.07604179 val_loss: 0.06885767\n",
      "Step = 5 train_loss: 0.06142511 val_loss: 0.06856875\n",
      "Step = 6 train_loss: 0.062461633 val_loss: 0.06837056\n",
      "Step = 7 train_loss: 0.065061636 val_loss: 0.06804004\n",
      "Step = 8 train_loss: 0.0656126 val_loss: 0.06765617\n",
      "Step = 9 train_loss: 0.06014623 val_loss: 0.06724825\n",
      "Step = 10 train_loss: 0.057835817 val_loss: 0.06670608\n",
      "Step = 11 train_loss: 0.047915515 val_loss: 0.06610385\n",
      "Step = 12 train_loss: 0.052338816 val_loss: 0.06538975\n",
      "Step = 13 train_loss: 0.050764766 val_loss: 0.06443179\n",
      "Step = 14 train_loss: 0.044790562 val_loss: 0.06330486\n",
      "Step = 15 train_loss: 0.053988393 val_loss: 0.062042393\n",
      "Step = 16 train_loss: 0.047163077 val_loss: 0.060849898\n",
      "Step = 17 train_loss: 0.037552733 val_loss: 0.05967392\n",
      "Step = 18 train_loss: 0.048690338 val_loss: 0.058415238\n",
      "Step = 19 train_loss: 0.038234364 val_loss: 0.057086613\n",
      "Step = 20 train_loss: 0.034235336 val_loss: 0.055749677\n",
      "Step = 21 train_loss: 0.037260868 val_loss: 0.05439606\n",
      "Step = 22 train_loss: 0.035677996 val_loss: 0.053076807\n",
      "Step = 23 train_loss: 0.034004234 val_loss: 0.051692918\n",
      "Step = 24 train_loss: 0.035153683 val_loss: 0.05025003\n",
      "Step = 25 train_loss: 0.034561582 val_loss: 0.048800062\n",
      "Step = 26 train_loss: 0.03715265 val_loss: 0.04740924\n",
      "Step = 27 train_loss: 0.033280168 val_loss: 0.046055585\n",
      "Step = 28 train_loss: 0.027174313 val_loss: 0.04462832\n",
      "Step = 29 train_loss: 0.027964888 val_loss: 0.043334465\n",
      "Step = 30 train_loss: 0.03493603 val_loss: 0.04202749\n",
      "Step = 31 train_loss: 0.03269271 val_loss: 0.04080536\n",
      "Step = 32 train_loss: 0.0326212 val_loss: 0.03968379\n",
      "Step = 33 train_loss: 0.026539432 val_loss: 0.038796358\n",
      "Step = 34 train_loss: 0.03372072 val_loss: 0.03794517\n",
      "Step = 35 train_loss: 0.029935915 val_loss: 0.037257634\n",
      "Step = 36 train_loss: 0.023112655 val_loss: 0.03672817\n",
      "Step = 37 train_loss: 0.030226197 val_loss: 0.03623354\n",
      "Step = 38 train_loss: 0.033490144 val_loss: 0.035691183\n",
      "Step = 39 train_loss: 0.030616648 val_loss: 0.035152376\n",
      "Step = 40 train_loss: 0.030382548 val_loss: 0.03464152\n",
      "Step = 41 train_loss: 0.027625771 val_loss: 0.034217935\n",
      "Step = 42 train_loss: 0.027631702 val_loss: 0.033864923\n",
      "Step = 43 train_loss: 0.029459072 val_loss: 0.03351452\n",
      "Step = 44 train_loss: 0.029576587 val_loss: 0.03314256\n",
      "Step = 45 train_loss: 0.03718504 val_loss: 0.03286644\n",
      "Step = 46 train_loss: 0.035492145 val_loss: 0.032649048\n",
      "Step = 47 train_loss: 0.034295157 val_loss: 0.032453544\n",
      "Step = 48 train_loss: 0.034543823 val_loss: 0.03227864\n",
      "Step = 49 train_loss: 0.02740357 val_loss: 0.032175843\n",
      "Step = 50 train_loss: 0.03089735 val_loss: 0.032086793\n",
      "Step = 51 train_loss: 0.032823678 val_loss: 0.03201596\n",
      "Step = 52 train_loss: 0.027953217 val_loss: 0.031961665\n",
      "Step = 53 train_loss: 0.03298744 val_loss: 0.03191514\n",
      "Step = 54 train_loss: 0.03361132 val_loss: 0.031768028\n",
      "Step = 55 train_loss: 0.03249727 val_loss: 0.031680353\n",
      "Step = 56 train_loss: 0.030773338 val_loss: 0.03175873\n",
      "Step = 57 train_loss: 0.030867487 val_loss: 0.031832512\n",
      "Step = 58 train_loss: 0.03449835 val_loss: 0.031888112\n",
      "Step = 59 train_loss: 0.032380395 val_loss: 0.03196655\n",
      "Step = 60 train_loss: 0.028447282 val_loss: 0.03201517\n",
      "Step = 61 train_loss: 0.027323384 val_loss: 0.031964872\n",
      "Step = 62 train_loss: 0.02561727 val_loss: 0.03180773\n",
      "Step = 63 train_loss: 0.03058937 val_loss: 0.031701162\n",
      "Step = 64 train_loss: 0.035696384 val_loss: 0.03152695\n",
      "Step = 65 train_loss: 0.029744102 val_loss: 0.03139577\n",
      "Step = 66 train_loss: 0.02957714 val_loss: 0.031186514\n",
      "Step = 67 train_loss: 0.03265811 val_loss: 0.030977145\n",
      "Step = 68 train_loss: 0.028280016 val_loss: 0.030752353\n",
      "Step = 69 train_loss: 0.029237986 val_loss: 0.030472174\n",
      "Step = 70 train_loss: 0.027956516 val_loss: 0.030317284\n",
      "Step = 71 train_loss: 0.030676894 val_loss: 0.030179225\n",
      "Step = 72 train_loss: 0.029185565 val_loss: 0.030024337\n",
      "Step = 73 train_loss: 0.027994104 val_loss: 0.029834975\n",
      "Step = 74 train_loss: 0.02891355 val_loss: 0.029512275\n",
      "Step = 75 train_loss: 0.027815295 val_loss: 0.029219592\n",
      "Step = 76 train_loss: 0.028651047 val_loss: 0.028863497\n",
      "Step = 77 train_loss: 0.029504169 val_loss: 0.028555993\n",
      "Step = 78 train_loss: 0.03163864 val_loss: 0.02807317\n",
      "Step = 79 train_loss: 0.032317992 val_loss: 0.027172603\n",
      "Step = 80 train_loss: 0.029951494 val_loss: 0.026684579\n",
      "Step = 81 train_loss: 0.026408305 val_loss: 0.025979893\n",
      "Step = 82 train_loss: 0.032829106 val_loss: 0.024996793\n",
      "Step = 83 train_loss: 0.030429047 val_loss: 0.02461393\n",
      "Step = 84 train_loss: 0.030404089 val_loss: 0.02350938\n",
      "Step = 85 train_loss: 0.03511608 val_loss: 0.022398302\n",
      "Step = 86 train_loss: 0.03304863 val_loss: 0.021650199\n",
      "Step = 87 train_loss: 0.03398194 val_loss: 0.021369651\n",
      "Step = 88 train_loss: 0.029313821 val_loss: 0.021670152\n",
      "Step = 89 train_loss: 0.030442178 val_loss: 0.02155549\n",
      "Step = 90 train_loss: 0.02939994 val_loss: 0.021946345\n",
      "Step = 91 train_loss: 0.028329514 val_loss: 0.022602629\n",
      "Step = 92 train_loss: 0.032837622 val_loss: 0.02297231\n",
      "Step = 93 train_loss: 0.024878753 val_loss: 0.024004813\n",
      "Step = 94 train_loss: 0.03108761 val_loss: 0.024681274\n",
      "Step = 95 train_loss: 0.036240004 val_loss: 0.02466835\n",
      "Step = 96 train_loss: 0.030236311 val_loss: 0.0247213\n",
      "Step = 97 train_loss: 0.027319567 val_loss: 0.024732305\n",
      "Step = 98 train_loss: 0.029458148 val_loss: 0.02468927\n",
      "Step = 99 train_loss: 0.030000959 val_loss: 0.024720613\n",
      "Step = 100 train_loss: 0.029360764 val_loss: 0.024719689\n",
      "Step = 101 train_loss: 0.02961575 val_loss: 0.024797093\n",
      "Step = 102 train_loss: 0.03370215 val_loss: 0.025293836\n",
      "Step = 103 train_loss: 0.0356537 val_loss: 0.02565032\n",
      "Step = 104 train_loss: 0.029396823 val_loss: 0.024698371\n",
      "Step = 105 train_loss: 0.030444432 val_loss: 0.02396444\n",
      "Step = 106 train_loss: 0.027160497 val_loss: 0.023853801\n",
      "Step = 107 train_loss: 0.03011054 val_loss: 0.023241946\n",
      "Step = 108 train_loss: 0.030983187 val_loss: 0.022692133\n",
      "Step = 109 train_loss: 0.03058718 val_loss: 0.023228565\n",
      "Step = 110 train_loss: 0.027656239 val_loss: 0.023662427\n",
      "Step = 111 train_loss: 0.024677303 val_loss: 0.023404976\n",
      "Step = 112 train_loss: 0.031656027 val_loss: 0.025085371\n",
      "Step = 113 train_loss: 0.025741357 val_loss: 0.026376808\n",
      "Step = 114 train_loss: 0.034469582 val_loss: 0.026562633\n",
      "Step = 115 train_loss: 0.033711877 val_loss: 0.02669592\n",
      "Step = 116 train_loss: 0.031446047 val_loss: 0.026835669\n",
      "Step = 117 train_loss: 0.03133053 val_loss: 0.027066678\n",
      "Step = 118 train_loss: 0.03408374 val_loss: 0.0275167\n",
      "Step = 119 train_loss: 0.023969082 val_loss: 0.028036736\n",
      "Step = 120 train_loss: 0.032137785 val_loss: 0.02854982\n",
      "Step = 121 train_loss: 0.03203586 val_loss: 0.028950589\n",
      "Step = 122 train_loss: 0.02949408 val_loss: 0.029284554\n",
      "Step = 123 train_loss: 0.029302265 val_loss: 0.029736903\n",
      "Step = 124 train_loss: 0.03419238 val_loss: 0.030064588\n",
      "Step = 125 train_loss: 0.036324564 val_loss: 0.030261206\n",
      "Step = 126 train_loss: 0.03134745 val_loss: 0.030309863\n",
      "Step = 127 train_loss: 0.030773602 val_loss: 0.030411134\n",
      "Step = 128 train_loss: 0.028333725 val_loss: 0.030617382\n",
      "Step = 129 train_loss: 0.029573051 val_loss: 0.030719118\n",
      "Step = 130 train_loss: 0.032816924 val_loss: 0.030877938\n",
      "Step = 131 train_loss: 0.028027078 val_loss: 0.031425375\n",
      "Step = 132 train_loss: 0.031493373 val_loss: 0.03205496\n",
      "Step = 133 train_loss: 0.028061617 val_loss: 0.03244098\n",
      "Step = 134 train_loss: 0.028958384 val_loss: 0.032309458\n",
      "Step = 135 train_loss: 0.03196977 val_loss: 0.03252451\n",
      "Step = 136 train_loss: 0.02957647 val_loss: 0.0326819\n",
      "Step = 137 train_loss: 0.032886054 val_loss: 0.03279935\n",
      "Step = 138 train_loss: 0.034080565 val_loss: 0.032987263\n",
      "Step = 139 train_loss: 0.027776556 val_loss: 0.033202536\n",
      "Step = 140 train_loss: 0.031377163 val_loss: 0.033312403\n",
      "Step = 141 train_loss: 0.032973144 val_loss: 0.033465955\n",
      "Step = 142 train_loss: 0.03282638 val_loss: 0.03359585\n",
      "Step = 143 train_loss: 0.03704244 val_loss: 0.03367959\n",
      "Step = 144 train_loss: 0.034144465 val_loss: 0.033793204\n",
      "Step = 145 train_loss: 0.028897928 val_loss: 0.03388489\n",
      "Step = 146 train_loss: 0.03130458 val_loss: 0.034034815\n",
      "Step = 147 train_loss: 0.030922154 val_loss: 0.034168597\n",
      "Step = 148 train_loss: 0.029585987 val_loss: 0.034316506\n",
      "Step = 149 train_loss: 0.027793126 val_loss: 0.034357846\n",
      "Step = 150 train_loss: 0.031159164 val_loss: 0.034400433\n",
      "Step = 151 train_loss: 0.027009435 val_loss: 0.034411073\n",
      "Step = 152 train_loss: 0.026910318 val_loss: 0.034419872\n",
      "Step = 153 train_loss: 0.0300288 val_loss: 0.034307167\n",
      "Step = 154 train_loss: 0.03448972 val_loss: 0.03424989\n",
      "Step = 155 train_loss: 0.03137461 val_loss: 0.034255963\n",
      "Step = 156 train_loss: 0.02868994 val_loss: 0.034306314\n",
      "Step = 157 train_loss: 0.032809082 val_loss: 0.03432554\n",
      "Step = 158 train_loss: 0.028971918 val_loss: 0.034312844\n",
      "Step = 159 train_loss: 0.03298301 val_loss: 0.034293734\n",
      "Step = 160 train_loss: 0.031155352 val_loss: 0.034310136\n",
      "Step = 161 train_loss: 0.03100668 val_loss: 0.03427914\n",
      "Step = 162 train_loss: 0.02473661 val_loss: 0.03423498\n",
      "Step = 163 train_loss: 0.031172264 val_loss: 0.034201503\n",
      "Step = 164 train_loss: 0.031810407 val_loss: 0.034259643\n",
      "Step = 165 train_loss: 0.031491905 val_loss: 0.03427959\n",
      "Step = 166 train_loss: 0.033796623 val_loss: 0.034307867\n",
      "Step = 167 train_loss: 0.027305888 val_loss: 0.034341313\n",
      "Step = 168 train_loss: 0.027092725 val_loss: 0.034280796\n",
      "Step = 169 train_loss: 0.028383907 val_loss: 0.034225382\n",
      "Step = 170 train_loss: 0.02476761 val_loss: 0.034242384\n",
      "Step = 171 train_loss: 0.030689077 val_loss: 0.034050453\n",
      "Step = 172 train_loss: 0.027555691 val_loss: 0.033617653\n",
      "Step = 173 train_loss: 0.028207298 val_loss: 0.033314355\n",
      "Step = 174 train_loss: 0.03204995 val_loss: 0.033095803\n",
      "Step = 175 train_loss: 0.026532583 val_loss: 0.032944713\n",
      "Step = 176 train_loss: 0.029401 val_loss: 0.032929037\n",
      "Step = 177 train_loss: 0.025982011 val_loss: 0.03289647\n",
      "Step = 178 train_loss: 0.022767566 val_loss: 0.03223732\n",
      "Step = 179 train_loss: 0.0327932 val_loss: 0.03183409\n",
      "Step = 180 train_loss: 0.03205742 val_loss: 0.03165731\n",
      "Step = 181 train_loss: 0.025005158 val_loss: 0.031460293\n",
      "Step = 182 train_loss: 0.027356338 val_loss: 0.031218125\n",
      "Step = 183 train_loss: 0.02801579 val_loss: 0.030944772\n",
      "Step = 184 train_loss: 0.029894266 val_loss: 0.030676976\n",
      "Step = 185 train_loss: 0.027407072 val_loss: 0.030493757\n",
      "Step = 186 train_loss: 0.0331198 val_loss: 0.030235779\n",
      "Step = 187 train_loss: 0.02856519 val_loss: 0.030078858\n",
      "Step = 188 train_loss: 0.025750099 val_loss: 0.029939651\n",
      "Step = 189 train_loss: 0.023888508 val_loss: 0.029817633\n",
      "Step = 190 train_loss: 0.02619165 val_loss: 0.029674837\n",
      "Step = 191 train_loss: 0.029205402 val_loss: 0.029555917\n",
      "Step = 192 train_loss: 0.029686041 val_loss: 0.029435406\n",
      "Step = 193 train_loss: 0.031293843 val_loss: 0.029273324\n",
      "Step = 194 train_loss: 0.029203836 val_loss: 0.029123727\n",
      "Step = 195 train_loss: 0.03215489 val_loss: 0.029029705\n",
      "Step = 196 train_loss: 0.03184036 val_loss: 0.028960444\n",
      "Step = 197 train_loss: 0.028251221 val_loss: 0.028867135\n",
      "Step = 198 train_loss: 0.027179431 val_loss: 0.028777596\n",
      "Step = 199 train_loss: 0.03043918 val_loss: 0.028657192\n",
      "Step = 200 train_loss: 0.024312992 val_loss: 0.028539408\n",
      "Step = 201 train_loss: 0.03399684 val_loss: 0.028446343\n",
      "Step = 202 train_loss: 0.028985888 val_loss: 0.028316163\n",
      "Step = 203 train_loss: 0.031316508 val_loss: 0.028230822\n",
      "Step = 204 train_loss: 0.030417584 val_loss: 0.028192962\n",
      "Step = 205 train_loss: 0.02269085 val_loss: 0.028166812\n",
      "Step = 206 train_loss: 0.03331481 val_loss: 0.028088817\n",
      "Step = 207 train_loss: 0.036684413 val_loss: 0.028025564\n",
      "Step = 208 train_loss: 0.030927716 val_loss: 0.027888106\n",
      "Step = 209 train_loss: 0.022946801 val_loss: 0.027863208\n",
      "Step = 210 train_loss: 0.031625755 val_loss: 0.027840756\n",
      "Step = 211 train_loss: 0.026605407 val_loss: 0.027810967\n",
      "Step = 212 train_loss: 0.03144103 val_loss: 0.027789919\n",
      "Step = 213 train_loss: 0.030444669 val_loss: 0.027816566\n",
      "Step = 214 train_loss: 0.02870492 val_loss: 0.027822558\n",
      "Step = 215 train_loss: 0.03245961 val_loss: 0.027893752\n",
      "Step = 216 train_loss: 0.025781961 val_loss: 0.02794074\n",
      "Step = 217 train_loss: 0.027620293 val_loss: 0.02791609\n",
      "Step = 218 train_loss: 0.026318086 val_loss: 0.027921168\n",
      "Step = 219 train_loss: 0.025854953 val_loss: 0.027965225\n",
      "Step = 220 train_loss: 0.030580528 val_loss: 0.027937936\n",
      "Step = 221 train_loss: 0.027548525 val_loss: 0.027959963\n",
      "Step = 222 train_loss: 0.029795472 val_loss: 0.02796115\n",
      "Step = 223 train_loss: 0.028108709 val_loss: 0.027924465\n",
      "Step = 224 train_loss: 0.03011575 val_loss: 0.028022999\n",
      "Step = 225 train_loss: 0.030880334 val_loss: 0.028078936\n",
      "Step = 226 train_loss: 0.034451935 val_loss: 0.028063908\n",
      "Step = 227 train_loss: 0.03400128 val_loss: 0.028089523\n",
      "Step = 228 train_loss: 0.03007277 val_loss: 0.02807948\n",
      "Step = 229 train_loss: 0.029086335 val_loss: 0.027915826\n",
      "Step = 230 train_loss: 0.030552648 val_loss: 0.027657151\n",
      "Step = 231 train_loss: 0.029037736 val_loss: 0.02707218\n",
      "Step = 232 train_loss: 0.030856632 val_loss: 0.02652204\n",
      "Step = 233 train_loss: 0.030388378 val_loss: 0.025791023\n",
      "Step = 234 train_loss: 0.02829664 val_loss: 0.02560219\n",
      "Step = 235 train_loss: 0.027357152 val_loss: 0.025325019\n",
      "Step = 236 train_loss: 0.030730644 val_loss: 0.024717115\n",
      "Step = 237 train_loss: 0.028312763 val_loss: 0.02308308\n",
      "Step = 238 train_loss: 0.03203637 val_loss: 0.020668136\n",
      "Step = 239 train_loss: 0.026933517 val_loss: 0.019845188\n",
      "Step = 240 train_loss: 0.022338223 val_loss: 0.020015575\n",
      "Step = 241 train_loss: 0.029155014 val_loss: 0.02254398\n",
      "Step = 242 train_loss: 0.032210626 val_loss: 0.024966365\n",
      "Step = 243 train_loss: 0.028524576 val_loss: 0.026270458\n",
      "Step = 244 train_loss: 0.030133046 val_loss: 0.026784867\n",
      "Step = 245 train_loss: 0.029429523 val_loss: 0.02609633\n",
      "Step = 246 train_loss: 0.03335393 val_loss: 0.024300372\n",
      "Step = 247 train_loss: 0.03056566 val_loss: 0.022562878\n",
      "Step = 248 train_loss: 0.031955905 val_loss: 0.021140564\n",
      "Step = 249 train_loss: 0.024899654 val_loss: 0.020418007\n",
      "Step = 250 train_loss: 0.035576023 val_loss: 0.020162106\n",
      "Step = 251 train_loss: 0.031204661 val_loss: 0.020342836\n",
      "Step = 252 train_loss: 0.02665924 val_loss: 0.021115731\n",
      "Step = 253 train_loss: 0.024440818 val_loss: 0.022458922\n",
      "Step = 254 train_loss: 0.032491367 val_loss: 0.023863351\n",
      "Step = 255 train_loss: 0.024204165 val_loss: 0.025183575\n",
      "Step = 256 train_loss: 0.025135847 val_loss: 0.026656084\n",
      "Step = 257 train_loss: 0.028302995 val_loss: 0.028412208\n",
      "Step = 258 train_loss: 0.030912075 val_loss: 0.030657262\n",
      "Step = 259 train_loss: 0.03176826 val_loss: 0.031634986\n",
      "Step = 260 train_loss: 0.029892858 val_loss: 0.032549404\n",
      "Step = 261 train_loss: 0.032088406 val_loss: 0.03176518\n",
      "Step = 262 train_loss: 0.033076935 val_loss: 0.034012068\n",
      "Step = 263 train_loss: 0.029180773 val_loss: 0.03581341\n",
      "Step = 264 train_loss: 0.031018518 val_loss: 0.037580594\n",
      "Step = 265 train_loss: 0.029975954 val_loss: 0.03764726\n",
      "Step = 266 train_loss: 0.034199513 val_loss: 0.03765371\n",
      "Step = 267 train_loss: 0.026449958 val_loss: 0.037478372\n",
      "Step = 268 train_loss: 0.025415966 val_loss: 0.036974277\n",
      "Step = 269 train_loss: 0.032665785 val_loss: 0.03627885\n",
      "Step = 270 train_loss: 0.030496042 val_loss: 0.035565935\n",
      "Step = 271 train_loss: 0.028734447 val_loss: 0.035007257\n",
      "Step = 272 train_loss: 0.029671397 val_loss: 0.03464734\n",
      "Step = 273 train_loss: 0.030055724 val_loss: 0.034399226\n",
      "Step = 274 train_loss: 0.032094356 val_loss: 0.03385111\n",
      "Step = 275 train_loss: 0.030281126 val_loss: 0.03317334\n",
      "Step = 276 train_loss: 0.027341368 val_loss: 0.032684483\n",
      "Step = 277 train_loss: 0.029494762 val_loss: 0.032397702\n",
      "Step = 278 train_loss: 0.027633995 val_loss: 0.032093108\n",
      "Step = 279 train_loss: 0.03139104 val_loss: 0.031867027\n",
      "Step = 280 train_loss: 0.03049654 val_loss: 0.031681195\n",
      "Step = 281 train_loss: 0.031020442 val_loss: 0.031404547\n",
      "Step = 282 train_loss: 0.02776544 val_loss: 0.031131532\n",
      "Step = 283 train_loss: 0.033200517 val_loss: 0.030861866\n",
      "Step = 284 train_loss: 0.030201137 val_loss: 0.030591223\n",
      "Step = 285 train_loss: 0.029492468 val_loss: 0.0304795\n",
      "Step = 286 train_loss: 0.031161 val_loss: 0.030279791\n",
      "Step = 287 train_loss: 0.030147882 val_loss: 0.030267103\n",
      "Step = 288 train_loss: 0.031226754 val_loss: 0.030202849\n",
      "Step = 289 train_loss: 0.027571393 val_loss: 0.030127339\n",
      "Step = 290 train_loss: 0.03132038 val_loss: 0.030039186\n",
      "Step = 291 train_loss: 0.029554682 val_loss: 0.029958602\n",
      "Step = 292 train_loss: 0.0278428 val_loss: 0.029828226\n",
      "Step = 293 train_loss: 0.030074332 val_loss: 0.029797\n",
      "Step = 294 train_loss: 0.028509982 val_loss: 0.029750975\n",
      "Step = 295 train_loss: 0.028609289 val_loss: 0.029639017\n",
      "Step = 296 train_loss: 0.032412484 val_loss: 0.029598594\n",
      "Step = 297 train_loss: 0.030453406 val_loss: 0.029484624\n",
      "Step = 298 train_loss: 0.02783042 val_loss: 0.02934969\n",
      "Step = 299 train_loss: 0.028816165 val_loss: 0.029301487\n",
      "Step = 300 train_loss: 0.02978964 val_loss: 0.029167958\n",
      "Step = 301 train_loss: 0.031101707 val_loss: 0.029056817\n",
      "Step = 302 train_loss: 0.027825322 val_loss: 0.02891808\n",
      "Step = 303 train_loss: 0.033858318 val_loss: 0.028792439\n",
      "Step = 304 train_loss: 0.03295708 val_loss: 0.028640172\n",
      "Step = 305 train_loss: 0.030522654 val_loss: 0.028582437\n",
      "Step = 306 train_loss: 0.029122014 val_loss: 0.0285042\n",
      "Step = 307 train_loss: 0.026550505 val_loss: 0.028428499\n",
      "Step = 308 train_loss: 0.031083943 val_loss: 0.028333409\n",
      "Step = 309 train_loss: 0.028376311 val_loss: 0.02825318\n",
      "Step = 310 train_loss: 0.03381573 val_loss: 0.028183414\n",
      "Step = 311 train_loss: 0.030161658 val_loss: 0.028096644\n",
      "Step = 312 train_loss: 0.026208429 val_loss: 0.028012354\n",
      "Step = 313 train_loss: 0.029740559 val_loss: 0.02797642\n",
      "Step = 314 train_loss: 0.028591696 val_loss: 0.027933775\n",
      "Step = 315 train_loss: 0.02736617 val_loss: 0.027871335\n",
      "Step = 316 train_loss: 0.033643737 val_loss: 0.027784165\n",
      "Step = 317 train_loss: 0.03187668 val_loss: 0.027718315\n",
      "Step = 318 train_loss: 0.03009798 val_loss: 0.027677579\n",
      "Step = 319 train_loss: 0.028071048 val_loss: 0.027619446\n",
      "Step = 320 train_loss: 0.025500791 val_loss: 0.027545054\n",
      "Step = 321 train_loss: 0.028398355 val_loss: 0.027533127\n",
      "Step = 322 train_loss: 0.029406112 val_loss: 0.027487665\n",
      "Step = 323 train_loss: 0.029139183 val_loss: 0.027357018\n",
      "Step = 324 train_loss: 0.028917624 val_loss: 0.027273003\n",
      "Step = 325 train_loss: 0.028160201 val_loss: 0.02721551\n",
      "Step = 326 train_loss: 0.031099852 val_loss: 0.02717379\n",
      "Step = 327 train_loss: 0.027814325 val_loss: 0.027084162\n",
      "Step = 328 train_loss: 0.027538054 val_loss: 0.02692818\n",
      "Step = 329 train_loss: 0.029383758 val_loss: 0.026960332\n",
      "Step = 330 train_loss: 0.029625963 val_loss: 0.026993452\n",
      "Step = 331 train_loss: 0.02359484 val_loss: 0.02694968\n",
      "Step = 332 train_loss: 0.029594589 val_loss: 0.026921673\n",
      "Step = 333 train_loss: 0.02819838 val_loss: 0.02687758\n",
      "Step = 334 train_loss: 0.026258148 val_loss: 0.026869457\n",
      "Step = 335 train_loss: 0.026784046 val_loss: 0.02690191\n",
      "Step = 336 train_loss: 0.029118601 val_loss: 0.026913228\n",
      "Step = 337 train_loss: 0.028231902 val_loss: 0.026853172\n",
      "Step = 338 train_loss: 0.03353481 val_loss: 0.026736083\n",
      "Step = 339 train_loss: 0.028560968 val_loss: 0.026604556\n",
      "Step = 340 train_loss: 0.03098726 val_loss: 0.026608275\n",
      "Step = 341 train_loss: 0.028312434 val_loss: 0.026553469\n",
      "Step = 342 train_loss: 0.032635298 val_loss: 0.026535999\n",
      "Step = 343 train_loss: 0.032911386 val_loss: 0.026476108\n",
      "Step = 344 train_loss: 0.03146952 val_loss: 0.026401225\n",
      "Step = 345 train_loss: 0.025010604 val_loss: 0.026392817\n",
      "Step = 346 train_loss: 0.02900541 val_loss: 0.026313905\n",
      "Step = 347 train_loss: 0.027125927 val_loss: 0.026231905\n",
      "Step = 348 train_loss: 0.0274482 val_loss: 0.026169486\n",
      "Step = 349 train_loss: 0.031004068 val_loss: 0.026029095\n",
      "Step = 350 train_loss: 0.027287325 val_loss: 0.02589669\n",
      "Step = 351 train_loss: 0.023986202 val_loss: 0.025634823\n",
      "Step = 352 train_loss: 0.033883553 val_loss: 0.025125958\n",
      "Step = 353 train_loss: 0.028383186 val_loss: 0.024269564\n",
      "Step = 354 train_loss: 0.02587269 val_loss: 0.023295095\n",
      "Step = 355 train_loss: 0.03333449 val_loss: 0.02221874\n",
      "Step = 356 train_loss: 0.028625902 val_loss: 0.020886809\n",
      "Step = 357 train_loss: 0.031467732 val_loss: 0.019853858\n",
      "Step = 358 train_loss: 0.030455807 val_loss: 0.018993616\n",
      "Step = 359 train_loss: 0.026946327 val_loss: 0.0197363\n",
      "Step = 360 train_loss: 0.026282469 val_loss: 0.022767344\n",
      "Step = 361 train_loss: 0.025638597 val_loss: 0.031173643\n",
      "Step = 362 train_loss: 0.027507475 val_loss: 0.0387123\n",
      "Step = 363 train_loss: 0.02767674 val_loss: 0.043478206\n",
      "Step = 364 train_loss: 0.033804 val_loss: 0.04481279\n",
      "Step = 365 train_loss: 0.031964563 val_loss: 0.04319977\n",
      "Step = 366 train_loss: 0.027092686 val_loss: 0.039627686\n",
      "Step = 367 train_loss: 0.027648814 val_loss: 0.035022546\n",
      "Step = 368 train_loss: 0.029697776 val_loss: 0.030846342\n",
      "Step = 369 train_loss: 0.030583594 val_loss: 0.026950074\n",
      "Step = 370 train_loss: 0.027199544 val_loss: 0.02371373\n",
      "Step = 371 train_loss: 0.026612831 val_loss: 0.021334795\n",
      "Step = 372 train_loss: 0.02935655 val_loss: 0.020520605\n",
      "Step = 373 train_loss: 0.031265233 val_loss: 0.020930573\n",
      "Step = 374 train_loss: 0.034883942 val_loss: 0.02232383\n",
      "Step = 375 train_loss: 0.031705912 val_loss: 0.023079814\n",
      "Step = 376 train_loss: 0.02700405 val_loss: 0.024111642\n",
      "Step = 377 train_loss: 0.02986963 val_loss: 0.025094835\n",
      "Step = 378 train_loss: 0.029076198 val_loss: 0.02594114\n",
      "Step = 379 train_loss: 0.032307908 val_loss: 0.026847493\n",
      "Step = 380 train_loss: 0.033975013 val_loss: 0.027616391\n",
      "Step = 381 train_loss: 0.02830221 val_loss: 0.027400456\n",
      "Step = 382 train_loss: 0.032285478 val_loss: 0.02654398\n",
      "Step = 383 train_loss: 0.033980604 val_loss: 0.026317632\n",
      "Step = 384 train_loss: 0.032598652 val_loss: 0.027900508\n",
      "Step = 385 train_loss: 0.032778013 val_loss: 0.029210215\n",
      "Step = 386 train_loss: 0.025037246 val_loss: 0.029989397\n",
      "Step = 387 train_loss: 0.03041146 val_loss: 0.028111853\n",
      "Step = 388 train_loss: 0.02957253 val_loss: 0.026139434\n",
      "Step = 389 train_loss: 0.028433215 val_loss: 0.025891254\n",
      "Step = 390 train_loss: 0.029384114 val_loss: 0.025885515\n",
      "Step = 391 train_loss: 0.026512694 val_loss: 0.02566426\n",
      "Step = 392 train_loss: 0.030501354 val_loss: 0.02549143\n",
      "Step = 393 train_loss: 0.03028876 val_loss: 0.02574393\n",
      "Step = 394 train_loss: 0.028520346 val_loss: 0.027494421\n",
      "Step = 395 train_loss: 0.028277976 val_loss: 0.030200642\n",
      "Step = 396 train_loss: 0.0307221 val_loss: 0.032074858\n",
      "Step = 397 train_loss: 0.027108928 val_loss: 0.026690692\n",
      "Step = 398 train_loss: 0.029003521 val_loss: 0.02503381\n",
      "Step = 399 train_loss: 0.026082098 val_loss: 0.025362449\n",
      "Step = 400 train_loss: 0.027663251 val_loss: 0.025468385\n",
      "Step = 401 train_loss: 0.028253602 val_loss: 0.025519302\n",
      "Step = 402 train_loss: 0.029525029 val_loss: 0.02560535\n",
      "Step = 403 train_loss: 0.029436018 val_loss: 0.0257769\n",
      "Step = 404 train_loss: 0.03468109 val_loss: 0.025920473\n",
      "Step = 405 train_loss: 0.028681232 val_loss: 0.026043233\n",
      "Step = 406 train_loss: 0.02542156 val_loss: 0.026108235\n",
      "Step = 407 train_loss: 0.033148028 val_loss: 0.026068453\n",
      "Step = 408 train_loss: 0.026282664 val_loss: 0.025880095\n",
      "Step = 409 train_loss: 0.027669026 val_loss: 0.025688762\n",
      "Step = 410 train_loss: 0.029227935 val_loss: 0.02565301\n",
      "Step = 411 train_loss: 0.032664683 val_loss: 0.026047619\n",
      "Step = 412 train_loss: 0.027039759 val_loss: 0.027863177\n",
      "Step = 413 train_loss: 0.031061266 val_loss: 0.029280402\n",
      "Step = 414 train_loss: 0.031947892 val_loss: 0.030587211\n",
      "Step = 415 train_loss: 0.02928415 val_loss: 0.031463973\n",
      "Step = 416 train_loss: 0.028629635 val_loss: 0.031833034\n",
      "Step = 417 train_loss: 0.025003076 val_loss: 0.032360148\n",
      "Step = 418 train_loss: 0.026475124 val_loss: 0.032112807\n",
      "Step = 419 train_loss: 0.028646477 val_loss: 0.031707864\n",
      "Step = 420 train_loss: 0.025886972 val_loss: 0.034420617\n",
      "Step = 421 train_loss: 0.029830135 val_loss: 0.038323037\n",
      "Step = 422 train_loss: 0.029007766 val_loss: 0.042192016\n",
      "Step = 423 train_loss: 0.027261298 val_loss: 0.045830827\n",
      "Step = 424 train_loss: 0.03265806 val_loss: 0.046824887\n",
      "Step = 425 train_loss: 0.028409269 val_loss: 0.047111426\n",
      "Step = 426 train_loss: 0.027640961 val_loss: 0.0471965\n",
      "Step = 427 train_loss: 0.022727609 val_loss: 0.04657868\n",
      "Step = 428 train_loss: 0.028318843 val_loss: 0.043557633\n",
      "Step = 429 train_loss: 0.027381636 val_loss: 0.04291729\n",
      "Step = 430 train_loss: 0.02842358 val_loss: 0.045404468\n",
      "Step = 431 train_loss: 0.030230688 val_loss: 0.046217144\n",
      "Step = 432 train_loss: 0.02824058 val_loss: 0.045886494\n",
      "Step = 433 train_loss: 0.028585693 val_loss: 0.0449946\n",
      "Step = 434 train_loss: 0.02913765 val_loss: 0.043730423\n",
      "Step = 435 train_loss: 0.03125168 val_loss: 0.03934713\n",
      "Step = 436 train_loss: 0.031605408 val_loss: 0.029107539\n",
      "Step = 437 train_loss: 0.027832257 val_loss: 0.032148924\n",
      "Step = 438 train_loss: 0.030058276 val_loss: 0.042699277\n",
      "Step = 439 train_loss: 0.025952037 val_loss: 0.039104514\n",
      "Step = 440 train_loss: 0.028955473 val_loss: 0.039768193\n",
      "Step = 441 train_loss: 0.032279484 val_loss: 0.039507825\n",
      "Step = 442 train_loss: 0.03371099 val_loss: 0.03877493\n",
      "Step = 443 train_loss: 0.026874434 val_loss: 0.037973728\n",
      "Step = 444 train_loss: 0.02700093 val_loss: 0.03722387\n",
      "Step = 445 train_loss: 0.027625682 val_loss: 0.036572106\n",
      "Step = 446 train_loss: 0.028846761 val_loss: 0.03592656\n",
      "Step = 447 train_loss: 0.029476391 val_loss: 0.0352938\n",
      "Step = 448 train_loss: 0.027925368 val_loss: 0.03467313\n",
      "Step = 449 train_loss: 0.031513777 val_loss: 0.03401826\n",
      "Step = 450 train_loss: 0.025755297 val_loss: 0.0334252\n",
      "Step = 451 train_loss: 0.026003934 val_loss: 0.032870676\n",
      "Step = 452 train_loss: 0.032041423 val_loss: 0.032340314\n",
      "Step = 453 train_loss: 0.024936153 val_loss: 0.03189808\n",
      "Step = 454 train_loss: 0.033749327 val_loss: 0.03143659\n",
      "Step = 455 train_loss: 0.03558629 val_loss: 0.031108778\n",
      "Step = 456 train_loss: 0.024490772 val_loss: 0.030652383\n",
      "Step = 457 train_loss: 0.029838432 val_loss: 0.03029023\n",
      "Step = 458 train_loss: 0.030526964 val_loss: 0.029962074\n",
      "Step = 459 train_loss: 0.030239442 val_loss: 0.029557202\n",
      "Step = 460 train_loss: 0.03276421 val_loss: 0.02924005\n",
      "Step = 461 train_loss: 0.029819803 val_loss: 0.028912218\n",
      "Step = 462 train_loss: 0.028961847 val_loss: 0.028664108\n",
      "Step = 463 train_loss: 0.023710323 val_loss: 0.028404154\n",
      "Step = 464 train_loss: 0.028964303 val_loss: 0.02809308\n",
      "Step = 465 train_loss: 0.03281053 val_loss: 0.027781429\n",
      "Step = 466 train_loss: 0.030293973 val_loss: 0.0274851\n",
      "Step = 467 train_loss: 0.03035341 val_loss: 0.027223961\n",
      "Step = 468 train_loss: 0.030225758 val_loss: 0.026991466\n",
      "Step = 469 train_loss: 0.030896068 val_loss: 0.02668474\n",
      "Step = 470 train_loss: 0.026988357 val_loss: 0.026532153\n",
      "Step = 471 train_loss: 0.026642503 val_loss: 0.02635019\n",
      "Step = 472 train_loss: 0.028325208 val_loss: 0.02617187\n",
      "Step = 473 train_loss: 0.03094305 val_loss: 0.026076028\n",
      "Step = 474 train_loss: 0.026339922 val_loss: 0.025973132\n",
      "Step = 475 train_loss: 0.031135347 val_loss: 0.02583422\n",
      "Step = 476 train_loss: 0.029542658 val_loss: 0.025689445\n",
      "Step = 477 train_loss: 0.031668928 val_loss: 0.02551115\n",
      "Step = 478 train_loss: 0.028020965 val_loss: 0.025377138\n",
      "Step = 479 train_loss: 0.026592124 val_loss: 0.025249725\n",
      "Step = 480 train_loss: 0.034986988 val_loss: 0.02508536\n",
      "Step = 481 train_loss: 0.029793976 val_loss: 0.024973588\n",
      "Step = 482 train_loss: 0.025662798 val_loss: 0.024888236\n",
      "Step = 483 train_loss: 0.024298854 val_loss: 0.02480455\n",
      "Step = 484 train_loss: 0.026240785 val_loss: 0.024687177\n",
      "Step = 485 train_loss: 0.02654494 val_loss: 0.024566261\n",
      "Step = 486 train_loss: 0.031302646 val_loss: 0.024417888\n",
      "Step = 487 train_loss: 0.02591918 val_loss: 0.024319924\n",
      "Step = 488 train_loss: 0.031336226 val_loss: 0.024238132\n",
      "Step = 489 train_loss: 0.031113781 val_loss: 0.024181698\n",
      "Step = 490 train_loss: 0.030539393 val_loss: 0.02410442\n",
      "Step = 491 train_loss: 0.028714737 val_loss: 0.0239728\n",
      "Step = 492 train_loss: 0.029304786 val_loss: 0.023816887\n",
      "Step = 493 train_loss: 0.026609646 val_loss: 0.02368272\n",
      "Step = 494 train_loss: 0.030695476 val_loss: 0.023528935\n",
      "Step = 495 train_loss: 0.031561457 val_loss: 0.023355622\n",
      "Step = 496 train_loss: 0.026568633 val_loss: 0.023141148\n",
      "Step = 497 train_loss: 0.030239526 val_loss: 0.022967692\n",
      "Step = 498 train_loss: 0.03170925 val_loss: 0.022810478\n",
      "Step = 499 train_loss: 0.028287463 val_loss: 0.022613438\n",
      "Step = 500 train_loss: 0.028285194 val_loss: 0.022436708\n",
      "Step = 501 train_loss: 0.029237691 val_loss: 0.022252688\n",
      "Step = 502 train_loss: 0.027221145 val_loss: 0.022091819\n",
      "Step = 503 train_loss: 0.029297652 val_loss: 0.02200275\n",
      "Step = 504 train_loss: 0.023893468 val_loss: 0.0218825\n",
      "Step = 505 train_loss: 0.025736134 val_loss: 0.021763807\n",
      "Step = 506 train_loss: 0.026336368 val_loss: 0.021576304\n",
      "Step = 507 train_loss: 0.02787741 val_loss: 0.021431902\n",
      "Step = 508 train_loss: 0.028596614 val_loss: 0.021258917\n",
      "Step = 509 train_loss: 0.028720913 val_loss: 0.021074833\n",
      "Step = 510 train_loss: 0.029276662 val_loss: 0.020855315\n",
      "Step = 511 train_loss: 0.027747273 val_loss: 0.020729579\n",
      "Step = 512 train_loss: 0.031975523 val_loss: 0.020662101\n",
      "Step = 513 train_loss: 0.028718568 val_loss: 0.020727135\n",
      "Step = 514 train_loss: 0.024837153 val_loss: 0.020775646\n",
      "Step = 515 train_loss: 0.025237335 val_loss: 0.020746347\n",
      "Step = 516 train_loss: 0.028911475 val_loss: 0.020749323\n",
      "Step = 517 train_loss: 0.026885938 val_loss: 0.020712432\n",
      "Step = 518 train_loss: 0.025802553 val_loss: 0.020719705\n",
      "Step = 519 train_loss: 0.031145526 val_loss: 0.02071577\n",
      "Step = 520 train_loss: 0.028579565 val_loss: 0.02069408\n",
      "Step = 521 train_loss: 0.028212212 val_loss: 0.02066819\n",
      "Step = 522 train_loss: 0.027158737 val_loss: 0.02063582\n",
      "Step = 523 train_loss: 0.02734318 val_loss: 0.021074783\n",
      "Step = 524 train_loss: 0.028333964 val_loss: 0.02107\n",
      "Step = 525 train_loss: 0.026420884 val_loss: 0.021031734\n",
      "Step = 526 train_loss: 0.025043432 val_loss: 0.020959591\n",
      "Step = 527 train_loss: 0.028692825 val_loss: 0.02089657\n",
      "Step = 528 train_loss: 0.027561676 val_loss: 0.020825353\n",
      "Step = 529 train_loss: 0.0290484 val_loss: 0.020778254\n",
      "Step = 530 train_loss: 0.026995884 val_loss: 0.02103898\n",
      "Step = 531 train_loss: 0.025435302 val_loss: 0.02071841\n",
      "Step = 532 train_loss: 0.027779201 val_loss: 0.023511432\n",
      "Step = 533 train_loss: 0.029449003 val_loss: 0.03052556\n",
      "Step = 534 train_loss: 0.027100904 val_loss: 0.034260295\n",
      "Step = 535 train_loss: 0.026951693 val_loss: 0.036160916\n",
      "Step = 536 train_loss: 0.031960294 val_loss: 0.036820203\n",
      "Step = 537 train_loss: 0.02806626 val_loss: 0.03683704\n",
      "Step = 538 train_loss: 0.027011532 val_loss: 0.03666975\n",
      "Step = 539 train_loss: 0.028033515 val_loss: 0.03626097\n",
      "Step = 540 train_loss: 0.023476936 val_loss: 0.03555901\n",
      "Step = 541 train_loss: 0.026527813 val_loss: 0.034323394\n",
      "Step = 542 train_loss: 0.031140348 val_loss: 0.032989647\n",
      "Step = 543 train_loss: 0.026230404 val_loss: 0.031530805\n",
      "Step = 544 train_loss: 0.02613935 val_loss: 0.029851427\n",
      "Step = 545 train_loss: 0.022724012 val_loss: 0.027444921\n",
      "Step = 546 train_loss: 0.026350044 val_loss: 0.025044007\n",
      "Step = 547 train_loss: 0.026992043 val_loss: 0.023511816\n",
      "Step = 548 train_loss: 0.02879823 val_loss: 0.023330914\n",
      "Step = 549 train_loss: 0.02673143 val_loss: 0.02368271\n",
      "Step = 550 train_loss: 0.023512172 val_loss: 0.02427873\n",
      "Step = 551 train_loss: 0.025220709 val_loss: 0.024615293\n",
      "Step = 552 train_loss: 0.024017313 val_loss: 0.025060328\n",
      "Step = 553 train_loss: 0.022212526 val_loss: 0.025842585\n",
      "Step = 554 train_loss: 0.020525586 val_loss: 0.026063219\n",
      "Step = 555 train_loss: 0.02401629 val_loss: 0.025446897\n",
      "Step = 556 train_loss: 0.02207135 val_loss: 0.024074955\n",
      "Step = 557 train_loss: 0.02376367 val_loss: 0.02336049\n",
      "Step = 558 train_loss: 0.022960884 val_loss: 0.023067178\n",
      "Step = 559 train_loss: 0.026995849 val_loss: 0.023320286\n",
      "Step = 560 train_loss: 0.02204792 val_loss: 0.023562077\n",
      "Step = 561 train_loss: 0.021398904 val_loss: 0.023410253\n",
      "Step = 562 train_loss: 0.023455672 val_loss: 0.024622275\n",
      "Step = 563 train_loss: 0.020109 val_loss: 0.025427278\n",
      "Step = 564 train_loss: 0.023926172 val_loss: 0.026351396\n",
      "Step = 565 train_loss: 0.023705116 val_loss: 0.02271159\n",
      "Step = 566 train_loss: 0.01974148 val_loss: 0.02507764\n",
      "Step = 567 train_loss: 0.021321353 val_loss: 0.043269925\n",
      "Step = 568 train_loss: 0.027597807 val_loss: 0.048771095\n",
      "Step = 569 train_loss: 0.029071113 val_loss: 0.050532974\n",
      "Step = 570 train_loss: 0.026131572 val_loss: 0.049671575\n",
      "Step = 571 train_loss: 0.020534636 val_loss: 0.047385525\n",
      "Step = 572 train_loss: 0.026236432 val_loss: 0.043669477\n",
      "Step = 573 train_loss: 0.022896262 val_loss: 0.037747443\n",
      "Step = 574 train_loss: 0.026369404 val_loss: 0.027314974\n",
      "Step = 575 train_loss: 0.025020044 val_loss: 0.018413093\n",
      "Step = 576 train_loss: 0.023150833 val_loss: 0.020112654\n",
      "Step = 577 train_loss: 0.019156188 val_loss: 0.020001624\n",
      "Step = 578 train_loss: 0.027430745 val_loss: 0.020085724\n",
      "Step = 579 train_loss: 0.028425155 val_loss: 0.020086838\n",
      "Step = 580 train_loss: 0.020582166 val_loss: 0.01983766\n",
      "Step = 581 train_loss: 0.019711414 val_loss: 0.018152451\n",
      "Step = 582 train_loss: 0.022435728 val_loss: 0.019735254\n",
      "Step = 583 train_loss: 0.018667828 val_loss: 0.019764036\n",
      "Step = 584 train_loss: 0.022084562 val_loss: 0.019706592\n",
      "Step = 585 train_loss: 0.0244772 val_loss: 0.018441021\n",
      "Step = 586 train_loss: 0.020320842 val_loss: 0.022940911\n",
      "Step = 587 train_loss: 0.019702883 val_loss: 0.030303344\n",
      "Step = 588 train_loss: 0.024087168 val_loss: 0.024632974\n",
      "Step = 589 train_loss: 0.021565218 val_loss: 0.017134901\n",
      "Step = 590 train_loss: 0.020226229 val_loss: 0.019283501\n",
      "Step = 591 train_loss: 0.022677127 val_loss: 0.01945401\n",
      "Step = 592 train_loss: 0.023614835 val_loss: 0.019377716\n",
      "Step = 593 train_loss: 0.020419987 val_loss: 0.015115976\n",
      "Step = 594 train_loss: 0.019893784 val_loss: 0.025312267\n",
      "Step = 595 train_loss: 0.023615416 val_loss: 0.020190489\n",
      "Step = 596 train_loss: 0.022666842 val_loss: 0.017394215\n",
      "Step = 597 train_loss: 0.021241078 val_loss: 0.019455256\n",
      "Step = 598 train_loss: 0.018026551 val_loss: 0.019388812\n",
      "Step = 599 train_loss: 0.017540194 val_loss: 0.014442535\n",
      "Step = 600 train_loss: 0.021274898 val_loss: 0.023959415\n",
      "Step = 601 train_loss: 0.01840293 val_loss: 0.02394667\n",
      "Step = 602 train_loss: 0.023092775 val_loss: 0.016695237\n",
      "Step = 603 train_loss: 0.020046266 val_loss: 0.019622562\n",
      "Step = 604 train_loss: 0.02210729 val_loss: 0.018266592\n",
      "Step = 605 train_loss: 0.021920225 val_loss: 0.014141819\n",
      "Step = 606 train_loss: 0.020868478 val_loss: 0.015802318\n",
      "Step = 607 train_loss: 0.01664749 val_loss: 0.018267993\n",
      "Step = 608 train_loss: 0.0154292565 val_loss: 0.019349499\n",
      "Step = 609 train_loss: 0.018008647 val_loss: 0.015675284\n",
      "Step = 610 train_loss: 0.018166011 val_loss: 0.015669856\n",
      "Step = 611 train_loss: 0.018914212 val_loss: 0.015846064\n",
      "Step = 612 train_loss: 0.018755473 val_loss: 0.018259943\n",
      "Step = 613 train_loss: 0.015315254 val_loss: 0.020608902\n",
      "Step = 614 train_loss: 0.015606203 val_loss: 0.01778181\n",
      "Step = 615 train_loss: 0.01936537 val_loss: 0.018290523\n",
      "Step = 616 train_loss: 0.019614823 val_loss: 0.018477555\n",
      "Step = 617 train_loss: 0.018609151 val_loss: 0.021332536\n",
      "Step = 618 train_loss: 0.017310062 val_loss: 0.027123613\n",
      "Step = 619 train_loss: 0.015726767 val_loss: 0.031550594\n",
      "Step = 620 train_loss: 0.017614413 val_loss: 0.009783278\n",
      "Step = 621 train_loss: 0.0138046425 val_loss: 0.012499871\n",
      "Step = 622 train_loss: 0.017515061 val_loss: 0.026356958\n",
      "Step = 623 train_loss: 0.013470009 val_loss: 0.029565614\n",
      "Step = 624 train_loss: 0.018290205 val_loss: 0.028039979\n",
      "Step = 625 train_loss: 0.016880115 val_loss: 0.01767254\n",
      "Step = 626 train_loss: 0.01582366 val_loss: 0.018742876\n",
      "Step = 627 train_loss: 0.017867038 val_loss: 0.04826754\n",
      "Step = 628 train_loss: 0.019372936 val_loss: 0.05419553\n",
      "Step = 629 train_loss: 0.0154685415 val_loss: 0.052066837\n",
      "Step = 630 train_loss: 0.017314842 val_loss: 0.041374292\n",
      "Step = 631 train_loss: 0.015579545 val_loss: 0.027583817\n",
      "Step = 632 train_loss: 0.019180205 val_loss: 0.023189034\n",
      "Step = 633 train_loss: 0.018850842 val_loss: 0.025602262\n",
      "Step = 634 train_loss: 0.016632771 val_loss: 0.035238255\n",
      "Step = 635 train_loss: 0.019057725 val_loss: 0.034636863\n",
      "Step = 636 train_loss: 0.01694036 val_loss: 0.029978229\n",
      "Step = 637 train_loss: 0.020906823 val_loss: 0.020667523\n",
      "Step = 638 train_loss: 0.017329462 val_loss: 0.009830596\n",
      "Step = 639 train_loss: 0.0154445255 val_loss: 0.020844888\n",
      "Step = 640 train_loss: 0.017529145 val_loss: 0.021713039\n",
      "Step = 641 train_loss: 0.014315456 val_loss: 0.014407839\n",
      "Step = 642 train_loss: 0.018055188 val_loss: 0.016120946\n",
      "Step = 643 train_loss: 0.015463625 val_loss: 0.021537738\n",
      "Step = 644 train_loss: 0.015188171 val_loss: 0.021787714\n",
      "Step = 645 train_loss: 0.014288058 val_loss: 0.02204488\n",
      "Step = 646 train_loss: 0.013636002 val_loss: 0.021842103\n",
      "Step = 647 train_loss: 0.01674697 val_loss: 0.04646657\n",
      "Step = 648 train_loss: 0.017930102 val_loss: 0.029618455\n",
      "Step = 649 train_loss: 0.016286526 val_loss: 0.021227349\n",
      "Step = 650 train_loss: 0.01716618 val_loss: 0.020940568\n",
      "Step = 651 train_loss: 0.023467572 val_loss: 0.019203508\n",
      "Step = 652 train_loss: 0.013770159 val_loss: 0.069413036\n",
      "Step = 653 train_loss: 0.0155012 val_loss: 0.08204328\n",
      "Step = 654 train_loss: 0.017889924 val_loss: 0.06729506\n",
      "Step = 655 train_loss: 0.015204965 val_loss: 0.03561687\n",
      "Step = 656 train_loss: 0.015717952 val_loss: 0.019082928\n",
      "Step = 657 train_loss: 0.016019888 val_loss: 0.022000711\n",
      "Step = 658 train_loss: 0.01247082 val_loss: 0.022154545\n",
      "Step = 659 train_loss: 0.013130175 val_loss: 0.023230033\n",
      "Step = 660 train_loss: 0.01615436 val_loss: 0.059198927\n",
      "Step = 661 train_loss: 0.01603257 val_loss: 0.04678291\n",
      "Step = 662 train_loss: 0.016262678 val_loss: 0.02023825\n",
      "Step = 663 train_loss: 0.015944974 val_loss: 0.023504462\n",
      "Step = 664 train_loss: 0.017040078 val_loss: 0.020822508\n",
      "Step = 665 train_loss: 0.016109766 val_loss: 0.054925285\n",
      "Step = 666 train_loss: 0.017091392 val_loss: 0.067262724\n",
      "Step = 667 train_loss: 0.013345416 val_loss: 0.036379263\n",
      "Step = 668 train_loss: 0.014011633 val_loss: 0.019679442\n",
      "Step = 669 train_loss: 0.01031916 val_loss: 0.023042716\n",
      "Step = 670 train_loss: 0.014746429 val_loss: 0.017682634\n",
      "Step = 671 train_loss: 0.014692841 val_loss: 0.014254208\n",
      "Step = 672 train_loss: 0.013040184 val_loss: 0.020211408\n",
      "Step = 673 train_loss: 0.012412269 val_loss: 0.008675302\n",
      "Step = 674 train_loss: 0.01158982 val_loss: 0.02089475\n",
      "Step = 675 train_loss: 0.014242435 val_loss: 0.020634504\n",
      "Step = 676 train_loss: 0.011134135 val_loss: 0.017702073\n",
      "Step = 677 train_loss: 0.014132813 val_loss: 0.010163533\n",
      "Step = 678 train_loss: 0.012501835 val_loss: 0.01616116\n",
      "Step = 679 train_loss: 0.014904513 val_loss: 0.008548528\n",
      "Step = 680 train_loss: 0.012296249 val_loss: 0.021696782\n",
      "Step = 681 train_loss: 0.012783221 val_loss: 0.022104278\n",
      "Step = 682 train_loss: 0.012493025 val_loss: 0.019128151\n",
      "Step = 683 train_loss: 0.01305105 val_loss: 0.08762911\n",
      "Step = 684 train_loss: 0.014548596 val_loss: 0.085594766\n",
      "Step = 685 train_loss: 0.012825265 val_loss: 0.045188975\n",
      "Step = 686 train_loss: 0.01367166 val_loss: 0.020327484\n",
      "Step = 687 train_loss: 0.011483073 val_loss: 0.023177378\n",
      "Step = 688 train_loss: 0.011972253 val_loss: 0.007733944\n",
      "Step = 689 train_loss: 0.013461865 val_loss: 0.02055085\n",
      "Step = 690 train_loss: 0.011118441 val_loss: 0.0147709\n",
      "Step = 691 train_loss: 0.010979947 val_loss: 0.016919153\n",
      "Step = 692 train_loss: 0.011410604 val_loss: 0.0074725803\n",
      "Step = 693 train_loss: 0.013671929 val_loss: 0.014130406\n",
      "Step = 694 train_loss: 0.009178036 val_loss: 0.0076106843\n",
      "Step = 695 train_loss: 0.010185551 val_loss: 0.007743246\n",
      "Step = 696 train_loss: 0.013057812 val_loss: 0.016292509\n",
      "Step = 697 train_loss: 0.00998638 val_loss: 0.007308811\n",
      "Step = 698 train_loss: 0.008581261 val_loss: 0.023014346\n",
      "Step = 699 train_loss: 0.01182949 val_loss: 0.021936668\n",
      "Step = 700 train_loss: 0.011021486 val_loss: 0.007987712\n",
      "Step = 701 train_loss: 0.012658005 val_loss: 0.011497923\n",
      "Step = 702 train_loss: 0.013779611 val_loss: 0.012732757\n",
      "Step = 703 train_loss: 0.009954412 val_loss: 0.00964605\n",
      "Step = 704 train_loss: 0.009817644 val_loss: 0.012701456\n",
      "Step = 705 train_loss: 0.013149112 val_loss: 0.015197156\n",
      "Step = 706 train_loss: 0.008913418 val_loss: 0.019618401\n",
      "Step = 707 train_loss: 0.011713996 val_loss: 0.027241562\n",
      "Step = 708 train_loss: 0.013134459 val_loss: 0.020402119\n",
      "Step = 709 train_loss: 0.010932795 val_loss: 0.024809724\n",
      "Step = 710 train_loss: 0.013494898 val_loss: 0.04394551\n",
      "Step = 711 train_loss: 0.011097333 val_loss: 0.122591205\n",
      "Step = 712 train_loss: 0.010123979 val_loss: 0.09340412\n",
      "Step = 713 train_loss: 0.009540751 val_loss: 0.012036018\n",
      "Step = 714 train_loss: 0.011392445 val_loss: 0.025706582\n",
      "Step = 715 train_loss: 0.009487816 val_loss: 0.024435773\n",
      "Step = 716 train_loss: 0.010524366 val_loss: 0.011240771\n",
      "Step = 717 train_loss: 0.016469233 val_loss: 0.037042387\n",
      "Step = 718 train_loss: 0.014232164 val_loss: 0.0067929486\n",
      "Step = 719 train_loss: 0.0113609005 val_loss: 0.025869364\n",
      "Step = 720 train_loss: 0.012422225 val_loss: 0.008121009\n",
      "Step = 721 train_loss: 0.010058522 val_loss: 0.06927954\n",
      "Step = 722 train_loss: 0.010205165 val_loss: 0.10784433\n",
      "Step = 723 train_loss: 0.012288085 val_loss: 0.063792825\n",
      "Step = 724 train_loss: 0.010163515 val_loss: 0.0066670827\n",
      "Step = 725 train_loss: 0.009533964 val_loss: 0.026387045\n",
      "Step = 726 train_loss: 0.010383223 val_loss: 0.027226444\n",
      "Step = 727 train_loss: 0.010053991 val_loss: 0.008027399\n",
      "Step = 728 train_loss: 0.007437482 val_loss: 0.04528247\n",
      "Step = 729 train_loss: 0.011036673 val_loss: 0.07444942\n",
      "Step = 730 train_loss: 0.009300407 val_loss: 0.063156545\n",
      "Step = 731 train_loss: 0.0079937205 val_loss: 0.013491019\n",
      "Step = 732 train_loss: 0.008992367 val_loss: 0.010801872\n",
      "Step = 733 train_loss: 0.01102659 val_loss: 0.026716309\n",
      "Step = 734 train_loss: 0.008500304 val_loss: 0.028043054\n",
      "Step = 735 train_loss: 0.009538015 val_loss: 0.013871681\n",
      "Step = 736 train_loss: 0.010191077 val_loss: 0.11722965\n",
      "Step = 737 train_loss: 0.009474359 val_loss: 0.19880453\n",
      "Step = 738 train_loss: 0.013871996 val_loss: 0.19102594\n",
      "Step = 739 train_loss: 0.010778982 val_loss: 0.1166751\n",
      "Step = 740 train_loss: 0.011676399 val_loss: 0.012146384\n",
      "Step = 741 train_loss: 0.013403414 val_loss: 0.030681578\n",
      "Step = 742 train_loss: 0.009520444 val_loss: 0.031679373\n",
      "Step = 743 train_loss: 0.01162555 val_loss: 0.02370097\n",
      "Step = 744 train_loss: 0.008759854 val_loss: 0.011437275\n",
      "Step = 745 train_loss: 0.009205065 val_loss: 0.03109476\n",
      "Step = 746 train_loss: 0.009625937 val_loss: 0.023424705\n",
      "Step = 747 train_loss: 0.010229377 val_loss: 0.00680727\n",
      "Step = 748 train_loss: 0.009482191 val_loss: 0.025421454\n",
      "Step = 749 train_loss: 0.009894494 val_loss: 0.030112868\n",
      "Step = 750 train_loss: 0.0073374514 val_loss: 0.029180346\n",
      "Step = 751 train_loss: 0.008548964 val_loss: 0.02124524\n",
      "Step = 752 train_loss: 0.010696659 val_loss: 0.007887197\n",
      "Step = 753 train_loss: 0.011019493 val_loss: 0.0064828577\n",
      "Step = 754 train_loss: 0.010072549 val_loss: 0.007398316\n",
      "Step = 755 train_loss: 0.009760287 val_loss: 0.0071284277\n",
      "Step = 756 train_loss: 0.008419905 val_loss: 0.0086921705\n",
      "Step = 757 train_loss: 0.010364636 val_loss: 0.012940933\n",
      "Step = 758 train_loss: 0.010439871 val_loss: 0.0251676\n",
      "Step = 759 train_loss: 0.011878661 val_loss: 0.027364926\n",
      "Step = 760 train_loss: 0.012733778 val_loss: 0.017894665\n",
      "Step = 761 train_loss: 0.00885448 val_loss: 0.016020836\n",
      "Step = 762 train_loss: 0.0072575165 val_loss: 0.037436597\n",
      "Step = 763 train_loss: 0.007722781 val_loss: 0.05870139\n",
      "Step = 764 train_loss: 0.0076814685 val_loss: 0.013637905\n",
      "Step = 765 train_loss: 0.006896963 val_loss: 0.015516242\n",
      "Step = 766 train_loss: 0.009481864 val_loss: 0.026658494\n",
      "Step = 767 train_loss: 0.007836681 val_loss: 0.024604622\n",
      "Step = 768 train_loss: 0.010725111 val_loss: 0.015911695\n",
      "Step = 769 train_loss: 0.0107488865 val_loss: 0.1580802\n",
      "Step = 770 train_loss: 0.008467835 val_loss: 0.24865618\n",
      "Step = 771 train_loss: 0.011514059 val_loss: 0.24135862\n",
      "Step = 772 train_loss: 0.010273575 val_loss: 0.16473356\n",
      "Step = 773 train_loss: 0.011760827 val_loss: 0.021021131\n",
      "Step = 774 train_loss: 0.0099391695 val_loss: 0.029925786\n",
      "Step = 775 train_loss: 0.010368297 val_loss: 0.030773794\n",
      "Step = 776 train_loss: 0.013243755 val_loss: 0.020776771\n",
      "Step = 777 train_loss: 0.0110473 val_loss: 0.12246153\n",
      "Step = 778 train_loss: 0.011973219 val_loss: 0.2868601\n",
      "Step = 779 train_loss: 0.006881967 val_loss: 0.33528408\n",
      "Step = 780 train_loss: 0.010657838 val_loss: 0.3046679\n",
      "Step = 781 train_loss: 0.012337381 val_loss: 0.21460703\n",
      "Step = 782 train_loss: 0.011929131 val_loss: 0.09201104\n",
      "Step = 783 train_loss: 0.012070994 val_loss: 0.008938059\n",
      "Step = 784 train_loss: 0.0103138285 val_loss: 0.035408072\n",
      "Step = 785 train_loss: 0.008389578 val_loss: 0.034526125\n",
      "Step = 786 train_loss: 0.014364729 val_loss: 0.0091507295\n",
      "Step = 787 train_loss: 0.011900409 val_loss: 0.2301928\n",
      "Step = 788 train_loss: 0.010608153 val_loss: 0.36414668\n",
      "Step = 789 train_loss: 0.011795025 val_loss: 0.3960859\n",
      "Step = 790 train_loss: 0.014954597 val_loss: 0.38748702\n",
      "Step = 791 train_loss: 0.013014381 val_loss: 0.3527719\n",
      "Step = 792 train_loss: 0.0156998 val_loss: 0.29944488\n",
      "Step = 793 train_loss: 0.017393162 val_loss: 0.23041344\n",
      "Step = 794 train_loss: 0.016936062 val_loss: 0.16336583\n",
      "Step = 795 train_loss: 0.018127393 val_loss: 0.097090304\n",
      "Step = 796 train_loss: 0.013705241 val_loss: 0.040998094\n",
      "Step = 797 train_loss: 0.012044873 val_loss: 0.01593114\n",
      "Step = 798 train_loss: 0.013078491 val_loss: 0.024956776\n",
      "Step = 799 train_loss: 0.013211375 val_loss: 0.025185902\n",
      "Step = 800 train_loss: 0.007384191 val_loss: 0.026320152\n",
      "Step = 801 train_loss: 0.011535232 val_loss: 0.033122104\n",
      "Step = 802 train_loss: 0.015725357 val_loss: 0.039360836\n",
      "Step = 803 train_loss: 0.0093423035 val_loss: 0.023316754\n",
      "Step = 804 train_loss: 0.007290632 val_loss: 0.024303567\n",
      "Step = 805 train_loss: 0.012452214 val_loss: 0.07145407\n",
      "Step = 806 train_loss: 0.012803197 val_loss: 0.08596642\n",
      "Step = 807 train_loss: 0.014203797 val_loss: 0.0655435\n",
      "Step = 808 train_loss: 0.009707849 val_loss: 0.021830713\n",
      "Step = 809 train_loss: 0.012691408 val_loss: 0.010529337\n",
      "Step = 810 train_loss: 0.009143444 val_loss: 0.040426116\n",
      "Step = 811 train_loss: 0.008703048 val_loss: 0.040827733\n",
      "Step = 812 train_loss: 0.0062155295 val_loss: 0.040484715\n",
      "Step = 813 train_loss: 0.009249836 val_loss: 0.039690543\n",
      "Step = 814 train_loss: 0.009633224 val_loss: 0.037899505\n",
      "Step = 815 train_loss: 0.010807746 val_loss: 0.0361452\n",
      "Step = 816 train_loss: 0.009173375 val_loss: 0.035039317\n",
      "Step = 817 train_loss: 0.007664644 val_loss: 0.03418908\n",
      "Step = 818 train_loss: 0.010175571 val_loss: 0.033365652\n",
      "Step = 819 train_loss: 0.006721967 val_loss: 0.03337155\n",
      "Step = 820 train_loss: 0.00789897 val_loss: 0.032805875\n",
      "Step = 821 train_loss: 0.010223226 val_loss: 0.031060917\n",
      "Step = 822 train_loss: 0.009534697 val_loss: 0.012081268\n",
      "Step = 823 train_loss: 0.0064583067 val_loss: 0.007101423\n",
      "Step = 824 train_loss: 0.0072927154 val_loss: 0.011130172\n",
      "Step = 825 train_loss: 0.009043819 val_loss: 0.015001204\n",
      "Step = 826 train_loss: 0.008994414 val_loss: 0.009625198\n",
      "Step = 827 train_loss: 0.008961967 val_loss: 0.017349862\n",
      "Step = 828 train_loss: 0.010134185 val_loss: 0.030623686\n",
      "Step = 829 train_loss: 0.0066703013 val_loss: 0.030694475\n",
      "Step = 830 train_loss: 0.012220863 val_loss: 0.011628195\n",
      "Step = 831 train_loss: 0.007782137 val_loss: 0.021836821\n",
      "Step = 832 train_loss: 0.0063658254 val_loss: 0.0719833\n",
      "Step = 833 train_loss: 0.009152665 val_loss: 0.039965168\n",
      "Step = 834 train_loss: 0.008547086 val_loss: 0.021519575\n",
      "Step = 835 train_loss: 0.012803848 val_loss: 0.006034218\n",
      "Step = 836 train_loss: 0.006293876 val_loss: 0.02142268\n",
      "Step = 837 train_loss: 0.008164325 val_loss: 0.029284908\n",
      "Step = 838 train_loss: 0.005663761 val_loss: 0.028762512\n",
      "Step = 839 train_loss: 0.008061896 val_loss: 0.02385269\n",
      "Step = 840 train_loss: 0.0059882393 val_loss: 0.027347721\n",
      "Step = 841 train_loss: 0.010672921 val_loss: 0.026937438\n",
      "Step = 842 train_loss: 0.008368559 val_loss: 0.016576719\n",
      "Step = 843 train_loss: 0.009009302 val_loss: 0.0056445412\n",
      "Step = 844 train_loss: 0.0076325256 val_loss: 0.0069006854\n",
      "Step = 845 train_loss: 0.008648454 val_loss: 0.006890752\n",
      "Step = 846 train_loss: 0.007061926 val_loss: 0.0062138014\n",
      "Step = 847 train_loss: 0.007541473 val_loss: 0.0072586364\n",
      "Step = 848 train_loss: 0.0070594097 val_loss: 0.00573547\n",
      "Step = 849 train_loss: 0.0049705678 val_loss: 0.013386063\n",
      "Step = 850 train_loss: 0.007863041 val_loss: 0.0077492995\n",
      "Step = 851 train_loss: 0.006762939 val_loss: 0.007897832\n",
      "Step = 852 train_loss: 0.0083853025 val_loss: 0.0056060166\n",
      "Step = 853 train_loss: 0.006913806 val_loss: 0.0076203593\n",
      "Step = 854 train_loss: 0.007393492 val_loss: 0.014625223\n",
      "Step = 855 train_loss: 0.0094928825 val_loss: 0.0077241203\n",
      "Step = 856 train_loss: 0.007946319 val_loss: 0.008486979\n",
      "Step = 857 train_loss: 0.008524315 val_loss: 0.007884057\n",
      "Step = 858 train_loss: 0.008052783 val_loss: 0.0059962594\n",
      "Step = 859 train_loss: 0.008281121 val_loss: 0.0077499473\n",
      "Step = 860 train_loss: 0.0075683775 val_loss: 0.023565052\n",
      "Step = 861 train_loss: 0.0070531443 val_loss: 0.025221892\n",
      "Step = 862 train_loss: 0.0077125984 val_loss: 0.09607194\n",
      "Step = 863 train_loss: 0.008189874 val_loss: 0.06251816\n",
      "Step = 864 train_loss: 0.0072509428 val_loss: 0.0137055265\n",
      "Step = 865 train_loss: 0.007831209 val_loss: 0.005804003\n",
      "Step = 866 train_loss: 0.0068551223 val_loss: 0.013446552\n",
      "Step = 867 train_loss: 0.0056414763 val_loss: 0.014055996\n",
      "Step = 868 train_loss: 0.007892203 val_loss: 0.020937303\n",
      "Step = 869 train_loss: 0.008605055 val_loss: 0.014628118\n",
      "Step = 870 train_loss: 0.0053013735 val_loss: 0.011385486\n",
      "Step = 871 train_loss: 0.0089169955 val_loss: 0.068502106\n",
      "Step = 872 train_loss: 0.009204187 val_loss: 0.122019306\n",
      "Step = 873 train_loss: 0.010506978 val_loss: 0.14047414\n",
      "Step = 874 train_loss: 0.008425726 val_loss: 0.12299745\n",
      "Step = 875 train_loss: 0.008694449 val_loss: 0.0501635\n",
      "Step = 876 train_loss: 0.009587624 val_loss: 0.014980572\n",
      "Step = 877 train_loss: 0.011174116 val_loss: 0.018753711\n",
      "Step = 878 train_loss: 0.007849657 val_loss: 0.02777829\n",
      "Step = 879 train_loss: 0.009990613 val_loss: 0.026968125\n",
      "Step = 880 train_loss: 0.0075965035 val_loss: 0.02081316\n",
      "Step = 881 train_loss: 0.0065703085 val_loss: 0.008768769\n",
      "Step = 882 train_loss: 0.006971083 val_loss: 0.053068742\n",
      "Step = 883 train_loss: 0.009334671 val_loss: 0.082428776\n",
      "Step = 884 train_loss: 0.009056065 val_loss: 0.07067063\n",
      "Step = 885 train_loss: 0.007747594 val_loss: 0.03398475\n",
      "Step = 886 train_loss: 0.008633228 val_loss: 0.0065340823\n",
      "Step = 887 train_loss: 0.007945109 val_loss: 0.022466473\n",
      "Step = 888 train_loss: 0.0064506214 val_loss: 0.028851982\n",
      "Step = 889 train_loss: 0.007197315 val_loss: 0.026029587\n",
      "Step = 890 train_loss: 0.008007672 val_loss: 0.00978026\n",
      "Step = 891 train_loss: 0.0065848 val_loss: 0.005311375\n",
      "Step = 892 train_loss: 0.0048253406 val_loss: 0.008358381\n",
      "Step = 893 train_loss: 0.007939174 val_loss: 0.0053496854\n",
      "Step = 894 train_loss: 0.00873004 val_loss: 0.00679095\n",
      "Step = 895 train_loss: 0.007250659 val_loss: 0.02325817\n",
      "Step = 896 train_loss: 0.0068555074 val_loss: 0.024821505\n",
      "Step = 897 train_loss: 0.0091384305 val_loss: 0.0054790014\n",
      "Step = 898 train_loss: 0.0065913107 val_loss: 0.0133225145\n",
      "Step = 899 train_loss: 0.007096708 val_loss: 0.048969645\n",
      "Step = 900 train_loss: 0.008615051 val_loss: 0.039856747\n",
      "Step = 901 train_loss: 0.009385707 val_loss: 0.012288861\n",
      "Step = 902 train_loss: 0.009485378 val_loss: 0.007818541\n",
      "Step = 903 train_loss: 0.007848485 val_loss: 0.0080046365\n",
      "Step = 904 train_loss: 0.0059528276 val_loss: 0.0156555\n",
      "Step = 905 train_loss: 0.0057298318 val_loss: 0.014034326\n",
      "Step = 906 train_loss: 0.0074093556 val_loss: 0.013955598\n",
      "Step = 907 train_loss: 0.0073218774 val_loss: 0.013217897\n",
      "Step = 908 train_loss: 0.006021343 val_loss: 0.007927138\n",
      "Step = 909 train_loss: 0.009320648 val_loss: 0.0055454513\n",
      "Step = 910 train_loss: 0.007387735 val_loss: 0.0051192916\n",
      "Step = 911 train_loss: 0.008051238 val_loss: 0.005100099\n",
      "Step = 912 train_loss: 0.0069445795 val_loss: 0.0054181833\n",
      "Step = 913 train_loss: 0.007107628 val_loss: 0.011423833\n",
      "Step = 914 train_loss: 0.008432708 val_loss: 0.006406801\n",
      "Step = 915 train_loss: 0.005386576 val_loss: 0.020175332\n",
      "Step = 916 train_loss: 0.006988474 val_loss: 0.012331467\n",
      "Step = 917 train_loss: 0.0071249614 val_loss: 0.006088535\n",
      "Step = 918 train_loss: 0.0076609654 val_loss: 0.0050635883\n",
      "Step = 919 train_loss: 0.006296613 val_loss: 0.0064632893\n",
      "Step = 920 train_loss: 0.009034223 val_loss: 0.0063776583\n",
      "Step = 921 train_loss: 0.005111168 val_loss: 0.0070651094\n",
      "Step = 922 train_loss: 0.007061873 val_loss: 0.005193357\n",
      "Step = 923 train_loss: 0.005229484 val_loss: 0.016577737\n",
      "Step = 924 train_loss: 0.0043315957 val_loss: 0.047362372\n",
      "Step = 925 train_loss: 0.0076830154 val_loss: 0.0055638794\n",
      "Step = 926 train_loss: 0.0048765964 val_loss: 0.021564988\n",
      "Step = 927 train_loss: 0.00572583 val_loss: 0.005759408\n",
      "Step = 928 train_loss: 0.0074517205 val_loss: 0.03782853\n",
      "Step = 929 train_loss: 0.009645583 val_loss: 0.07193601\n",
      "Step = 930 train_loss: 0.008590769 val_loss: 0.044606127\n",
      "Step = 931 train_loss: 0.009566527 val_loss: 0.007837572\n",
      "Step = 932 train_loss: 0.009437891 val_loss: 0.013108021\n",
      "Step = 933 train_loss: 0.0065624714 val_loss: 0.01203365\n",
      "Step = 934 train_loss: 0.009276285 val_loss: 0.026809273\n",
      "Step = 935 train_loss: 0.009207768 val_loss: 0.11110818\n",
      "Step = 936 train_loss: 0.008327114 val_loss: 0.0946276\n",
      "Step = 937 train_loss: 0.006856146 val_loss: 0.02871211\n",
      "Step = 938 train_loss: 0.0062069464 val_loss: 0.008391697\n",
      "Step = 939 train_loss: 0.007937618 val_loss: 0.02558589\n",
      "Step = 940 train_loss: 0.008857913 val_loss: 0.025091877\n",
      "Step = 941 train_loss: 0.0097778905 val_loss: 0.006111733\n",
      "Step = 942 train_loss: 0.007349044 val_loss: 0.021382557\n",
      "Step = 943 train_loss: 0.00584505 val_loss: 0.037396718\n",
      "Step = 944 train_loss: 0.008235184 val_loss: 0.05985381\n",
      "Step = 945 train_loss: 0.0061236713 val_loss: 0.031535834\n",
      "Step = 946 train_loss: 0.0077307196 val_loss: 0.01129542\n",
      "Step = 947 train_loss: 0.007446279 val_loss: 0.023456838\n",
      "Step = 948 train_loss: 0.008648633 val_loss: 0.025098836\n",
      "Step = 949 train_loss: 0.009778801 val_loss: 0.01944533\n",
      "Step = 950 train_loss: 0.007687539 val_loss: 0.013892895\n",
      "Step = 951 train_loss: 0.008549063 val_loss: 0.01771114\n",
      "Step = 952 train_loss: 0.006256253 val_loss: 0.020069009\n",
      "Step = 953 train_loss: 0.008205104 val_loss: 0.020900426\n",
      "Step = 954 train_loss: 0.007588511 val_loss: 0.022370128\n",
      "Step = 955 train_loss: 0.007658994 val_loss: 0.022569912\n",
      "Step = 956 train_loss: 0.009328274 val_loss: 0.020891292\n",
      "Step = 957 train_loss: 0.0068842703 val_loss: 0.021238158\n",
      "Step = 958 train_loss: 0.007603973 val_loss: 0.023827225\n",
      "Step = 959 train_loss: 0.0065620346 val_loss: 0.02076476\n",
      "Step = 960 train_loss: 0.009042241 val_loss: 0.016684838\n",
      "Step = 961 train_loss: 0.011622297 val_loss: 0.01739697\n",
      "Step = 962 train_loss: 0.012601733 val_loss: 0.016339034\n",
      "Step = 963 train_loss: 0.010515983 val_loss: 0.034624916\n",
      "Step = 964 train_loss: 0.003661404 val_loss: 0.044654414\n",
      "Step = 965 train_loss: 0.008845255 val_loss: 0.019171912\n",
      "Step = 966 train_loss: 0.009103123 val_loss: 0.013886923\n",
      "Step = 967 train_loss: 0.005473608 val_loss: 0.017480893\n",
      "Step = 968 train_loss: 0.009965185 val_loss: 0.013175729\n",
      "Step = 969 train_loss: 0.008044614 val_loss: 0.017484495\n",
      "Step = 970 train_loss: 0.008205007 val_loss: 0.06049975\n",
      "Step = 971 train_loss: 0.0077063534 val_loss: 0.06055551\n",
      "Step = 972 train_loss: 0.008313082 val_loss: 0.03822798\n",
      "Step = 973 train_loss: 0.0056950888 val_loss: 0.016486805\n",
      "Step = 974 train_loss: 0.008155268 val_loss: 0.0075109717\n",
      "Step = 975 train_loss: 0.0068482854 val_loss: 0.012293878\n",
      "Step = 976 train_loss: 0.0065372405 val_loss: 0.015569344\n",
      "Step = 977 train_loss: 0.005113947 val_loss: 0.013625523\n",
      "Step = 978 train_loss: 0.0063327197 val_loss: 0.008467988\n",
      "Step = 979 train_loss: 0.006243587 val_loss: 0.007579718\n",
      "Step = 980 train_loss: 0.006801453 val_loss: 0.00633439\n",
      "Step = 981 train_loss: 0.0039265277 val_loss: 0.00703054\n",
      "Step = 982 train_loss: 0.007079618 val_loss: 0.016806044\n",
      "Step = 983 train_loss: 0.007152224 val_loss: 0.018451681\n",
      "Step = 984 train_loss: 0.010452986 val_loss: 0.014309336\n",
      "Step = 985 train_loss: 0.006645178 val_loss: 0.0058116023\n",
      "Step = 986 train_loss: 0.0070539573 val_loss: 0.015960716\n",
      "Step = 987 train_loss: 0.005218508 val_loss: 0.022767343\n",
      "Step = 988 train_loss: 0.008773344 val_loss: 0.036977697\n",
      "Step = 989 train_loss: 0.0072795292 val_loss: 0.026766926\n",
      "Step = 990 train_loss: 0.006524134 val_loss: 0.0059166774\n",
      "Step = 991 train_loss: 0.0069927443 val_loss: 0.020120531\n",
      "Step = 992 train_loss: 0.008981365 val_loss: 0.021245724\n",
      "Step = 993 train_loss: 0.009092884 val_loss: 0.0058040414\n",
      "Step = 994 train_loss: 0.007987434 val_loss: 0.10992743\n",
      "Step = 995 train_loss: 0.007920771 val_loss: 0.28268832\n",
      "Step = 996 train_loss: 0.0073375055 val_loss: 0.31137457\n",
      "Step = 997 train_loss: 0.0057204664 val_loss: 0.26484102\n",
      "Step = 998 train_loss: 0.010026268 val_loss: 0.14740801\n",
      "Step = 999 train_loss: 0.0068664188 val_loss: 0.029947188\n",
      "918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_16896\\2290839578.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_16896\\2290839578.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "val_losses=[]\n",
    "\n",
    "for t in range(1000):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    '''save all the model in each epoch'''    \n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, 'Target_model/net_parameters'+str(t)+'.pkl')\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print ('Step = %d' % t, 'train_loss:',train_loss.data.numpy(),'val_loss:',val_loss.data.numpy())\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "   \n",
    "'''find the best model'''\n",
    "best_index=val_losses.index(np.min(val_losses))\n",
    "print(best_index)\n",
    "\n",
    "'''copy the best model from model file to the model_test file'''\n",
    "if 1:\n",
    "    shutil.copyfile('Target_model/net_parameters'+str(best_index)+'.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "'''Reload the best model''' \n",
    "model = Net()\n",
    "a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
    "optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.14525968\n",
      "mpe_val: 0.13881427\n",
      "mpe_a: 0.1646753636658006\n",
      "mpe_b: 0.1556845050013193\n",
      "rmse_train: 178.50555\n",
      "rmse_val: 158.98611\n",
      "rmse_a: 178.9392517419068\n",
      "rmse_b: 222.81505783945573\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.780303955078125\n",
      "r2_val: 0.757173478603363\n",
      "r2_a: 0.7901904334717177\n",
      "r2_b: 0.38925433189972136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
