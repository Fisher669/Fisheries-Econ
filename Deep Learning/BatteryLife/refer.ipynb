{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOOGLENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "'''Define the CNN model''' \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 公用的前3层卷积层\n",
    "        self.shared_cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(8, 16, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # 插入Google net Inception模块\n",
    "        self.inceptions = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True),\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=1, padding=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True),\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(5, 5), stride=1, padding=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        # 融合后的全连接层\n",
    "        self.fc1 = torch.nn.Linear(4832, 1000)\n",
    "        self.drop_layer = torch.nn.Dropout(p=0.2)\n",
    "        self.fc2 = torch.nn.Linear(1000, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            # 先计算公用卷积层\n",
    "            shared_out = self.shared_cnn(x)\n",
    "            \n",
    "            # 计算Inception模块的输出\n",
    "            inception_outputs = [inception(shared_out) for inception in self.inceptions]\n",
    "            combined = torch.cat(inception_outputs, dim=1)  # 在最后一个维度拼接\n",
    "\n",
    "            # 全连接层\n",
    "            x = torch.relu(self.fc1(combined))\n",
    "            x = self.drop_layer(x)\n",
    "            x = self.fc2(x)\n",
    "            x = torch.sigmoid(x)  # 修复了这里的错误，去掉了不必要的fc3层\n",
    "            return x\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during forward pass: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet-34架构\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ResNet类实现\n",
    "class ResNet(nn.Module):\n",
    "    # 初始化方法，设置ResNet的构造函数\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64  # 初始化inplanes\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(64, 3)\n",
    "        self.layer2 = self._make_layer(128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)  # 这里不需要乘法\n",
    "\n",
    "    # 创建残差块的方法\n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * 4:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * 4, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * 4)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(self._make_block(planes, stride, downsample))  # 添加第一个块\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(self._make_block(planes))  # 添加后续块\n",
    "        return nn.Sequential(*layers)  # 将layers转换为Sequential\n",
    "\n",
    "    def _make_block(self, planes, stride=1, downsample=None):\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(planes))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(planes))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Conv2d(planes, planes * 4, kernel_size=1, stride=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(planes * 4))\n",
    "        \n",
    "        if downsample is not None:\n",
    "            layers.append(downsample)\n",
    "        \n",
    "        self.inplanes = planes * 4\n",
    "        return nn.Sequential(*layers)  # 将block转换为Sequential\n",
    "\n",
    "    # 前向传播方法\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise ValueError(\"Input must be a Torch tensor.\")\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionResNetV2架构的示例代码\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnext架构的示例代码\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义ResNeXt类，继承自nn.Module\n",
    "class ResNeXt(nn.Module):\n",
    "    # 初始化方法，设置网络结构及参数\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(ResNeXt, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        \n",
    "        # 定义卷积层和其他层\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # 使用自适应平均池化\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # 权重初始化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # 构建网络的层\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        layers.extend(block(self.inplanes, planes) for _ in range(1, blocks))  # 使用列表推导式简化代码\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # 定义前向传播方法\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "            # 依次经过各层\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "\n",
    "            # 使用自适应平均池化，避免硬编码\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)  # 使用torch.flatten简化代码\n",
    "            x = self.fc(x)\n",
    "\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"在前向传播中发生错误: {e}\")\n",
    "            return None  # 这里返回None或根据需要返回其他值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 余弦退火算法的实现\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
    "    \"\"\"\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        first_cycle_steps (int): First cycle step size.\n",
    "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
    "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
    "        min_lr(float): Min learning rate. Default: 0.001.\n",
    "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
    "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 optimizer : torch.optim.Optimizer,\n",
    "                 first_cycle_steps : int,\n",
    "                 cycle_mult : float = 1.,\n",
    "                 max_lr : float = 0.1,\n",
    "                 min_lr : float = 0.001,\n",
    "                 warmup_steps : int = 0,\n",
    "                 gamma : float = 1.,\n",
    "                 last_epoch : int = -1\n",
    "        ):\n",
    "        assert warmup_steps < first_cycle_steps\n",
    "        \n",
    "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
    "        self.base_max_lr = max_lr # first max learning rate\n",
    "        self.max_lr = max_lr # max learning rate in the current cycle\n",
    "        self.min_lr = min_lr # min learning rate\n",
    "        self.warmup_steps = warmup_steps # warmup step size\n",
    "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
    "        \n",
    "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle = 0 # cycle count\n",
    "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
    "        \n",
    "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "        # set learning rate min_lr\n",
    "        self.init_lr()\n",
    "    \n",
    "    def init_lr(self):\n",
    "        self.base_lrs = []\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.min_lr\n",
    "            self.base_lrs.append(self.min_lr)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.step_in_cycle == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.step_in_cycle < self.warmup_steps:\n",
    "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.max_lr - base_lr) \\\n",
    "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
    "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.step_in_cycle = self.step_in_cycle + 1\n",
    "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
    "                self.cycle += 1\n",
    "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
    "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
    "        else:\n",
    "            if epoch >= self.first_cycle_steps:\n",
    "                if self.cycle_mult == 1.:\n",
    "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
    "                    self.cycle = epoch // self.first_cycle_steps\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
    "                    self.cycle = n\n",
    "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
    "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
    "            else:\n",
    "                self.cur_cycle_steps = self.first_cycle_steps\n",
    "                self.step_in_cycle = epoch\n",
    "                \n",
    "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 SGD 优化器并添加余弦退火学习率调度器\n",
    "optimizer_CosineLR = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "CosineLR = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_CosineLR, T_max=100, eta_min=0.000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMA-attention-module\n",
    "import torch\n",
    "from torch import nn\n",
    "#GitHub地址：https://github.com/YOLOonMe/EMA-attention-module\n",
    "#论文地址：https://arxiv.org/abs/2305.13563v2\n",
    "class EMA(nn.Module):\n",
    "    def __init__(self, channels, factor=8):\n",
    "        super(EMA, self).__init__()\n",
    "        self.groups = factor\n",
    "        assert channels // self.groups > 0\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.agp = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "        self.gn = nn.GroupNorm(channels // self.groups, channels // self.groups)\n",
    "        self.conv1x1 = nn.Conv2d(channels // self.groups, channels // self.groups, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3x3 = nn.Conv2d(channels // self.groups, channels // self.groups, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        group_x = x.reshape(b * self.groups, -1, h, w)  # b*g,c//g,h,w\n",
    "        x_h = self.pool_h(group_x)\n",
    "        x_w = self.pool_w(group_x).permute(0, 1, 3, 2)\n",
    "        hw = self.conv1x1(torch.cat([x_h, x_w], dim=2))\n",
    "        x_h, x_w = torch.split(hw, [h, w], dim=2)\n",
    "        x1 = self.gn(group_x * x_h.sigmoid() * x_w.permute(0, 1, 3, 2).sigmoid())\n",
    "        x2 = self.conv3x3(group_x)\n",
    "        x11 = self.softmax(self.agp(x1).reshape(b * self.groups, -1, 1).permute(0, 2, 1))\n",
    "        x12 = x2.reshape(b * self.groups, c // self.groups, -1)  # b*g, c//g, hw\n",
    "        x21 = self.softmax(self.agp(x2).reshape(b * self.groups, -1, 1).permute(0, 2, 1))\n",
    "        x22 = x1.reshape(b * self.groups, c // self.groups, -1)  # b*g, c//g, hw\n",
    "        weights = (torch.matmul(x11, x12) + torch.matmul(x21, x22)).reshape(b * self.groups, 1, h, w)\n",
    "        return (group_x * weights.sigmoid()).reshape(b, c, h, w)\n",
    "\n",
    "\n",
    "# 输入 B C H W,  输出 B C H W\n",
    "if __name__ == '__main__':\n",
    "    block = EMA(64).cuda()\n",
    "    input = torch.rand(1, 64, 64, 64).cuda()\n",
    "    output = block(input)\n",
    "    print(input.size(), output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM: Convolutional Block Attention Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self,in_channel,reduction=16,kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        #通道注意力机制\n",
    "        self.max_pool=nn.AdaptiveMaxPool2d(output_size=1)\n",
    "        self.avg_pool=nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.mlp=nn.Sequential(\n",
    "            nn.Linear(in_features=in_channel,out_features=in_channel//reduction,bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=in_channel//reduction,out_features=in_channel,bias=False)\n",
    "        )\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        #空间注意力机制\n",
    "        self.conv=nn.Conv2d(in_channels=2,out_channels=1,kernel_size=kernel_size ,stride=1,padding=kernel_size//2,bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #通道注意力机制\n",
    "        maxout=self.max_pool(x)\n",
    "        maxout=self.mlp(maxout.view(maxout.size(0),-1))\n",
    "        avgout=self.avg_pool(x)\n",
    "        avgout=self.mlp(avgout.view(avgout.size(0),-1))\n",
    "        channel_out=self.sigmoid(maxout+avgout)\n",
    "        channel_out=channel_out.view(x.size(0),x.size(1),1,1)\n",
    "        channel_out=channel_out*x\n",
    "        #空间注意力机制\n",
    "        max_out,_=torch.max(channel_out,dim=1,keepdim=True)\n",
    "        mean_out=torch.mean(channel_out,dim=1,keepdim=True)\n",
    "        out=torch.cat((max_out,mean_out),dim=1)\n",
    "        out=self.sigmoid(self.conv(out))\n",
    "        out=out*channel_out\n",
    "        return out\n",
    "    \n",
    "# 拆分版，提升效率\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channel, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(output_size=1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=in_channel, out_features=in_channel // reduction, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=in_channel // reduction, out_features=in_channel, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_out = self.max_pool(x)\n",
    "        max_out = self.mlp(max_out.view(max_out.size(0), -1))\n",
    "        avg_out = self.avg_pool(x)\n",
    "        avg_out = self.mlp(avg_out.view(avg_out.size(0), -1))\n",
    "        channel_out = self.sigmoid(max_out + avg_out).view(x.size(0), x.size(1), 1, 1)\n",
    "        return channel_out * x\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=kernel_size, stride=1,\n",
    "                              padding=kernel_size // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        mean_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        out = torch.cat((max_out, mean_out), dim=1)\n",
    "        return out * self.sigmoid(self.conv(out))\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # ... 其他层定义 ...\n",
    "        self.channel_attention = ChannelAttention(64)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers1(x)\n",
    "        x = self.channel_attention(x)  # 使用通道注意力\n",
    "        x = self.spatial_attention(x)   # 使用空间注意力\n",
    "        x = self.layer2(x)\n",
    "        # ... 其他操作 ...\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
