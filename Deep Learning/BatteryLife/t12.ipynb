{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "class EcaLayer(nn.Module):\n",
    "    \"\"\"Constructs a ECA module.\"\"\"\n",
    "    \n",
    "    def __init__(self, channel, k_size=3):\n",
    "        super(EcaLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)  # 使用平均池化\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        return x * self.sigmoid(y).expand_as(x)  # 计算ECA模块输出\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(8, 16, kernel_size=(2, 3)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(16, 32, kernel_size=(2, 3)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.eca1 = EcaLayer(32)\n",
    "        self.layer2 =nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(2, 2)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=(1, 2)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(128,256,kernel_size=(1,2)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # self.lstm = nn.LSTM(64, 32, batch_first=True)\n",
    "        self.fc1 = nn.Linear(28672, 32)  # 修正为与LSTM输出相匹配\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(p=0.2)  # 添加Dropout层\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            x = self.layers1(x)\n",
    "            x = self.eca1(x)\n",
    "            x = self.layer2(x)\n",
    "            # x = to_3d(x)  # 转换为3D\n",
    "            # x, _ = self.lstm(x)  # LSTM处理\n",
    "            # x = x[:, -1, :].unsqueeze(2).unsqueeze(3)\n",
    "            # x = self.eca1(x)\n",
    "            # x = x[:,-1,:]\n",
    "            x = x.view(x.size(0), -1)  # 压缩为2d\n",
    "            x = torch.relu(self.fc1(x))  # 压缩多余维度\n",
    "            x = self.dropout(x)  # 使用定义的Dropout层\n",
    "            return torch.sigmoid(self.fc2(x))\n",
    "        except RuntimeError as e:\n",
    "            print(f\"运行时错误：{e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"发生意外错误：{e}\")\n",
    "            raise  # 重新抛出异常，以便进一步处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (12): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (eca1): EcaLayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (conv): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=28672, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Step = 0, train_loss: 0.09572531282901764, val_loss: 0.06228459253907204\n",
      "Step = 1, train_loss: 0.026091933250427246, val_loss: 0.06045908480882645\n",
      "Step = 2, train_loss: 0.041343312710523605, val_loss: 0.05982936918735504\n",
      "Step = 3, train_loss: 0.03647635504603386, val_loss: 0.060331426560878754\n",
      "Step = 4, train_loss: 0.03805653750896454, val_loss: 0.061613038182258606\n",
      "Step = 5, train_loss: 0.03004264459013939, val_loss: 0.062458619475364685\n",
      "Step = 6, train_loss: 0.03874038904905319, val_loss: 0.06233436241745949\n",
      "Step = 7, train_loss: 0.029924320057034492, val_loss: 0.06167913228273392\n",
      "Step = 8, train_loss: 0.03522749990224838, val_loss: 0.05997057259082794\n",
      "Step = 9, train_loss: 0.031521257013082504, val_loss: 0.057461418211460114\n",
      "Step = 10, train_loss: 0.0256232637912035, val_loss: 0.0544164702296257\n",
      "Step = 11, train_loss: 0.030871771275997162, val_loss: 0.052024517208337784\n",
      "Step = 12, train_loss: 0.035346031188964844, val_loss: 0.050815239548683167\n",
      "Step = 13, train_loss: 0.037182919681072235, val_loss: 0.0502663254737854\n",
      "Step = 14, train_loss: 0.034965332597494125, val_loss: 0.05055399611592293\n",
      "Step = 15, train_loss: 0.0365091934800148, val_loss: 0.05115148425102234\n",
      "Step = 16, train_loss: 0.029171429574489594, val_loss: 0.05178404599428177\n",
      "Step = 17, train_loss: 0.03267817571759224, val_loss: 0.052045635879039764\n",
      "Step = 18, train_loss: 0.037480246275663376, val_loss: 0.051581721752882004\n",
      "Step = 19, train_loss: 0.0294217299669981, val_loss: 0.05084815248847008\n",
      "Step = 20, train_loss: 0.0298469141125679, val_loss: 0.04983505234122276\n",
      "Step = 21, train_loss: 0.027957549318671227, val_loss: 0.04859911650419235\n",
      "Step = 22, train_loss: 0.025389842689037323, val_loss: 0.04706668481230736\n",
      "Step = 23, train_loss: 0.03086838498711586, val_loss: 0.04533914104104042\n",
      "Step = 24, train_loss: 0.03350427746772766, val_loss: 0.04381127282977104\n",
      "Step = 25, train_loss: 0.029112253338098526, val_loss: 0.04259258508682251\n",
      "Step = 26, train_loss: 0.03319747745990753, val_loss: 0.041923750191926956\n",
      "Step = 27, train_loss: 0.028737813234329224, val_loss: 0.041578102856874466\n",
      "Step = 28, train_loss: 0.026607802137732506, val_loss: 0.04161374270915985\n",
      "Step = 29, train_loss: 0.03083842247724533, val_loss: 0.041713032871484756\n",
      "Step = 30, train_loss: 0.028567107394337654, val_loss: 0.042489469051361084\n",
      "Step = 31, train_loss: 0.03162017837166786, val_loss: 0.043519847095012665\n",
      "Step = 32, train_loss: 0.0321841798722744, val_loss: 0.04457555338740349\n",
      "Step = 33, train_loss: 0.036695003509521484, val_loss: 0.04572349786758423\n",
      "Step = 34, train_loss: 0.0299252700060606, val_loss: 0.046732645481824875\n",
      "Step = 35, train_loss: 0.028984008356928825, val_loss: 0.0472586452960968\n",
      "Step = 36, train_loss: 0.03511093184351921, val_loss: 0.046899545937776566\n",
      "Step = 37, train_loss: 0.03163957968354225, val_loss: 0.046228181570768356\n",
      "Step = 38, train_loss: 0.028443844988942146, val_loss: 0.04505755007266998\n",
      "Step = 39, train_loss: 0.026439759880304337, val_loss: 0.04389970749616623\n",
      "Step = 40, train_loss: 0.03186364471912384, val_loss: 0.043211571872234344\n",
      "Step = 41, train_loss: 0.03067789226770401, val_loss: 0.04186694324016571\n",
      "Step = 42, train_loss: 0.029243944212794304, val_loss: 0.040487829595804214\n",
      "Step = 43, train_loss: 0.026559213176369667, val_loss: 0.03928485885262489\n",
      "Step = 44, train_loss: 0.030853619799017906, val_loss: 0.04003326594829559\n",
      "Step = 45, train_loss: 0.033269934356212616, val_loss: 0.0400729775428772\n",
      "Step = 46, train_loss: 0.030501816421747208, val_loss: 0.03970017656683922\n",
      "Step = 47, train_loss: 0.03326169401407242, val_loss: 0.0392642617225647\n",
      "Step = 48, train_loss: 0.028928635641932487, val_loss: 0.03886067122220993\n",
      "Step = 49, train_loss: 0.03625945374369621, val_loss: 0.03888944536447525\n",
      "Step = 50, train_loss: 0.025995559990406036, val_loss: 0.03860071673989296\n",
      "Step = 51, train_loss: 0.022371141240000725, val_loss: 0.03807590901851654\n",
      "Step = 52, train_loss: 0.03195274621248245, val_loss: 0.03773880749940872\n",
      "Step = 53, train_loss: 0.03040204755961895, val_loss: 0.03724151849746704\n",
      "Step = 54, train_loss: 0.028095731511712074, val_loss: 0.03663995489478111\n",
      "Step = 55, train_loss: 0.028134070336818695, val_loss: 0.036191921681165695\n",
      "Step = 56, train_loss: 0.03291518613696098, val_loss: 0.035722214728593826\n",
      "Step = 57, train_loss: 0.03653731569647789, val_loss: 0.035137027502059937\n",
      "Step = 58, train_loss: 0.025364799425005913, val_loss: 0.03439636901021004\n",
      "Step = 59, train_loss: 0.02872508019208908, val_loss: 0.033388394862413406\n",
      "Step = 60, train_loss: 0.03058432601392269, val_loss: 0.031663887202739716\n",
      "Step = 61, train_loss: 0.0240201223641634, val_loss: 0.028906388208270073\n",
      "Step = 62, train_loss: 0.02673419937491417, val_loss: 0.026254162192344666\n",
      "Step = 63, train_loss: 0.02606966346502304, val_loss: 0.02392301708459854\n",
      "Step = 64, train_loss: 0.022652480751276016, val_loss: 0.02364591509103775\n",
      "Step = 65, train_loss: 0.02656709961593151, val_loss: 0.026186032220721245\n",
      "Step = 66, train_loss: 0.028232354670763016, val_loss: 0.026092778891324997\n",
      "Step = 67, train_loss: 0.03283649682998657, val_loss: 0.024144142866134644\n",
      "Step = 68, train_loss: 0.028355658054351807, val_loss: 0.02157832309603691\n",
      "Step = 69, train_loss: 0.027189088985323906, val_loss: 0.020304802805185318\n",
      "Step = 70, train_loss: 0.030564241111278534, val_loss: 0.01992393471300602\n",
      "Step = 71, train_loss: 0.025668207556009293, val_loss: 0.02126123197376728\n",
      "Step = 72, train_loss: 0.028489023447036743, val_loss: 0.021565821021795273\n",
      "Step = 73, train_loss: 0.02465907670557499, val_loss: 0.021826520562171936\n",
      "Step = 74, train_loss: 0.022777076810598373, val_loss: 0.030688175931572914\n",
      "Step = 75, train_loss: 0.03078482672572136, val_loss: 0.033108778297901154\n",
      "Step = 76, train_loss: 0.03303149715065956, val_loss: 0.0353284515440464\n",
      "Step = 77, train_loss: 0.02703326940536499, val_loss: 0.0376460999250412\n",
      "Step = 78, train_loss: 0.030825955793261528, val_loss: 0.0398828461766243\n",
      "Step = 79, train_loss: 0.02584921382367611, val_loss: 0.041063226759433746\n",
      "Step = 80, train_loss: 0.033095620572566986, val_loss: 0.04129066690802574\n",
      "Step = 81, train_loss: 0.03057563118636608, val_loss: 0.040373317897319794\n",
      "Step = 82, train_loss: 0.02458622120320797, val_loss: 0.03852237015962601\n",
      "Step = 83, train_loss: 0.027631355449557304, val_loss: 0.033617131412029266\n",
      "Step = 84, train_loss: 0.02656605839729309, val_loss: 0.021949253976345062\n",
      "Step = 85, train_loss: 0.023033495992422104, val_loss: 0.019908199086785316\n",
      "Step = 86, train_loss: 0.020899921655654907, val_loss: 0.019133061170578003\n",
      "Step = 87, train_loss: 0.01587723381817341, val_loss: 0.01911461167037487\n",
      "Step = 88, train_loss: 0.01936786063015461, val_loss: 0.02002277970314026\n",
      "Step = 89, train_loss: 0.02184893749654293, val_loss: 0.022750159725546837\n",
      "Step = 90, train_loss: 0.015363600105047226, val_loss: 0.029054176062345505\n",
      "Step = 91, train_loss: 0.02622115984559059, val_loss: 0.02892332524061203\n",
      "Step = 92, train_loss: 0.03140569478273392, val_loss: 0.028484094887971878\n",
      "Step = 93, train_loss: 0.027608772739768028, val_loss: 0.02825913578271866\n",
      "Step = 94, train_loss: 0.030532250180840492, val_loss: 0.02823183685541153\n",
      "Step = 95, train_loss: 0.02898583933711052, val_loss: 0.028083650395274162\n",
      "Step = 96, train_loss: 0.028892330825328827, val_loss: 0.027895087376236916\n",
      "Step = 97, train_loss: 0.030458228662610054, val_loss: 0.027913646772503853\n",
      "Step = 98, train_loss: 0.02292964793741703, val_loss: 0.0280633382499218\n",
      "Step = 99, train_loss: 0.030496157705783844, val_loss: 0.027636144310235977\n",
      "Step = 100, train_loss: 0.030060220509767532, val_loss: 0.026734191924333572\n",
      "Step = 101, train_loss: 0.02696821093559265, val_loss: 0.025619274005293846\n",
      "Step = 102, train_loss: 0.021542545408010483, val_loss: 0.024362308904528618\n",
      "Step = 103, train_loss: 0.026060044765472412, val_loss: 0.023175310343503952\n",
      "Step = 104, train_loss: 0.026152005419135094, val_loss: 0.02220899611711502\n",
      "Step = 105, train_loss: 0.026758860796689987, val_loss: 0.021765001118183136\n",
      "Step = 106, train_loss: 0.02263818308711052, val_loss: 0.02180551178753376\n",
      "Step = 107, train_loss: 0.0375731959939003, val_loss: 0.022728433832526207\n",
      "Step = 108, train_loss: 0.03581386059522629, val_loss: 0.02353995107114315\n",
      "Step = 109, train_loss: 0.02311510592699051, val_loss: 0.02441752329468727\n",
      "Step = 110, train_loss: 0.02543461136519909, val_loss: 0.025091171264648438\n",
      "Step = 111, train_loss: 0.023764289915561676, val_loss: 0.025796158239245415\n",
      "Step = 112, train_loss: 0.030595894902944565, val_loss: 0.026593850925564766\n",
      "Step = 113, train_loss: 0.02681729570031166, val_loss: 0.027535630390048027\n",
      "Step = 114, train_loss: 0.025408057495951653, val_loss: 0.02819811925292015\n",
      "Step = 115, train_loss: 0.02824723720550537, val_loss: 0.028559638187289238\n",
      "Step = 116, train_loss: 0.029405875131487846, val_loss: 0.02854195237159729\n",
      "Step = 117, train_loss: 0.029640866443514824, val_loss: 0.02828815206885338\n",
      "Step = 118, train_loss: 0.029983697459101677, val_loss: 0.027818849310278893\n",
      "Step = 119, train_loss: 0.02659158781170845, val_loss: 0.027202874422073364\n",
      "Step = 120, train_loss: 0.023477550595998764, val_loss: 0.026440605521202087\n",
      "Step = 121, train_loss: 0.029233982786536217, val_loss: 0.02587403915822506\n",
      "Step = 122, train_loss: 0.028311921283602715, val_loss: 0.025729568675160408\n",
      "Step = 123, train_loss: 0.031292106956243515, val_loss: 0.026277925819158554\n",
      "Step = 124, train_loss: 0.02796359919011593, val_loss: 0.028706468641757965\n",
      "Step = 125, train_loss: 0.024569449946284294, val_loss: 0.032121751457452774\n",
      "Step = 126, train_loss: 0.024484962224960327, val_loss: 0.04018491134047508\n",
      "Step = 127, train_loss: 0.028806356713175774, val_loss: 0.04824557900428772\n",
      "Step = 128, train_loss: 0.033935822546482086, val_loss: 0.05389215424656868\n",
      "Step = 129, train_loss: 0.020754648372530937, val_loss: 0.05618629977107048\n",
      "Step = 130, train_loss: 0.02969781495630741, val_loss: 0.05675569921731949\n",
      "Step = 131, train_loss: 0.028384080156683922, val_loss: 0.057522911578416824\n",
      "Step = 132, train_loss: 0.027193721383810043, val_loss: 0.05845891684293747\n",
      "Step = 133, train_loss: 0.03195038437843323, val_loss: 0.056419964879751205\n",
      "Step = 134, train_loss: 0.022850750014185905, val_loss: 0.04000372812151909\n",
      "Step = 135, train_loss: 0.027050040662288666, val_loss: 0.02303854376077652\n",
      "Step = 136, train_loss: 0.02516677789390087, val_loss: 0.01816536672413349\n",
      "Step = 137, train_loss: 0.026943495497107506, val_loss: 0.018391050398349762\n",
      "Step = 138, train_loss: 0.02311527542769909, val_loss: 0.019657766446471214\n",
      "Step = 139, train_loss: 0.023611674085259438, val_loss: 0.019927075132727623\n",
      "Step = 140, train_loss: 0.024228163063526154, val_loss: 0.020520908758044243\n",
      "Step = 141, train_loss: 0.02316819317638874, val_loss: 0.021204277873039246\n",
      "Step = 142, train_loss: 0.023902561515569687, val_loss: 0.02160017006099224\n",
      "Step = 143, train_loss: 0.020028848201036453, val_loss: 0.021626383066177368\n",
      "Step = 144, train_loss: 0.02282695658504963, val_loss: 0.022054564207792282\n",
      "Step = 145, train_loss: 0.024879654869437218, val_loss: 0.018354834988713264\n",
      "Step = 146, train_loss: 0.025676680728793144, val_loss: 0.036339037120342255\n",
      "Step = 147, train_loss: 0.02107367105782032, val_loss: 0.05273297801613808\n",
      "Step = 148, train_loss: 0.02350427955389023, val_loss: 0.0581703819334507\n",
      "Step = 149, train_loss: 0.02689730003476143, val_loss: 0.0597916916012764\n",
      "Step = 150, train_loss: 0.016915442422032356, val_loss: 0.060109853744506836\n",
      "Step = 151, train_loss: 0.02198963612318039, val_loss: 0.05768021568655968\n",
      "Step = 152, train_loss: 0.016445470973849297, val_loss: 0.05451347678899765\n",
      "Step = 153, train_loss: 0.030354861170053482, val_loss: 0.05862027406692505\n",
      "Step = 154, train_loss: 0.02191382460296154, val_loss: 0.05960865691304207\n",
      "Step = 155, train_loss: 0.018502458930015564, val_loss: 0.060016509145498276\n",
      "Step = 156, train_loss: 0.017060570418834686, val_loss: 0.06055334210395813\n",
      "Step = 157, train_loss: 0.025299808010458946, val_loss: 0.06058119982481003\n",
      "Step = 158, train_loss: 0.021343829110264778, val_loss: 0.060034945607185364\n",
      "Step = 159, train_loss: 0.025340668857097626, val_loss: 0.058667320758104324\n",
      "Step = 160, train_loss: 0.019997816532850266, val_loss: 0.05649430677294731\n",
      "Step = 161, train_loss: 0.0226039569824934, val_loss: 0.05315380543470383\n",
      "Step = 162, train_loss: 0.016612056642770767, val_loss: 0.05312899127602577\n",
      "Step = 163, train_loss: 0.02815340831875801, val_loss: 0.05198564752936363\n",
      "Step = 164, train_loss: 0.02560419775545597, val_loss: 0.049992479383945465\n",
      "Step = 165, train_loss: 0.029250148683786392, val_loss: 0.04956376180052757\n",
      "Step = 166, train_loss: 0.025653578341007233, val_loss: 0.050248581916093826\n",
      "Step = 167, train_loss: 0.02697899006307125, val_loss: 0.04547674208879471\n",
      "Step = 168, train_loss: 0.023137224838137627, val_loss: 0.03172549977898598\n",
      "Step = 169, train_loss: 0.02097776159644127, val_loss: 0.02417331002652645\n",
      "Step = 170, train_loss: 0.023059172555804253, val_loss: 0.02365812659263611\n",
      "Step = 171, train_loss: 0.022990910336375237, val_loss: 0.022926246747374535\n",
      "Step = 172, train_loss: 0.02799755148589611, val_loss: 0.022062884643673897\n",
      "Step = 173, train_loss: 0.02439984492957592, val_loss: 0.022115053609013557\n",
      "Step = 174, train_loss: 0.024246325716376305, val_loss: 0.024336064234375954\n",
      "Step = 175, train_loss: 0.026155024766921997, val_loss: 0.022316010668873787\n",
      "Step = 176, train_loss: 0.019690124318003654, val_loss: 0.024259405210614204\n",
      "Step = 177, train_loss: 0.017643911764025688, val_loss: 0.02777339704334736\n",
      "Step = 178, train_loss: 0.02056722156703472, val_loss: 0.025342825800180435\n",
      "Step = 179, train_loss: 0.01640206016600132, val_loss: 0.032739344984292984\n",
      "Step = 180, train_loss: 0.021148469299077988, val_loss: 0.03235127031803131\n",
      "Step = 181, train_loss: 0.01791946403682232, val_loss: 0.022522469982504845\n",
      "Step = 182, train_loss: 0.01725117489695549, val_loss: 0.021816512569785118\n",
      "Step = 183, train_loss: 0.022109268233180046, val_loss: 0.035505350679159164\n",
      "Step = 184, train_loss: 0.019555827602744102, val_loss: 0.04104483872652054\n",
      "Step = 185, train_loss: 0.02123459428548813, val_loss: 0.034704744815826416\n",
      "Step = 186, train_loss: 0.02183716371655464, val_loss: 0.01636205054819584\n",
      "Step = 187, train_loss: 0.015030150301754475, val_loss: 0.015497399494051933\n",
      "Step = 188, train_loss: 0.01693374663591385, val_loss: 0.0167769156396389\n",
      "Step = 189, train_loss: 0.027011502534151077, val_loss: 0.013847327791154385\n",
      "Step = 190, train_loss: 0.02042228728532791, val_loss: 0.027888422831892967\n",
      "Step = 191, train_loss: 0.01829521730542183, val_loss: 0.03954995423555374\n",
      "Step = 192, train_loss: 0.026693593710660934, val_loss: 0.044166263192892075\n",
      "Step = 193, train_loss: 0.021976273506879807, val_loss: 0.045091383159160614\n",
      "Step = 194, train_loss: 0.021478623151779175, val_loss: 0.045482534915208817\n",
      "Step = 195, train_loss: 0.016893945634365082, val_loss: 0.04468775540590286\n",
      "Step = 196, train_loss: 0.02221771702170372, val_loss: 0.042508356273174286\n",
      "Step = 197, train_loss: 0.018038304522633553, val_loss: 0.037425696849823\n",
      "Step = 198, train_loss: 0.018257075920701027, val_loss: 0.028236865997314453\n",
      "Step = 199, train_loss: 0.0191703699529171, val_loss: 0.017852965742349625\n",
      "Step = 200, train_loss: 0.023978233337402344, val_loss: 0.011251145973801613\n",
      "Step = 201, train_loss: 0.015263459645211697, val_loss: 0.01662316918373108\n",
      "Step = 202, train_loss: 0.015583912841975689, val_loss: 0.021609792485833168\n",
      "Step = 203, train_loss: 0.020939379930496216, val_loss: 0.023967165499925613\n",
      "Step = 204, train_loss: 0.01697920635342598, val_loss: 0.02447904832661152\n",
      "Step = 205, train_loss: 0.013852051459252834, val_loss: 0.022287940606474876\n",
      "Step = 206, train_loss: 0.018985599279403687, val_loss: 0.018011469393968582\n",
      "Step = 207, train_loss: 0.017033187672495842, val_loss: 0.012494655326008797\n",
      "Step = 208, train_loss: 0.01549789123237133, val_loss: 0.01290312223136425\n",
      "Step = 209, train_loss: 0.01836308278143406, val_loss: 0.01387860532850027\n",
      "Step = 210, train_loss: 0.013477463275194168, val_loss: 0.014196398667991161\n",
      "Step = 211, train_loss: 0.015131289139389992, val_loss: 0.01200736965984106\n",
      "Step = 212, train_loss: 0.015093671157956123, val_loss: 0.01206346694380045\n",
      "Step = 213, train_loss: 0.014284162782132626, val_loss: 0.01205840427428484\n",
      "Step = 214, train_loss: 0.015460199676454067, val_loss: 0.014865318313241005\n",
      "Step = 215, train_loss: 0.017983853816986084, val_loss: 0.03657048940658569\n",
      "Step = 216, train_loss: 0.02115565538406372, val_loss: 0.04317955672740936\n",
      "Step = 217, train_loss: 0.017020732164382935, val_loss: 0.03599356859922409\n",
      "Step = 218, train_loss: 0.01395993772894144, val_loss: 0.024696558713912964\n",
      "Step = 219, train_loss: 0.015397368930280209, val_loss: 0.015763146802783012\n",
      "Step = 220, train_loss: 0.017265940085053444, val_loss: 0.009331168606877327\n",
      "Step = 221, train_loss: 0.01327640749514103, val_loss: 0.010134479030966759\n",
      "Step = 222, train_loss: 0.014648026786744595, val_loss: 0.009375769644975662\n",
      "Step = 223, train_loss: 0.014983322471380234, val_loss: 0.008591941557824612\n",
      "Step = 224, train_loss: 0.014417462982237339, val_loss: 0.011848237365484238\n",
      "Step = 225, train_loss: 0.013770321384072304, val_loss: 0.012431743554770947\n",
      "Step = 226, train_loss: 0.015745287761092186, val_loss: 0.009035629220306873\n",
      "Step = 227, train_loss: 0.01362560037523508, val_loss: 0.01921115443110466\n",
      "Step = 228, train_loss: 0.018798965960741043, val_loss: 0.0252147875726223\n",
      "Step = 229, train_loss: 0.014984340406954288, val_loss: 0.032110124826431274\n",
      "Step = 230, train_loss: 0.01541595533490181, val_loss: 0.03778635710477829\n",
      "Step = 231, train_loss: 0.013723362237215042, val_loss: 0.038355354219675064\n",
      "Step = 232, train_loss: 0.017830226570367813, val_loss: 0.04005242884159088\n",
      "Step = 233, train_loss: 0.014340140856802464, val_loss: 0.039684344083070755\n",
      "Step = 234, train_loss: 0.015163639560341835, val_loss: 0.03325699269771576\n",
      "Step = 235, train_loss: 0.013811876997351646, val_loss: 0.022880377247929573\n",
      "Step = 236, train_loss: 0.013041012920439243, val_loss: 0.012974313460290432\n",
      "Step = 237, train_loss: 0.01322266273200512, val_loss: 0.007714840583503246\n",
      "Step = 238, train_loss: 0.0158770140260458, val_loss: 0.01497302670031786\n",
      "Step = 239, train_loss: 0.015413329005241394, val_loss: 0.021860089153051376\n",
      "Step = 240, train_loss: 0.016435420140624046, val_loss: 0.019771631807088852\n",
      "Step = 241, train_loss: 0.01651429384946823, val_loss: 0.010680167004466057\n",
      "Step = 242, train_loss: 0.016683358699083328, val_loss: 0.01680988073348999\n",
      "Step = 243, train_loss: 0.011617978103458881, val_loss: 0.03890635818243027\n",
      "Step = 244, train_loss: 0.014215572737157345, val_loss: 0.05320268124341965\n",
      "Step = 245, train_loss: 0.014970283024013042, val_loss: 0.05696939304471016\n",
      "Step = 246, train_loss: 0.01634218543767929, val_loss: 0.05592010170221329\n",
      "Step = 247, train_loss: 0.015839867293834686, val_loss: 0.046970292925834656\n",
      "Step = 248, train_loss: 0.015035524033010006, val_loss: 0.018799666315317154\n",
      "Step = 249, train_loss: 0.012968902476131916, val_loss: 0.00852307491004467\n",
      "Step = 250, train_loss: 0.013452554121613503, val_loss: 0.022700591012835503\n",
      "Step = 251, train_loss: 0.01563423126935959, val_loss: 0.024558020755648613\n",
      "Step = 252, train_loss: 0.01748080365359783, val_loss: 0.012683388777077198\n",
      "Step = 253, train_loss: 0.013539622537791729, val_loss: 0.0262177512049675\n",
      "Step = 254, train_loss: 0.0164653267711401, val_loss: 0.05432058498263359\n",
      "Step = 255, train_loss: 0.018954334780573845, val_loss: 0.058560166507959366\n",
      "Step = 256, train_loss: 0.014534380286931992, val_loss: 0.05918079614639282\n",
      "Step = 257, train_loss: 0.015714282169938087, val_loss: 0.057986799627542496\n",
      "Step = 258, train_loss: 0.0157465860247612, val_loss: 0.048377227038145065\n",
      "Step = 259, train_loss: 0.015179367735981941, val_loss: 0.021945415064692497\n",
      "Step = 260, train_loss: 0.016469759866595268, val_loss: 0.0071559506468474865\n",
      "Step = 261, train_loss: 0.013985617086291313, val_loss: 0.018442312255501747\n",
      "Step = 262, train_loss: 0.014121601358056068, val_loss: 0.027742775157094002\n",
      "Step = 263, train_loss: 0.014398888684809208, val_loss: 0.02742324024438858\n",
      "Step = 264, train_loss: 0.016451451927423477, val_loss: 0.022357212379574776\n",
      "Step = 265, train_loss: 0.013200507499277592, val_loss: 0.009072369895875454\n",
      "Step = 266, train_loss: 0.014241259545087814, val_loss: 0.019908569753170013\n",
      "Step = 267, train_loss: 0.015885742381215096, val_loss: 0.03880462795495987\n",
      "Step = 268, train_loss: 0.013938470743596554, val_loss: 0.04681427776813507\n",
      "Step = 269, train_loss: 0.015268568880856037, val_loss: 0.04906175658106804\n",
      "Step = 270, train_loss: 0.014440592378377914, val_loss: 0.039102375507354736\n",
      "Step = 271, train_loss: 0.017317991703748703, val_loss: 0.028025668114423752\n",
      "Step = 272, train_loss: 0.014255518093705177, val_loss: 0.013813795521855354\n",
      "Step = 273, train_loss: 0.012451648712158203, val_loss: 0.007394797168672085\n",
      "Step = 274, train_loss: 0.012512310408055782, val_loss: 0.018424414098262787\n",
      "Step = 275, train_loss: 0.013847459107637405, val_loss: 0.026518428698182106\n",
      "Step = 276, train_loss: 0.014811618253588676, val_loss: 0.01863371580839157\n",
      "Step = 277, train_loss: 0.014986855909228325, val_loss: 0.007726787589490414\n",
      "Step = 278, train_loss: 0.01456401776522398, val_loss: 0.007545726839452982\n",
      "Step = 279, train_loss: 0.014313455671072006, val_loss: 0.008964641019701958\n",
      "Step = 280, train_loss: 0.014089341275393963, val_loss: 0.00888060312718153\n",
      "Step = 281, train_loss: 0.015087575651705265, val_loss: 0.00703441072255373\n",
      "Step = 282, train_loss: 0.01187900360673666, val_loss: 0.009954643435776234\n",
      "Step = 283, train_loss: 0.015947606414556503, val_loss: 0.012827768921852112\n",
      "Step = 284, train_loss: 0.013231515884399414, val_loss: 0.011384385637938976\n",
      "Step = 285, train_loss: 0.014461347833275795, val_loss: 0.007276936899870634\n",
      "Step = 286, train_loss: 0.01296605821698904, val_loss: 0.01128734927624464\n",
      "Step = 287, train_loss: 0.013865772634744644, val_loss: 0.014742376282811165\n",
      "Step = 288, train_loss: 0.01393984630703926, val_loss: 0.011739524081349373\n",
      "Step = 289, train_loss: 0.01278412714600563, val_loss: 0.00784579012542963\n",
      "Step = 290, train_loss: 0.013043283484876156, val_loss: 0.008068384602665901\n",
      "Step = 291, train_loss: 0.014507818967103958, val_loss: 0.007601426914334297\n",
      "Step = 292, train_loss: 0.014028086327016354, val_loss: 0.009565569460391998\n",
      "Step = 293, train_loss: 0.01317285094410181, val_loss: 0.008126871660351753\n",
      "Step = 294, train_loss: 0.014543549157679081, val_loss: 0.008480517193675041\n",
      "Step = 295, train_loss: 0.013115201145410538, val_loss: 0.01570383831858635\n",
      "Step = 296, train_loss: 0.013425316661596298, val_loss: 0.010058803483843803\n",
      "Step = 297, train_loss: 0.011156667955219746, val_loss: 0.008359757252037525\n",
      "Step = 298, train_loss: 0.011727734468877316, val_loss: 0.014034286141395569\n",
      "Step = 299, train_loss: 0.01429358683526516, val_loss: 0.02447107434272766\n",
      "Step = 300, train_loss: 0.012739907950162888, val_loss: 0.018181541934609413\n",
      "Step = 301, train_loss: 0.016072489321231842, val_loss: 0.008781411685049534\n",
      "Step = 302, train_loss: 0.011138705536723137, val_loss: 0.007812761701643467\n",
      "Step = 303, train_loss: 0.013439124450087547, val_loss: 0.021076606586575508\n",
      "Step = 304, train_loss: 0.015324586071074009, val_loss: 0.016766147688031197\n",
      "Step = 305, train_loss: 0.015978125855326653, val_loss: 0.017433417961001396\n",
      "Step = 306, train_loss: 0.0151901850476861, val_loss: 0.056003883481025696\n",
      "Step = 307, train_loss: 0.014709128066897392, val_loss: 0.06177020072937012\n",
      "Step = 308, train_loss: 0.014836962334811687, val_loss: 0.0628793016076088\n",
      "Step = 309, train_loss: 0.01380887534469366, val_loss: 0.06320210546255112\n",
      "Step = 310, train_loss: 0.019524408504366875, val_loss: 0.06317431479692459\n",
      "Step = 311, train_loss: 0.01398612093180418, val_loss: 0.06273987144231796\n",
      "Step = 312, train_loss: 0.014384566806256771, val_loss: 0.06096272170543671\n",
      "Step = 313, train_loss: 0.0183700080960989, val_loss: 0.05023839324712753\n",
      "Step = 314, train_loss: 0.015544488094747066, val_loss: 0.028148958459496498\n",
      "Step = 315, train_loss: 0.017088454216718674, val_loss: 0.00867170188575983\n",
      "Step = 316, train_loss: 0.013096340931952, val_loss: 0.01358400471508503\n",
      "Step = 317, train_loss: 0.011456280015408993, val_loss: 0.02532973513007164\n",
      "Step = 318, train_loss: 0.012880776077508926, val_loss: 0.028287654742598534\n",
      "Step = 319, train_loss: 0.013372785411775112, val_loss: 0.02703886292874813\n",
      "Step = 320, train_loss: 0.01738782785832882, val_loss: 0.023595819249749184\n",
      "Step = 321, train_loss: 0.013022242113947868, val_loss: 0.014059866778552532\n",
      "Step = 322, train_loss: 0.014722606167197227, val_loss: 0.010648285038769245\n",
      "Step = 323, train_loss: 0.013999183662235737, val_loss: 0.022463014349341393\n",
      "Step = 324, train_loss: 0.01405179314315319, val_loss: 0.031180543825030327\n",
      "Step = 325, train_loss: 0.013234739191830158, val_loss: 0.037076644599437714\n",
      "Step = 326, train_loss: 0.015549864619970322, val_loss: 0.03341607376933098\n",
      "Step = 327, train_loss: 0.013790911994874477, val_loss: 0.027948131784796715\n",
      "Step = 328, train_loss: 0.014598256908357143, val_loss: 0.020535901188850403\n",
      "Step = 329, train_loss: 0.01423803623765707, val_loss: 0.012449342757463455\n",
      "Step = 330, train_loss: 0.014001000672578812, val_loss: 0.007518343161791563\n",
      "Step = 331, train_loss: 0.013480355963110924, val_loss: 0.00777917867526412\n",
      "Step = 332, train_loss: 0.01061258651316166, val_loss: 0.01213212963193655\n",
      "Step = 333, train_loss: 0.012540088035166264, val_loss: 0.01761067472398281\n",
      "Step = 334, train_loss: 0.013189509510993958, val_loss: 0.019125062972307205\n",
      "Step = 335, train_loss: 0.012052634730935097, val_loss: 0.01596173644065857\n",
      "Step = 336, train_loss: 0.015107314102351665, val_loss: 0.009380237199366093\n",
      "Step = 337, train_loss: 0.013135365210473537, val_loss: 0.008203777484595776\n",
      "Step = 338, train_loss: 0.013058849610388279, val_loss: 0.010009906254708767\n",
      "Step = 339, train_loss: 0.016361624002456665, val_loss: 0.010552133433520794\n",
      "Step = 340, train_loss: 0.0133932214230299, val_loss: 0.015442407689988613\n",
      "Step = 341, train_loss: 0.012437883764505386, val_loss: 0.016669759526848793\n",
      "Step = 342, train_loss: 0.014038817025721073, val_loss: 0.012036122381687164\n",
      "Step = 343, train_loss: 0.012065496295690536, val_loss: 0.00700751505792141\n",
      "Step = 344, train_loss: 0.012326731346547604, val_loss: 0.010535509325563908\n",
      "Step = 345, train_loss: 0.014497131109237671, val_loss: 0.015660494565963745\n",
      "Step = 346, train_loss: 0.012594437226653099, val_loss: 0.024901753291487694\n",
      "Step = 347, train_loss: 0.010912347584962845, val_loss: 0.03147219121456146\n",
      "Step = 348, train_loss: 0.015684571117162704, val_loss: 0.02763255126774311\n",
      "Step = 349, train_loss: 0.01499983761459589, val_loss: 0.028731726109981537\n",
      "Step = 350, train_loss: 0.014199348166584969, val_loss: 0.02063416875898838\n",
      "Step = 351, train_loss: 0.014386334456503391, val_loss: 0.015888968482613564\n",
      "Step = 352, train_loss: 0.014058991335332394, val_loss: 0.008270762860774994\n",
      "Step = 353, train_loss: 0.015884103253483772, val_loss: 0.012046032585203648\n",
      "Step = 354, train_loss: 0.014802172780036926, val_loss: 0.018944524228572845\n",
      "Step = 355, train_loss: 0.012645451352000237, val_loss: 0.022751275449991226\n",
      "Step = 356, train_loss: 0.012860988266766071, val_loss: 0.022827856242656708\n",
      "Step = 357, train_loss: 0.018651489168405533, val_loss: 0.012663322500884533\n",
      "Step = 358, train_loss: 0.012466659769415855, val_loss: 0.008778753690421581\n",
      "Step = 359, train_loss: 0.011831220239400864, val_loss: 0.024536283686757088\n",
      "Step = 360, train_loss: 0.012740029022097588, val_loss: 0.03915690630674362\n",
      "Step = 361, train_loss: 0.014074374921619892, val_loss: 0.04382326453924179\n",
      "Step = 362, train_loss: 0.013956304639577866, val_loss: 0.04563441500067711\n",
      "Step = 363, train_loss: 0.014731321483850479, val_loss: 0.044040266424417496\n",
      "Step = 364, train_loss: 0.015758143737912178, val_loss: 0.03507611155509949\n",
      "Step = 365, train_loss: 0.014864952303469181, val_loss: 0.023023046553134918\n",
      "Step = 366, train_loss: 0.012970035895705223, val_loss: 0.013436070643365383\n",
      "Step = 367, train_loss: 0.013960414566099644, val_loss: 0.007567607797682285\n",
      "Step = 368, train_loss: 0.011942722834646702, val_loss: 0.010480006225407124\n",
      "Step = 369, train_loss: 0.013910866342484951, val_loss: 0.016433658078312874\n",
      "Step = 370, train_loss: 0.01647927053272724, val_loss: 0.01598450168967247\n",
      "Step = 371, train_loss: 0.01369515061378479, val_loss: 0.008507173508405685\n",
      "Step = 372, train_loss: 0.011962536722421646, val_loss: 0.006988643668591976\n",
      "Step = 373, train_loss: 0.013446266762912273, val_loss: 0.015420403331518173\n",
      "Step = 374, train_loss: 0.012570683844387531, val_loss: 0.02483813650906086\n",
      "Step = 375, train_loss: 0.014046307653188705, val_loss: 0.02811586484313011\n",
      "Step = 376, train_loss: 0.014952649362385273, val_loss: 0.037993770092725754\n",
      "Step = 377, train_loss: 0.013403127901256084, val_loss: 0.040517792105674744\n",
      "Step = 378, train_loss: 0.013484016992151737, val_loss: 0.036816537380218506\n",
      "Step = 379, train_loss: 0.011688858270645142, val_loss: 0.03010084293782711\n",
      "Step = 380, train_loss: 0.01283804140985012, val_loss: 0.018544554710388184\n",
      "Step = 381, train_loss: 0.014027992263436317, val_loss: 0.010150228627026081\n",
      "Step = 382, train_loss: 0.012154855765402317, val_loss: 0.0072581572458148\n",
      "Step = 383, train_loss: 0.01107180118560791, val_loss: 0.012942997738718987\n",
      "Step = 384, train_loss: 0.013060078024864197, val_loss: 0.01830274984240532\n",
      "Step = 385, train_loss: 0.01315321959555149, val_loss: 0.01730332151055336\n",
      "Step = 386, train_loss: 0.014844045042991638, val_loss: 0.00746501749381423\n",
      "Step = 387, train_loss: 0.014263179153203964, val_loss: 0.016690809279680252\n",
      "Step = 388, train_loss: 0.012016624212265015, val_loss: 0.04236292093992233\n",
      "Step = 389, train_loss: 0.013449841178953648, val_loss: 0.05673085153102875\n",
      "Step = 390, train_loss: 0.014888009056448936, val_loss: 0.05931547284126282\n",
      "Step = 391, train_loss: 0.011287369765341282, val_loss: 0.06053644418716431\n",
      "Step = 392, train_loss: 0.015625547617673874, val_loss: 0.06032701954245567\n",
      "Step = 393, train_loss: 0.011807108297944069, val_loss: 0.059791043400764465\n",
      "Step = 394, train_loss: 0.015432626008987427, val_loss: 0.056343067437410355\n",
      "Step = 395, train_loss: 0.013458715751767159, val_loss: 0.046710073947906494\n",
      "Step = 396, train_loss: 0.015305828303098679, val_loss: 0.031761351972818375\n",
      "Step = 397, train_loss: 0.013913249596953392, val_loss: 0.018415097147226334\n",
      "Step = 398, train_loss: 0.013683849945664406, val_loss: 0.008111733943223953\n",
      "Step = 399, train_loss: 0.013914506882429123, val_loss: 0.00908820703625679\n",
      "Step = 400, train_loss: 0.013882934115827084, val_loss: 0.017194388434290886\n",
      "Step = 401, train_loss: 0.015096014365553856, val_loss: 0.025816170498728752\n",
      "Step = 402, train_loss: 0.01156360562890768, val_loss: 0.028662940487265587\n",
      "Step = 403, train_loss: 0.011671612039208412, val_loss: 0.028648240491747856\n",
      "Step = 404, train_loss: 0.01507890410721302, val_loss: 0.027610132470726967\n",
      "Step = 405, train_loss: 0.011184882372617722, val_loss: 0.02601032517850399\n",
      "Step = 406, train_loss: 0.012073218822479248, val_loss: 0.021677762269973755\n",
      "Step = 407, train_loss: 0.012741835787892342, val_loss: 0.009867202490568161\n",
      "Step = 408, train_loss: 0.013015290722250938, val_loss: 0.007088318932801485\n",
      "Step = 409, train_loss: 0.013644607737660408, val_loss: 0.01424349844455719\n",
      "Step = 410, train_loss: 0.013692399486899376, val_loss: 0.023564375936985016\n",
      "Step = 411, train_loss: 0.013082941994071007, val_loss: 0.029917297884821892\n",
      "Step = 412, train_loss: 0.01235798466950655, val_loss: 0.030275991186499596\n",
      "Step = 413, train_loss: 0.014965021051466465, val_loss: 0.021524576470255852\n",
      "Step = 414, train_loss: 0.013483015820384026, val_loss: 0.014809858053922653\n",
      "Step = 415, train_loss: 0.013968419283628464, val_loss: 0.009661784395575523\n",
      "Step = 416, train_loss: 0.012736939825117588, val_loss: 0.0075004687532782555\n",
      "Step = 417, train_loss: 0.012926421128213406, val_loss: 0.007279244251549244\n",
      "Step = 418, train_loss: 0.012953140772879124, val_loss: 0.010140919126570225\n",
      "Step = 419, train_loss: 0.01307325903326273, val_loss: 0.013184189796447754\n",
      "Step = 420, train_loss: 0.013040110468864441, val_loss: 0.014856047928333282\n",
      "Step = 421, train_loss: 0.014205972664058208, val_loss: 0.017834089696407318\n",
      "Step = 422, train_loss: 0.01227988488972187, val_loss: 0.019155383110046387\n",
      "Step = 423, train_loss: 0.013506900519132614, val_loss: 0.020243581384420395\n",
      "Step = 424, train_loss: 0.01244272943586111, val_loss: 0.015192444436252117\n",
      "Step = 425, train_loss: 0.01478369627147913, val_loss: 0.0070320540107786655\n",
      "Step = 426, train_loss: 0.013851220719516277, val_loss: 0.011121693067252636\n",
      "Step = 427, train_loss: 0.013134103268384933, val_loss: 0.018767202273011208\n",
      "Step = 428, train_loss: 0.011537796817719936, val_loss: 0.025990808382630348\n",
      "Step = 429, train_loss: 0.011898042634129524, val_loss: 0.029758375138044357\n",
      "Step = 430, train_loss: 0.013488597236573696, val_loss: 0.03241125866770744\n",
      "Step = 431, train_loss: 0.012852216139435768, val_loss: 0.03319806605577469\n",
      "Step = 432, train_loss: 0.013681212440133095, val_loss: 0.029486771672964096\n",
      "Step = 433, train_loss: 0.012919103726744652, val_loss: 0.02206953801214695\n",
      "Step = 434, train_loss: 0.015096860006451607, val_loss: 0.014195086434483528\n",
      "Step = 435, train_loss: 0.012697397731244564, val_loss: 0.00849339459091425\n",
      "Step = 436, train_loss: 0.014011496677994728, val_loss: 0.007745306473225355\n",
      "Step = 437, train_loss: 0.012372748926281929, val_loss: 0.01516715157777071\n",
      "Step = 438, train_loss: 0.012213725596666336, val_loss: 0.017117561772465706\n",
      "Step = 439, train_loss: 0.014311826787889004, val_loss: 0.019074300304055214\n",
      "Step = 440, train_loss: 0.01497657224535942, val_loss: 0.01891711726784706\n",
      "Step = 441, train_loss: 0.015482047572731972, val_loss: 0.017912164330482483\n",
      "Step = 442, train_loss: 0.01207839883863926, val_loss: 0.015866214409470558\n",
      "Step = 443, train_loss: 0.012065001763403416, val_loss: 0.011876001954078674\n",
      "Step = 444, train_loss: 0.012844627723097801, val_loss: 0.00826699286699295\n",
      "Step = 445, train_loss: 0.012950761243700981, val_loss: 0.007665869779884815\n",
      "Step = 446, train_loss: 0.013144210912287235, val_loss: 0.007045482750982046\n",
      "Step = 447, train_loss: 0.012407422997057438, val_loss: 0.007169095333665609\n",
      "Step = 448, train_loss: 0.012513736262917519, val_loss: 0.007765347138047218\n",
      "Step = 449, train_loss: 0.012140478007495403, val_loss: 0.008303607814013958\n",
      "Step = 450, train_loss: 0.013002607971429825, val_loss: 0.00769719248637557\n",
      "Step = 451, train_loss: 0.011453866958618164, val_loss: 0.007169661577790976\n",
      "Step = 452, train_loss: 0.0132888313382864, val_loss: 0.00830648373812437\n",
      "Step = 453, train_loss: 0.010901651345193386, val_loss: 0.009725423529744148\n",
      "Step = 454, train_loss: 0.01104144286364317, val_loss: 0.014388344250619411\n",
      "Step = 455, train_loss: 0.011988872662186623, val_loss: 0.01482744887471199\n",
      "Step = 456, train_loss: 0.013993827626109123, val_loss: 0.010739661753177643\n",
      "Step = 457, train_loss: 0.01234168279916048, val_loss: 0.010284975171089172\n",
      "Step = 458, train_loss: 0.011566784232854843, val_loss: 0.00803279411047697\n",
      "Step = 459, train_loss: 0.01486886851489544, val_loss: 0.007233855780214071\n",
      "Step = 460, train_loss: 0.013252174481749535, val_loss: 0.009781577624380589\n",
      "Step = 461, train_loss: 0.012875312007963657, val_loss: 0.017792023718357086\n",
      "Step = 462, train_loss: 0.011584466323256493, val_loss: 0.02403264306485653\n",
      "Step = 463, train_loss: 0.014491855166852474, val_loss: 0.025433938950300217\n",
      "Step = 464, train_loss: 0.013275002129375935, val_loss: 0.01841532438993454\n",
      "Step = 465, train_loss: 0.013640538789331913, val_loss: 0.01302212942391634\n",
      "Step = 466, train_loss: 0.013088202103972435, val_loss: 0.008314766921103\n",
      "Step = 467, train_loss: 0.013962049037218094, val_loss: 0.007088955026119947\n",
      "Step = 468, train_loss: 0.015052814967930317, val_loss: 0.008423433639109135\n",
      "Step = 469, train_loss: 0.01524762436747551, val_loss: 0.014013850130140781\n",
      "Step = 470, train_loss: 0.012224981561303139, val_loss: 0.014658399857580662\n",
      "Step = 471, train_loss: 0.012219675816595554, val_loss: 0.014728245325386524\n",
      "Step = 472, train_loss: 0.013920631259679794, val_loss: 0.009977530688047409\n",
      "Step = 473, train_loss: 0.011859332211315632, val_loss: 0.0071129826828837395\n",
      "Step = 474, train_loss: 0.011845399625599384, val_loss: 0.007281591184437275\n",
      "Step = 475, train_loss: 0.011545930989086628, val_loss: 0.009607032872736454\n",
      "Step = 476, train_loss: 0.013951563276350498, val_loss: 0.008462158963084221\n",
      "Step = 477, train_loss: 0.013227657414972782, val_loss: 0.008288491517305374\n",
      "Step = 478, train_loss: 0.012129945680499077, val_loss: 0.00709164934232831\n",
      "Step = 479, train_loss: 0.014423339627683163, val_loss: 0.007120505906641483\n",
      "Step = 480, train_loss: 0.012268397957086563, val_loss: 0.007076555863022804\n",
      "Step = 481, train_loss: 0.013063715770840645, val_loss: 0.007510082796216011\n",
      "Step = 482, train_loss: 0.011389484629034996, val_loss: 0.009020617231726646\n",
      "Step = 483, train_loss: 0.011664578691124916, val_loss: 0.01049023400992155\n",
      "Step = 484, train_loss: 0.012492652982473373, val_loss: 0.008879352360963821\n",
      "Step = 485, train_loss: 0.013546084985136986, val_loss: 0.007215820252895355\n",
      "Step = 486, train_loss: 0.011628678999841213, val_loss: 0.007207172457128763\n",
      "Step = 487, train_loss: 0.01239018328487873, val_loss: 0.00930844247341156\n",
      "Step = 488, train_loss: 0.01188652403652668, val_loss: 0.012489035725593567\n",
      "Step = 489, train_loss: 0.012704214081168175, val_loss: 0.012637182138860226\n",
      "Step = 490, train_loss: 0.012956300750374794, val_loss: 0.00937347300350666\n",
      "Step = 491, train_loss: 0.01359112560749054, val_loss: 0.007000415585935116\n",
      "Step = 492, train_loss: 0.011295684613287449, val_loss: 0.009323056787252426\n",
      "Step = 493, train_loss: 0.011409090831875801, val_loss: 0.013683449476957321\n",
      "Step = 494, train_loss: 0.011105809360742569, val_loss: 0.01959732361137867\n",
      "Step = 495, train_loss: 0.015806622803211212, val_loss: 0.018253164365887642\n",
      "Step = 496, train_loss: 0.012670227326452732, val_loss: 0.012283558025956154\n",
      "Step = 497, train_loss: 0.012922163121402264, val_loss: 0.008300087414681911\n",
      "Step = 498, train_loss: 0.012993399053812027, val_loss: 0.007130193989723921\n",
      "Step = 499, train_loss: 0.012719894759356976, val_loss: 0.00822935439646244\n",
      "Step = 500, train_loss: 0.011747063137590885, val_loss: 0.010541209019720554\n",
      "Step = 501, train_loss: 0.011852988973259926, val_loss: 0.013272466138005257\n",
      "Step = 502, train_loss: 0.014673982746899128, val_loss: 0.010293095372617245\n",
      "Step = 503, train_loss: 0.014924694783985615, val_loss: 0.00704218028113246\n",
      "Step = 504, train_loss: 0.01129689160734415, val_loss: 0.013166394084692001\n",
      "Step = 505, train_loss: 0.012120230123400688, val_loss: 0.022111259400844574\n",
      "Step = 506, train_loss: 0.012191186659038067, val_loss: 0.027825960889458656\n",
      "Step = 507, train_loss: 0.01320594735443592, val_loss: 0.02617763541638851\n",
      "Step = 508, train_loss: 0.01214800775051117, val_loss: 0.019085006788372993\n",
      "Step = 509, train_loss: 0.013664355501532555, val_loss: 0.011007511056959629\n",
      "Step = 510, train_loss: 0.01277561578899622, val_loss: 0.00792128499597311\n",
      "Step = 511, train_loss: 0.011773275211453438, val_loss: 0.007109242025762796\n",
      "Step = 512, train_loss: 0.011075451038777828, val_loss: 0.009160955436527729\n",
      "Step = 513, train_loss: 0.014083905145525932, val_loss: 0.01044457033276558\n",
      "Step = 514, train_loss: 0.012173386290669441, val_loss: 0.008288513869047165\n",
      "Step = 515, train_loss: 0.01257192250341177, val_loss: 0.007033208385109901\n",
      "Step = 516, train_loss: 0.011596158146858215, val_loss: 0.007848626002669334\n",
      "Step = 517, train_loss: 0.012368377298116684, val_loss: 0.009631890803575516\n",
      "Step = 518, train_loss: 0.013268908485770226, val_loss: 0.009207685478031635\n",
      "Step = 519, train_loss: 0.013903728686273098, val_loss: 0.007545202970504761\n",
      "Step = 520, train_loss: 0.011079693213105202, val_loss: 0.007197507191449404\n",
      "Step = 521, train_loss: 0.012243925593793392, val_loss: 0.007099919021129608\n",
      "Step = 522, train_loss: 0.011821171268820763, val_loss: 0.00722798565402627\n",
      "Step = 523, train_loss: 0.011280501261353493, val_loss: 0.007752692326903343\n",
      "Step = 524, train_loss: 0.011387079954147339, val_loss: 0.007977033033967018\n",
      "Step = 525, train_loss: 0.013217107392847538, val_loss: 0.010124528780579567\n",
      "Step = 526, train_loss: 0.012712741270661354, val_loss: 0.012555093504488468\n",
      "Step = 527, train_loss: 0.013084277510643005, val_loss: 0.011150133796036243\n",
      "Step = 528, train_loss: 0.011672441847622395, val_loss: 0.010307050310075283\n",
      "Step = 529, train_loss: 0.015431007370352745, val_loss: 0.010202780365943909\n",
      "Step = 530, train_loss: 0.01348709687590599, val_loss: 0.013614954426884651\n",
      "Step = 531, train_loss: 0.012634996324777603, val_loss: 0.01393919438123703\n",
      "Step = 532, train_loss: 0.014846608974039555, val_loss: 0.014767919667065144\n",
      "Step = 533, train_loss: 0.014874383807182312, val_loss: 0.014449943788349628\n",
      "Step = 534, train_loss: 0.012313755229115486, val_loss: 0.012300183065235615\n",
      "Step = 535, train_loss: 0.013074829243123531, val_loss: 0.00984102301299572\n",
      "Step = 536, train_loss: 0.01125902310013771, val_loss: 0.007966586388647556\n",
      "Step = 537, train_loss: 0.012767836451530457, val_loss: 0.00712515227496624\n",
      "Step = 538, train_loss: 0.012107962742447853, val_loss: 0.007339104078710079\n",
      "Step = 539, train_loss: 0.009972949512302876, val_loss: 0.007592954207211733\n",
      "Step = 540, train_loss: 0.012323354370892048, val_loss: 0.007694013882428408\n",
      "Step = 541, train_loss: 0.012047777883708477, val_loss: 0.008051753975450993\n",
      "Step = 542, train_loss: 0.013595527037978172, val_loss: 0.00720891123637557\n",
      "Step = 543, train_loss: 0.011026070453226566, val_loss: 0.0072418078780174255\n",
      "Step = 544, train_loss: 0.013088897801935673, val_loss: 0.007437015417963266\n",
      "Step = 545, train_loss: 0.011303668841719627, val_loss: 0.00921160914003849\n",
      "Step = 546, train_loss: 0.011797243729233742, val_loss: 0.01420404389500618\n",
      "Step = 547, train_loss: 0.013809433206915855, val_loss: 0.018519094213843346\n",
      "Step = 548, train_loss: 0.01354870107024908, val_loss: 0.01721300184726715\n",
      "Step = 549, train_loss: 0.01248650811612606, val_loss: 0.01136679295450449\n",
      "Step = 550, train_loss: 0.014143761247396469, val_loss: 0.008507998660206795\n",
      "Step = 551, train_loss: 0.014652379788458347, val_loss: 0.007416494190692902\n",
      "Step = 552, train_loss: 0.012716393917798996, val_loss: 0.007167537696659565\n",
      "Step = 553, train_loss: 0.012859652750194073, val_loss: 0.007153234910219908\n",
      "Step = 554, train_loss: 0.012389179319143295, val_loss: 0.007213793694972992\n",
      "Step = 555, train_loss: 0.0119034294039011, val_loss: 0.007390680722892284\n",
      "Step = 556, train_loss: 0.013158788904547691, val_loss: 0.0071789054200053215\n",
      "Step = 557, train_loss: 0.011885764077305794, val_loss: 0.00716462591663003\n",
      "Step = 558, train_loss: 0.012086759321391582, val_loss: 0.007214233744889498\n",
      "Step = 559, train_loss: 0.011971982195973396, val_loss: 0.007969611324369907\n",
      "Step = 560, train_loss: 0.012077908031642437, val_loss: 0.009214123710989952\n",
      "Step = 561, train_loss: 0.0125076062977314, val_loss: 0.011827751994132996\n",
      "Step = 562, train_loss: 0.013407032005488873, val_loss: 0.014664779417216778\n",
      "Step = 563, train_loss: 0.01189809013158083, val_loss: 0.013471956364810467\n",
      "Step = 564, train_loss: 0.013875172473490238, val_loss: 0.008447096683084965\n",
      "Step = 565, train_loss: 0.013367979787290096, val_loss: 0.0070807659067213535\n",
      "Step = 566, train_loss: 0.012831617146730423, val_loss: 0.0103081613779068\n",
      "Step = 567, train_loss: 0.012286411598324776, val_loss: 0.016600308939814568\n",
      "Step = 568, train_loss: 0.01105501689016819, val_loss: 0.022855402901768684\n",
      "Step = 569, train_loss: 0.011852293275296688, val_loss: 0.024539407342672348\n",
      "Step = 570, train_loss: 0.012616283260285854, val_loss: 0.023651573807001114\n",
      "Step = 571, train_loss: 0.015084875747561455, val_loss: 0.020158329978585243\n",
      "Step = 572, train_loss: 0.012699354439973831, val_loss: 0.01580771803855896\n",
      "Step = 573, train_loss: 0.013916851952672005, val_loss: 0.011244269087910652\n",
      "Step = 574, train_loss: 0.012733841314911842, val_loss: 0.00785921886563301\n",
      "Step = 575, train_loss: 0.012677237391471863, val_loss: 0.007184389978647232\n",
      "Step = 576, train_loss: 0.013203820213675499, val_loss: 0.007856032811105251\n",
      "Step = 577, train_loss: 0.013201345689594746, val_loss: 0.00874657928943634\n",
      "Step = 578, train_loss: 0.011651449836790562, val_loss: 0.0083413515239954\n",
      "Step = 579, train_loss: 0.012363285757601261, val_loss: 0.008437403477728367\n",
      "Step = 580, train_loss: 0.01130505558103323, val_loss: 0.0074642132967710495\n",
      "Step = 581, train_loss: 0.012738236226141453, val_loss: 0.007076235022395849\n",
      "Step = 582, train_loss: 0.01110581960529089, val_loss: 0.007288782391697168\n",
      "Step = 583, train_loss: 0.013884097337722778, val_loss: 0.007402417249977589\n",
      "Step = 584, train_loss: 0.013496475294232368, val_loss: 0.007715512532740831\n",
      "Step = 585, train_loss: 0.012551303952932358, val_loss: 0.008950648829340935\n",
      "Step = 586, train_loss: 0.01223413273692131, val_loss: 0.009243077598512173\n",
      "Step = 587, train_loss: 0.01153064426034689, val_loss: 0.008941560052335262\n",
      "Step = 588, train_loss: 0.01417133305221796, val_loss: 0.008106410503387451\n",
      "Step = 589, train_loss: 0.012753318063914776, val_loss: 0.007741652429103851\n",
      "Step = 590, train_loss: 0.012022498995065689, val_loss: 0.007757596205919981\n",
      "Step = 591, train_loss: 0.011046437546610832, val_loss: 0.007984563708305359\n",
      "Step = 592, train_loss: 0.011597626842558384, val_loss: 0.007302585989236832\n",
      "Step = 593, train_loss: 0.013404623605310917, val_loss: 0.007143751718103886\n",
      "Step = 594, train_loss: 0.012526498176157475, val_loss: 0.007848803885281086\n",
      "Step = 595, train_loss: 0.012891827151179314, val_loss: 0.008437595330178738\n",
      "Step = 596, train_loss: 0.01260797306895256, val_loss: 0.008262638002634048\n",
      "Step = 597, train_loss: 0.01130041666328907, val_loss: 0.00783785805106163\n",
      "Step = 598, train_loss: 0.014660891145467758, val_loss: 0.007971804589033127\n",
      "Step = 599, train_loss: 0.013882718048989773, val_loss: 0.007924819365143776\n",
      "Step = 600, train_loss: 0.01325895544141531, val_loss: 0.007402536924928427\n",
      "Step = 601, train_loss: 0.011799292638897896, val_loss: 0.007218169514089823\n",
      "Step = 602, train_loss: 0.01402491144835949, val_loss: 0.007106557954102755\n",
      "Step = 603, train_loss: 0.01309391763061285, val_loss: 0.007291887421160936\n",
      "Step = 604, train_loss: 0.011980867013335228, val_loss: 0.008920390158891678\n",
      "Step = 605, train_loss: 0.011602733284235, val_loss: 0.009383009746670723\n",
      "Step = 606, train_loss: 0.010944625362753868, val_loss: 0.008954022079706192\n",
      "Step = 607, train_loss: 0.012824932113289833, val_loss: 0.008469872176647186\n",
      "Step = 608, train_loss: 0.012628525495529175, val_loss: 0.008430426008999348\n",
      "Step = 609, train_loss: 0.011857321485877037, val_loss: 0.007476129103451967\n",
      "Step = 610, train_loss: 0.013838970102369785, val_loss: 0.007265782915055752\n",
      "Step = 611, train_loss: 0.012153447605669498, val_loss: 0.007103397976607084\n",
      "Step = 612, train_loss: 0.0112612284719944, val_loss: 0.007138486951589584\n",
      "Step = 613, train_loss: 0.011797879822552204, val_loss: 0.007100321352481842\n",
      "Step = 614, train_loss: 0.012180701829493046, val_loss: 0.007187179755419493\n",
      "Step = 615, train_loss: 0.011528179980814457, val_loss: 0.007385491393506527\n",
      "Step = 616, train_loss: 0.01284739188849926, val_loss: 0.007378230802714825\n",
      "Step = 617, train_loss: 0.013287789188325405, val_loss: 0.008200575597584248\n",
      "Step = 618, train_loss: 0.011120776645839214, val_loss: 0.008813670836389065\n",
      "Step = 619, train_loss: 0.012533768080174923, val_loss: 0.00924256257712841\n",
      "Step = 620, train_loss: 0.013190890662372112, val_loss: 0.008730822242796421\n",
      "Step = 621, train_loss: 0.01241713110357523, val_loss: 0.007963091135025024\n",
      "Step = 622, train_loss: 0.012643912807106972, val_loss: 0.007531059440225363\n",
      "Step = 623, train_loss: 0.012393554672598839, val_loss: 0.0071687945164740086\n",
      "Step = 624, train_loss: 0.013005777262151241, val_loss: 0.007115297019481659\n",
      "Step = 625, train_loss: 0.011502793058753014, val_loss: 0.007431122474372387\n",
      "Step = 626, train_loss: 0.013026166707277298, val_loss: 0.008321513421833515\n",
      "Step = 627, train_loss: 0.01271665096282959, val_loss: 0.00904827006161213\n",
      "Step = 628, train_loss: 0.012660355307161808, val_loss: 0.00963098369538784\n",
      "Step = 629, train_loss: 0.012948640622198582, val_loss: 0.008475161157548428\n",
      "Step = 630, train_loss: 0.013039977289736271, val_loss: 0.007072538137435913\n",
      "Step = 631, train_loss: 0.011577616445720196, val_loss: 0.007620479445904493\n",
      "Step = 632, train_loss: 0.01250369381159544, val_loss: 0.009560836479067802\n",
      "Step = 633, train_loss: 0.013116706162691116, val_loss: 0.011535474099218845\n",
      "Step = 634, train_loss: 0.011541678570210934, val_loss: 0.012843525968492031\n",
      "Step = 635, train_loss: 0.012391526252031326, val_loss: 0.015174521133303642\n",
      "Step = 636, train_loss: 0.013390383683145046, val_loss: 0.01664922572672367\n",
      "Step = 637, train_loss: 0.012089082971215248, val_loss: 0.015050415880978107\n",
      "Step = 638, train_loss: 0.011379016563296318, val_loss: 0.011944182217121124\n",
      "Step = 639, train_loss: 0.012616242282092571, val_loss: 0.009986022487282753\n",
      "Step = 640, train_loss: 0.012858668342232704, val_loss: 0.008299238979816437\n",
      "Step = 641, train_loss: 0.012711914256215096, val_loss: 0.007391184568405151\n",
      "Step = 642, train_loss: 0.012349619530141354, val_loss: 0.007143424823880196\n",
      "Step = 643, train_loss: 0.014311294071376324, val_loss: 0.007313538808375597\n",
      "Step = 644, train_loss: 0.011695902794599533, val_loss: 0.007980945520102978\n",
      "Step = 645, train_loss: 0.01314457319676876, val_loss: 0.00818393100053072\n",
      "Step = 646, train_loss: 0.015786467120051384, val_loss: 0.008321134373545647\n",
      "Step = 647, train_loss: 0.013494173064827919, val_loss: 0.007646940182894468\n",
      "Step = 648, train_loss: 0.013918768614530563, val_loss: 0.007118001114577055\n",
      "Step = 649, train_loss: 0.01210227981209755, val_loss: 0.007551086600869894\n",
      "Step = 650, train_loss: 0.012568539939820766, val_loss: 0.009341384284198284\n",
      "Step = 651, train_loss: 0.012994750402867794, val_loss: 0.00987559650093317\n",
      "Step = 652, train_loss: 0.013280405662953854, val_loss: 0.010396761819720268\n",
      "Step = 653, train_loss: 0.011495388112962246, val_loss: 0.009676539339125156\n",
      "Step = 654, train_loss: 0.011773589998483658, val_loss: 0.008572942577302456\n",
      "Step = 655, train_loss: 0.01192123256623745, val_loss: 0.007535574026405811\n",
      "Step = 656, train_loss: 0.011313280090689659, val_loss: 0.0071312179788947105\n",
      "Step = 657, train_loss: 0.013269776478409767, val_loss: 0.007187922019511461\n",
      "Step = 658, train_loss: 0.012250017374753952, val_loss: 0.007419837638735771\n",
      "Step = 659, train_loss: 0.011778205633163452, val_loss: 0.008124669082462788\n",
      "Step = 660, train_loss: 0.010707573033869267, val_loss: 0.008639686740934849\n",
      "Step = 661, train_loss: 0.01179883535951376, val_loss: 0.008845278061926365\n",
      "Step = 662, train_loss: 0.014269987121224403, val_loss: 0.008124135434627533\n",
      "Step = 663, train_loss: 0.01338848564773798, val_loss: 0.007341821677982807\n",
      "Step = 664, train_loss: 0.012647815980017185, val_loss: 0.007082302588969469\n",
      "Step = 665, train_loss: 0.01129770278930664, val_loss: 0.007472666446119547\n",
      "Step = 666, train_loss: 0.01327237393707037, val_loss: 0.008118520490825176\n",
      "Step = 667, train_loss: 0.012079413048923016, val_loss: 0.008991183713078499\n",
      "Step = 668, train_loss: 0.01349065825343132, val_loss: 0.009823104366660118\n",
      "Step = 669, train_loss: 0.011564720422029495, val_loss: 0.00979327317327261\n",
      "Step = 670, train_loss: 0.013332338072359562, val_loss: 0.010122744366526604\n",
      "Step = 671, train_loss: 0.011556730605661869, val_loss: 0.009589285589754581\n",
      "Step = 672, train_loss: 0.01138510461896658, val_loss: 0.008556113578379154\n",
      "Step = 673, train_loss: 0.012780679389834404, val_loss: 0.007407175377011299\n",
      "Step = 674, train_loss: 0.01261179056018591, val_loss: 0.007122433744370937\n",
      "Step = 675, train_loss: 0.011992719024419785, val_loss: 0.007187266834080219\n",
      "Step = 676, train_loss: 0.012601571157574654, val_loss: 0.0075423745438456535\n",
      "Step = 677, train_loss: 0.01247169729322195, val_loss: 0.008484141901135445\n",
      "Step = 678, train_loss: 0.013401583768427372, val_loss: 0.008823617361485958\n",
      "Step = 679, train_loss: 0.013405565172433853, val_loss: 0.008853880688548088\n",
      "Step = 680, train_loss: 0.01150260865688324, val_loss: 0.008220161311328411\n",
      "Step = 681, train_loss: 0.012668946757912636, val_loss: 0.007781903259456158\n",
      "Step = 682, train_loss: 0.011455402709543705, val_loss: 0.007389476988464594\n",
      "Step = 683, train_loss: 0.012152343988418579, val_loss: 0.007054375950247049\n",
      "Step = 684, train_loss: 0.01293500792235136, val_loss: 0.00771541940048337\n",
      "Step = 685, train_loss: 0.014487819746136665, val_loss: 0.009971361607313156\n",
      "Step = 686, train_loss: 0.012208996340632439, val_loss: 0.01095668040215969\n",
      "Step = 687, train_loss: 0.011989717371761799, val_loss: 0.010458534583449364\n",
      "Step = 688, train_loss: 0.01436214242130518, val_loss: 0.00987100601196289\n",
      "Step = 689, train_loss: 0.011139808222651482, val_loss: 0.009234079159796238\n",
      "Step = 690, train_loss: 0.01262440625578165, val_loss: 0.008038862608373165\n",
      "Step = 691, train_loss: 0.014238601550459862, val_loss: 0.007314898539334536\n",
      "Step = 692, train_loss: 0.014612942934036255, val_loss: 0.007101825904101133\n",
      "Step = 693, train_loss: 0.011777762323617935, val_loss: 0.007198673207312822\n",
      "Step = 694, train_loss: 0.01207195594906807, val_loss: 0.007377666886895895\n",
      "Step = 695, train_loss: 0.011067511513829231, val_loss: 0.007153219543397427\n",
      "Step = 696, train_loss: 0.012341807596385479, val_loss: 0.0070829507894814014\n",
      "Step = 697, train_loss: 0.012535051442682743, val_loss: 0.007664080709218979\n",
      "Step = 698, train_loss: 0.013269111514091492, val_loss: 0.008092219941318035\n",
      "Step = 699, train_loss: 0.01330007053911686, val_loss: 0.008669606409966946\n",
      "Step = 700, train_loss: 0.010583379305899143, val_loss: 0.008609205484390259\n",
      "Step = 701, train_loss: 0.011772030033171177, val_loss: 0.008260825648903847\n",
      "Step = 702, train_loss: 0.01170576736330986, val_loss: 0.00860440544784069\n",
      "Step = 703, train_loss: 0.011572277173399925, val_loss: 0.00857926718890667\n",
      "Step = 704, train_loss: 0.014093290083110332, val_loss: 0.00869758427143097\n",
      "Step = 705, train_loss: 0.012742364779114723, val_loss: 0.008263143710792065\n",
      "Step = 706, train_loss: 0.011909744702279568, val_loss: 0.008066695183515549\n",
      "Step = 707, train_loss: 0.012626825831830502, val_loss: 0.008022129535675049\n",
      "Step = 708, train_loss: 0.01111916545778513, val_loss: 0.00770582864060998\n",
      "Step = 709, train_loss: 0.01378636434674263, val_loss: 0.00809052586555481\n",
      "Step = 710, train_loss: 0.01041484996676445, val_loss: 0.007989422418177128\n",
      "Step = 711, train_loss: 0.01145955827087164, val_loss: 0.008023994974792004\n",
      "Step = 712, train_loss: 0.011300264857709408, val_loss: 0.007928571663796902\n",
      "Step = 713, train_loss: 0.01343480870127678, val_loss: 0.007985761389136314\n",
      "Step = 714, train_loss: 0.013773242011666298, val_loss: 0.007738096173852682\n",
      "Step = 715, train_loss: 0.012229793705046177, val_loss: 0.007348927203565836\n",
      "Step = 716, train_loss: 0.011477195657789707, val_loss: 0.007115098647773266\n",
      "Step = 717, train_loss: 0.013218400999903679, val_loss: 0.007371345069259405\n",
      "Step = 718, train_loss: 0.014750218950212002, val_loss: 0.007837490178644657\n",
      "Step = 719, train_loss: 0.012492434121668339, val_loss: 0.008138453587889671\n",
      "Step = 720, train_loss: 0.013069099746644497, val_loss: 0.008034111000597477\n",
      "Step = 721, train_loss: 0.01145610585808754, val_loss: 0.007817329838871956\n",
      "Step = 722, train_loss: 0.014855470508337021, val_loss: 0.007417525630444288\n",
      "Step = 723, train_loss: 0.014571893960237503, val_loss: 0.007082853466272354\n",
      "Step = 724, train_loss: 0.012025965377688408, val_loss: 0.007208135910332203\n",
      "Step = 725, train_loss: 0.012204216793179512, val_loss: 0.007935906760394573\n",
      "Step = 726, train_loss: 0.012005792930722237, val_loss: 0.00900026224553585\n",
      "Step = 727, train_loss: 0.012530694715678692, val_loss: 0.010092433542013168\n",
      "Step = 728, train_loss: 0.011123151518404484, val_loss: 0.011044399812817574\n",
      "Step = 729, train_loss: 0.013863400556147099, val_loss: 0.012287596240639687\n",
      "Step = 730, train_loss: 0.011430604383349419, val_loss: 0.012451334856450558\n",
      "Step = 731, train_loss: 0.01309961173683405, val_loss: 0.011777233332395554\n",
      "Step = 732, train_loss: 0.014123867265880108, val_loss: 0.011629928834736347\n",
      "Step = 733, train_loss: 0.012966747395694256, val_loss: 0.010939273051917553\n",
      "Step = 734, train_loss: 0.01070602796971798, val_loss: 0.010244622826576233\n",
      "Step = 735, train_loss: 0.0131766926497221, val_loss: 0.009240307845175266\n",
      "Step = 736, train_loss: 0.012246767990291119, val_loss: 0.008341602981090546\n",
      "Step = 737, train_loss: 0.011040640994906425, val_loss: 0.007811502553522587\n",
      "Step = 738, train_loss: 0.01058210525661707, val_loss: 0.007532934658229351\n",
      "Step = 739, train_loss: 0.012158106081187725, val_loss: 0.007245994172990322\n",
      "Step = 740, train_loss: 0.012228240258991718, val_loss: 0.00720925722271204\n",
      "Step = 741, train_loss: 0.016271285712718964, val_loss: 0.007144764997065067\n",
      "Step = 742, train_loss: 0.012714113108813763, val_loss: 0.007141555193811655\n",
      "Step = 743, train_loss: 0.014326075091958046, val_loss: 0.0071692815981805325\n",
      "Step = 744, train_loss: 0.011788292787969112, val_loss: 0.007264579180628061\n",
      "Step = 745, train_loss: 0.011851903051137924, val_loss: 0.007292847149074078\n",
      "Step = 746, train_loss: 0.012570141814649105, val_loss: 0.007186654955148697\n",
      "Step = 747, train_loss: 0.012662776745855808, val_loss: 0.007283850573003292\n",
      "Step = 748, train_loss: 0.015995247289538383, val_loss: 0.007289568893611431\n",
      "Step = 749, train_loss: 0.012697944417595863, val_loss: 0.0071827108040452\n",
      "Step = 750, train_loss: 0.01222489308565855, val_loss: 0.007112984079867601\n",
      "Step = 751, train_loss: 0.011032705195248127, val_loss: 0.007119161542505026\n",
      "Step = 752, train_loss: 0.013394307345151901, val_loss: 0.007122751325368881\n",
      "Step = 753, train_loss: 0.012229910120368004, val_loss: 0.007213297300040722\n",
      "Step = 754, train_loss: 0.012140174396336079, val_loss: 0.0071692983619868755\n",
      "Step = 755, train_loss: 0.014556141570210457, val_loss: 0.007232426665723324\n",
      "Step = 756, train_loss: 0.014991909265518188, val_loss: 0.007473292760550976\n",
      "Step = 757, train_loss: 0.011651316657662392, val_loss: 0.008081713691353798\n",
      "Step = 758, train_loss: 0.01438993215560913, val_loss: 0.00928985234349966\n",
      "Step = 759, train_loss: 0.011869111098349094, val_loss: 0.010932907462120056\n",
      "Step = 760, train_loss: 0.012941793538630009, val_loss: 0.011593754403293133\n",
      "Step = 761, train_loss: 0.011731698177754879, val_loss: 0.0113141518086195\n",
      "Step = 762, train_loss: 0.012508868239820004, val_loss: 0.009761537425220013\n",
      "Step = 763, train_loss: 0.012093852274119854, val_loss: 0.008422295562922955\n",
      "Step = 764, train_loss: 0.01316062267869711, val_loss: 0.007607664447277784\n",
      "Step = 765, train_loss: 0.013776286505162716, val_loss: 0.0071899788454174995\n",
      "Step = 766, train_loss: 0.01241258718073368, val_loss: 0.007219026796519756\n",
      "Step = 767, train_loss: 0.012390336953103542, val_loss: 0.007509426213800907\n",
      "Step = 768, train_loss: 0.01178362313657999, val_loss: 0.008093100041151047\n",
      "Step = 769, train_loss: 0.011442900635302067, val_loss: 0.008745476603507996\n",
      "Step = 770, train_loss: 0.012900972738862038, val_loss: 0.009218336082994938\n",
      "Step = 771, train_loss: 0.012007776647806168, val_loss: 0.00947276409715414\n",
      "Step = 772, train_loss: 0.010793794877827168, val_loss: 0.009905372746288776\n",
      "Step = 773, train_loss: 0.012445272877812386, val_loss: 0.009384638629853725\n",
      "Step = 774, train_loss: 0.012042098678648472, val_loss: 0.008685436099767685\n",
      "Step = 775, train_loss: 0.015270251780748367, val_loss: 0.00795729085803032\n",
      "Step = 776, train_loss: 0.013985718600451946, val_loss: 0.007453118916600943\n",
      "Step = 777, train_loss: 0.01107145007699728, val_loss: 0.007138652261346579\n",
      "Step = 778, train_loss: 0.0124716367572546, val_loss: 0.007059542927891016\n",
      "Step = 779, train_loss: 0.014262176118791103, val_loss: 0.00718970550224185\n",
      "Step = 780, train_loss: 0.012754344381392002, val_loss: 0.007526004686951637\n",
      "Step = 781, train_loss: 0.014095079153776169, val_loss: 0.007916034199297428\n",
      "Step = 782, train_loss: 0.0117189297452569, val_loss: 0.008284632116556168\n",
      "Step = 783, train_loss: 0.012305321171879768, val_loss: 0.0082280607894063\n",
      "Step = 784, train_loss: 0.011147633194923401, val_loss: 0.008192569948732853\n",
      "Step = 785, train_loss: 0.010941431857645512, val_loss: 0.008257439360022545\n",
      "Step = 786, train_loss: 0.01301180012524128, val_loss: 0.008313721977174282\n",
      "Step = 787, train_loss: 0.011935238726437092, val_loss: 0.008511030115187168\n",
      "Step = 788, train_loss: 0.010826830752193928, val_loss: 0.008981977589428425\n",
      "Step = 789, train_loss: 0.013159165158867836, val_loss: 0.009372868575155735\n",
      "Step = 790, train_loss: 0.013047220185399055, val_loss: 0.00956876203417778\n",
      "Step = 791, train_loss: 0.014156369492411613, val_loss: 0.009434906765818596\n",
      "Step = 792, train_loss: 0.01335494127124548, val_loss: 0.009376263245940208\n",
      "Step = 793, train_loss: 0.01193393673747778, val_loss: 0.008813589811325073\n",
      "Step = 794, train_loss: 0.010849621146917343, val_loss: 0.00806440506130457\n",
      "Step = 795, train_loss: 0.013907666318118572, val_loss: 0.007498742081224918\n",
      "Step = 796, train_loss: 0.010847419500350952, val_loss: 0.007192753255367279\n",
      "Step = 797, train_loss: 0.011369149200618267, val_loss: 0.007158677093684673\n",
      "Step = 798, train_loss: 0.011567165143787861, val_loss: 0.007448699325323105\n",
      "Step = 799, train_loss: 0.012429123744368553, val_loss: 0.00803439226001501\n",
      "Step = 800, train_loss: 0.014559068717062473, val_loss: 0.008266917429864407\n",
      "Step = 801, train_loss: 0.01294303871691227, val_loss: 0.008048959076404572\n",
      "Step = 802, train_loss: 0.011498496867716312, val_loss: 0.0075249201618134975\n",
      "Step = 803, train_loss: 0.011329741217195988, val_loss: 0.0072178104892373085\n",
      "Step = 804, train_loss: 0.012652779929339886, val_loss: 0.0070678251795470715\n",
      "Step = 805, train_loss: 0.012524714693427086, val_loss: 0.007113443687558174\n",
      "Step = 806, train_loss: 0.011100763455033302, val_loss: 0.007182017434388399\n",
      "Step = 807, train_loss: 0.011467363685369492, val_loss: 0.007285774685442448\n",
      "Step = 808, train_loss: 0.011848341673612595, val_loss: 0.007471785414963961\n",
      "Step = 809, train_loss: 0.011212779209017754, val_loss: 0.007697276771068573\n",
      "Step = 810, train_loss: 0.011270908638834953, val_loss: 0.007899090647697449\n",
      "Step = 811, train_loss: 0.013659244403243065, val_loss: 0.008197855204343796\n",
      "Step = 812, train_loss: 0.0135496249422431, val_loss: 0.008428266271948814\n",
      "Step = 813, train_loss: 0.011719441041350365, val_loss: 0.008640870451927185\n",
      "Step = 814, train_loss: 0.011502625420689583, val_loss: 0.009114949032664299\n",
      "Step = 815, train_loss: 0.012350348755717278, val_loss: 0.0092702591791749\n",
      "Step = 816, train_loss: 0.01269716490060091, val_loss: 0.009037490002810955\n",
      "Step = 817, train_loss: 0.012831413187086582, val_loss: 0.009175279177725315\n",
      "Step = 818, train_loss: 0.011677741073071957, val_loss: 0.009266646578907967\n",
      "Step = 819, train_loss: 0.011360159143805504, val_loss: 0.009654413908720016\n",
      "Step = 820, train_loss: 0.01165045890957117, val_loss: 0.009858463890850544\n",
      "Step = 821, train_loss: 0.013331383466720581, val_loss: 0.009832254610955715\n",
      "Step = 822, train_loss: 0.01281917653977871, val_loss: 0.009235996752977371\n",
      "Step = 823, train_loss: 0.012144846841692924, val_loss: 0.008327272720634937\n",
      "Step = 824, train_loss: 0.011324652470648289, val_loss: 0.007733443286269903\n",
      "Step = 825, train_loss: 0.012165429070591927, val_loss: 0.007473234552890062\n",
      "Step = 826, train_loss: 0.012171054258942604, val_loss: 0.0072113946080207825\n",
      "Step = 827, train_loss: 0.011383365839719772, val_loss: 0.007179261650890112\n",
      "Step = 828, train_loss: 0.01223005447536707, val_loss: 0.007171377073973417\n",
      "Step = 829, train_loss: 0.014213130809366703, val_loss: 0.007163175847381353\n",
      "Step = 830, train_loss: 0.012761405669152737, val_loss: 0.007171642500907183\n",
      "Step = 831, train_loss: 0.012677609920501709, val_loss: 0.007189297117292881\n",
      "Step = 832, train_loss: 0.012262482196092606, val_loss: 0.00720264483243227\n",
      "Step = 833, train_loss: 0.013742182403802872, val_loss: 0.007212385069578886\n",
      "Step = 834, train_loss: 0.010834389366209507, val_loss: 0.007243369240313768\n",
      "Step = 835, train_loss: 0.011965716257691383, val_loss: 0.0072987275198102\n",
      "Step = 836, train_loss: 0.012381170876324177, val_loss: 0.007259441539645195\n",
      "Step = 837, train_loss: 0.01185170654207468, val_loss: 0.007167846895754337\n",
      "Step = 838, train_loss: 0.014445994980633259, val_loss: 0.00712378416210413\n",
      "Step = 839, train_loss: 0.014217063784599304, val_loss: 0.007330934517085552\n",
      "Step = 840, train_loss: 0.012675552628934383, val_loss: 0.007660608272999525\n",
      "Step = 841, train_loss: 0.011778410524129868, val_loss: 0.007961235009133816\n",
      "Step = 842, train_loss: 0.011011766269803047, val_loss: 0.008109163492918015\n",
      "Step = 843, train_loss: 0.011546851135790348, val_loss: 0.008083918131887913\n",
      "Step = 844, train_loss: 0.01193914469331503, val_loss: 0.007989010773599148\n",
      "Step = 845, train_loss: 0.011629411019384861, val_loss: 0.007892499677836895\n",
      "Step = 846, train_loss: 0.011900474317371845, val_loss: 0.007846989668905735\n",
      "Step = 847, train_loss: 0.014055847190320492, val_loss: 0.0077330912463366985\n",
      "Step = 848, train_loss: 0.012526192702353, val_loss: 0.007679853588342667\n",
      "Step = 849, train_loss: 0.01078683603554964, val_loss: 0.007594285998493433\n",
      "Step = 850, train_loss: 0.013097171671688557, val_loss: 0.0073779565282166\n",
      "Step = 851, train_loss: 0.011741780675947666, val_loss: 0.007269138935953379\n",
      "Step = 852, train_loss: 0.011376669630408287, val_loss: 0.007252713665366173\n",
      "Step = 853, train_loss: 0.01199321262538433, val_loss: 0.007272263057529926\n",
      "Step = 854, train_loss: 0.01397850550711155, val_loss: 0.007260913494974375\n",
      "Step = 855, train_loss: 0.0110164824873209, val_loss: 0.00728090712800622\n",
      "Step = 856, train_loss: 0.013437614776194096, val_loss: 0.007337584160268307\n",
      "Step = 857, train_loss: 0.010358501225709915, val_loss: 0.00739013496786356\n",
      "Step = 858, train_loss: 0.010868688113987446, val_loss: 0.007401308510452509\n",
      "Step = 859, train_loss: 0.012782618403434753, val_loss: 0.00731314904987812\n",
      "Step = 860, train_loss: 0.011123869568109512, val_loss: 0.007260160520672798\n",
      "Step = 861, train_loss: 0.013437237590551376, val_loss: 0.007181610446423292\n",
      "Step = 862, train_loss: 0.01167064905166626, val_loss: 0.0071620154194533825\n",
      "Step = 863, train_loss: 0.011985787190496922, val_loss: 0.007183009758591652\n",
      "Step = 864, train_loss: 0.013107805512845516, val_loss: 0.0071764858439564705\n",
      "Step = 865, train_loss: 0.012641520239412785, val_loss: 0.007163713686168194\n",
      "Step = 866, train_loss: 0.013240777887403965, val_loss: 0.007145879324525595\n",
      "Step = 867, train_loss: 0.012154637835919857, val_loss: 0.007157373707741499\n",
      "Step = 868, train_loss: 0.013247819617390633, val_loss: 0.007236102130264044\n",
      "Step = 869, train_loss: 0.011383075267076492, val_loss: 0.007361664902418852\n",
      "Step = 870, train_loss: 0.011379043571650982, val_loss: 0.007450279779732227\n",
      "Step = 871, train_loss: 0.01350410282611847, val_loss: 0.00747054535895586\n",
      "Step = 872, train_loss: 0.012197554111480713, val_loss: 0.007417493499815464\n",
      "Step = 873, train_loss: 0.012102747336030006, val_loss: 0.00733054568991065\n",
      "Step = 874, train_loss: 0.014193208888173103, val_loss: 0.007226749323308468\n",
      "Step = 875, train_loss: 0.012396363541483879, val_loss: 0.0071952613070607185\n",
      "Step = 876, train_loss: 0.010472092777490616, val_loss: 0.007158343214541674\n",
      "Step = 877, train_loss: 0.009988516569137573, val_loss: 0.0071466825902462006\n",
      "Step = 878, train_loss: 0.013923406600952148, val_loss: 0.007149959448724985\n",
      "Step = 879, train_loss: 0.014599937945604324, val_loss: 0.0071617793291807175\n",
      "Step = 880, train_loss: 0.011067243292927742, val_loss: 0.007219634484499693\n",
      "Step = 881, train_loss: 0.01464206911623478, val_loss: 0.007320994511246681\n",
      "Step = 882, train_loss: 0.013944966718554497, val_loss: 0.007375756278634071\n",
      "Step = 883, train_loss: 0.011689339764416218, val_loss: 0.007335384376347065\n",
      "Step = 884, train_loss: 0.012264441698789597, val_loss: 0.00731824291869998\n",
      "Step = 885, train_loss: 0.012051020748913288, val_loss: 0.007412813138216734\n",
      "Step = 886, train_loss: 0.01360052265226841, val_loss: 0.007622119039297104\n",
      "Step = 887, train_loss: 0.013261963613331318, val_loss: 0.007749712094664574\n",
      "Step = 888, train_loss: 0.011527054943144321, val_loss: 0.007890153676271439\n",
      "Step = 889, train_loss: 0.011774405837059021, val_loss: 0.008084929548203945\n",
      "Step = 890, train_loss: 0.011638643220067024, val_loss: 0.008161109872162342\n",
      "Step = 891, train_loss: 0.01220955140888691, val_loss: 0.00803646445274353\n",
      "Step = 892, train_loss: 0.011648211628198624, val_loss: 0.00792660191655159\n",
      "Step = 893, train_loss: 0.01350150816142559, val_loss: 0.007778357248753309\n",
      "Step = 894, train_loss: 0.012597092427313328, val_loss: 0.007632110733538866\n",
      "Step = 895, train_loss: 0.01303129456937313, val_loss: 0.007586340885609388\n",
      "Step = 896, train_loss: 0.01139836385846138, val_loss: 0.007394763641059399\n",
      "Step = 897, train_loss: 0.011850854381918907, val_loss: 0.007270067930221558\n",
      "Step = 898, train_loss: 0.01270894892513752, val_loss: 0.007207947317510843\n",
      "Step = 899, train_loss: 0.01249206904321909, val_loss: 0.007196005433797836\n",
      "Step = 900, train_loss: 0.012086765840649605, val_loss: 0.007279704324901104\n",
      "Step = 901, train_loss: 0.012635184451937675, val_loss: 0.007439632900059223\n",
      "Step = 902, train_loss: 0.012334472499787807, val_loss: 0.007539799902588129\n",
      "Step = 903, train_loss: 0.011895899660885334, val_loss: 0.007706807926297188\n",
      "Step = 904, train_loss: 0.013269326649606228, val_loss: 0.00768304942175746\n",
      "Step = 905, train_loss: 0.01338834036141634, val_loss: 0.007632645312696695\n",
      "Step = 906, train_loss: 0.012310809455811977, val_loss: 0.007505741901695728\n",
      "Step = 907, train_loss: 0.01261647418141365, val_loss: 0.007376064546406269\n",
      "Step = 908, train_loss: 0.010916415601968765, val_loss: 0.007258141878992319\n",
      "Step = 909, train_loss: 0.0107659213244915, val_loss: 0.007149347569793463\n",
      "Step = 910, train_loss: 0.012806735001504421, val_loss: 0.007102967239916325\n",
      "Step = 911, train_loss: 0.013107932172715664, val_loss: 0.0071286484599113464\n",
      "Step = 912, train_loss: 0.01259972807019949, val_loss: 0.007297396659851074\n",
      "Step = 913, train_loss: 0.013492918573319912, val_loss: 0.007602653931826353\n",
      "Step = 914, train_loss: 0.014668031595647335, val_loss: 0.008111310191452503\n",
      "Step = 915, train_loss: 0.015715867280960083, val_loss: 0.008442618884146214\n",
      "Step = 916, train_loss: 0.01433184091001749, val_loss: 0.00874638743698597\n",
      "Step = 917, train_loss: 0.010522032156586647, val_loss: 0.008864715695381165\n",
      "Step = 918, train_loss: 0.011310264468193054, val_loss: 0.008884530514478683\n",
      "Step = 919, train_loss: 0.012275034561753273, val_loss: 0.00880744494497776\n",
      "Step = 920, train_loss: 0.013791123405098915, val_loss: 0.008770235814154148\n",
      "Step = 921, train_loss: 0.012868568301200867, val_loss: 0.008507340215146542\n",
      "Step = 922, train_loss: 0.011275998316705227, val_loss: 0.008315677754580975\n",
      "Step = 923, train_loss: 0.014029822312295437, val_loss: 0.008039109408855438\n",
      "Step = 924, train_loss: 0.010782942175865173, val_loss: 0.007752490695565939\n",
      "Step = 925, train_loss: 0.011858323588967323, val_loss: 0.007505853194743395\n",
      "Step = 926, train_loss: 0.011979346163570881, val_loss: 0.0073181381449103355\n",
      "Step = 927, train_loss: 0.013536930084228516, val_loss: 0.007187751587480307\n",
      "Step = 928, train_loss: 0.011535906232893467, val_loss: 0.0071366881020367146\n",
      "Step = 929, train_loss: 0.013070689514279366, val_loss: 0.007209035102277994\n",
      "Step = 930, train_loss: 0.01127559319138527, val_loss: 0.007302381098270416\n",
      "Step = 931, train_loss: 0.012931888923048973, val_loss: 0.007341661956161261\n",
      "Step = 932, train_loss: 0.013030181638896465, val_loss: 0.0074504222720861435\n",
      "Step = 933, train_loss: 0.012939894571900368, val_loss: 0.007474266923964024\n",
      "Step = 934, train_loss: 0.012302933260798454, val_loss: 0.0075594172812998295\n",
      "Step = 935, train_loss: 0.01351819559931755, val_loss: 0.007507303263992071\n",
      "Step = 936, train_loss: 0.013701340183615685, val_loss: 0.007416632957756519\n",
      "Step = 937, train_loss: 0.01306938100606203, val_loss: 0.007268419489264488\n",
      "Step = 938, train_loss: 0.01155902910977602, val_loss: 0.0071655381470918655\n",
      "Step = 939, train_loss: 0.012223733589053154, val_loss: 0.007104428950697184\n",
      "Step = 940, train_loss: 0.012240525335073471, val_loss: 0.007070872001349926\n",
      "Step = 941, train_loss: 0.013593854382634163, val_loss: 0.007087350822985172\n",
      "Step = 942, train_loss: 0.012777628377079964, val_loss: 0.007152532692998648\n",
      "Step = 943, train_loss: 0.011116543784737587, val_loss: 0.007256801705807447\n",
      "Step = 944, train_loss: 0.011562814004719257, val_loss: 0.007325669750571251\n",
      "Step = 945, train_loss: 0.013925059698522091, val_loss: 0.007434797007590532\n",
      "Step = 946, train_loss: 0.01216925773769617, val_loss: 0.007579535245895386\n",
      "Step = 947, train_loss: 0.012902570888400078, val_loss: 0.007791109383106232\n",
      "Step = 948, train_loss: 0.013702736236155033, val_loss: 0.00786782056093216\n",
      "Step = 949, train_loss: 0.01273287832736969, val_loss: 0.008142665028572083\n",
      "Step = 950, train_loss: 0.012812584638595581, val_loss: 0.008316192775964737\n",
      "Step = 951, train_loss: 0.01319865882396698, val_loss: 0.008348124101758003\n",
      "Step = 952, train_loss: 0.014770164154469967, val_loss: 0.008302252739667892\n",
      "Step = 953, train_loss: 0.011747634038329124, val_loss: 0.008294518105685711\n",
      "Step = 954, train_loss: 0.01279780175536871, val_loss: 0.00829913280904293\n",
      "Step = 955, train_loss: 0.01260458119213581, val_loss: 0.008334306068718433\n",
      "Step = 956, train_loss: 0.01117978896945715, val_loss: 0.008299143984913826\n",
      "Step = 957, train_loss: 0.013526823371648788, val_loss: 0.00822022370994091\n",
      "Step = 958, train_loss: 0.010393432341516018, val_loss: 0.008080889470875263\n",
      "Step = 959, train_loss: 0.01356154028326273, val_loss: 0.007928689010441303\n",
      "Step = 960, train_loss: 0.01225253939628601, val_loss: 0.007811598014086485\n",
      "Step = 961, train_loss: 0.011346929706633091, val_loss: 0.007669750601053238\n",
      "Step = 962, train_loss: 0.011372135020792484, val_loss: 0.007546924985945225\n",
      "Step = 963, train_loss: 0.014242936857044697, val_loss: 0.007529239635914564\n",
      "Step = 964, train_loss: 0.011933894827961922, val_loss: 0.007477412931621075\n",
      "Step = 965, train_loss: 0.012254118919372559, val_loss: 0.007498431485146284\n",
      "Step = 966, train_loss: 0.011649757623672485, val_loss: 0.007461782079190016\n",
      "Step = 967, train_loss: 0.013564931228756905, val_loss: 0.00738965068012476\n",
      "Step = 968, train_loss: 0.012063060887157917, val_loss: 0.007302124984562397\n",
      "Step = 969, train_loss: 0.01476815901696682, val_loss: 0.00723554752767086\n",
      "Step = 970, train_loss: 0.01332936156541109, val_loss: 0.007204430643469095\n",
      "Step = 971, train_loss: 0.013964972458779812, val_loss: 0.007225212641060352\n",
      "Step = 972, train_loss: 0.011965274810791016, val_loss: 0.007257685996592045\n",
      "Step = 973, train_loss: 0.015328248031437397, val_loss: 0.007289875764399767\n",
      "Step = 974, train_loss: 0.011303087696433067, val_loss: 0.007340168114751577\n",
      "Step = 975, train_loss: 0.011931384913623333, val_loss: 0.007405595853924751\n",
      "Step = 976, train_loss: 0.011290223337709904, val_loss: 0.007417710032314062\n",
      "Step = 977, train_loss: 0.011062903329730034, val_loss: 0.007404752541333437\n",
      "Step = 978, train_loss: 0.012404500506818295, val_loss: 0.007413698360323906\n",
      "Step = 979, train_loss: 0.011590886861085892, val_loss: 0.007389219012111425\n",
      "Step = 980, train_loss: 0.01304301992058754, val_loss: 0.007346597500145435\n",
      "Step = 981, train_loss: 0.012815558351576328, val_loss: 0.007344516459852457\n",
      "Step = 982, train_loss: 0.012334583327174187, val_loss: 0.00740300165489316\n",
      "Step = 983, train_loss: 0.013472621329128742, val_loss: 0.00746770016849041\n",
      "Step = 984, train_loss: 0.011315753683447838, val_loss: 0.00747271440923214\n",
      "Step = 985, train_loss: 0.010989478789269924, val_loss: 0.007471115328371525\n",
      "Step = 986, train_loss: 0.011794958263635635, val_loss: 0.007507027126848698\n",
      "Step = 987, train_loss: 0.014174425974488258, val_loss: 0.00753153907135129\n",
      "Step = 988, train_loss: 0.011687099933624268, val_loss: 0.007549704052507877\n",
      "Step = 989, train_loss: 0.012871296145021915, val_loss: 0.0075980788096785545\n",
      "Step = 990, train_loss: 0.013345098122954369, val_loss: 0.0076527721248567104\n",
      "Step = 991, train_loss: 0.013251874595880508, val_loss: 0.007666624151170254\n",
      "Step = 992, train_loss: 0.012025962583720684, val_loss: 0.0076630073599517345\n",
      "Step = 993, train_loss: 0.011910509318113327, val_loss: 0.007712320424616337\n",
      "Step = 994, train_loss: 0.012159592472016811, val_loss: 0.007767209317535162\n",
      "Step = 995, train_loss: 0.012894989922642708, val_loss: 0.007909958250820637\n",
      "Step = 996, train_loss: 0.011793083511292934, val_loss: 0.008006663993000984\n",
      "Step = 997, train_loss: 0.013988171704113483, val_loss: 0.008052827790379524\n",
      "Step = 998, train_loss: 0.014380363747477531, val_loss: 0.008121209219098091\n",
      "Step = 999, train_loss: 0.011547700501978397, val_loss: 0.00810070801526308\n",
      "Best model at index: 372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_30020\\3750089785.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('Best_target_model/net_parameters.pkl')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "# summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "# 每20次下降一半的函数\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.8 ** (epoch // 40))\n",
    "\n",
    "val_losses=[]\n",
    "\n",
    "for t in range(600):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "\n",
    "    '''save all the model in each epoch'''    \n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, 'Target_model/net_parameters'+str(t)+'.pkl')\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print(f'Step = {t}, train_loss: {train_loss.item()}, val_loss: {val_loss.item()}')\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "   \n",
    "'''find the best model'''\n",
    "best_index = val_losses.index(np.min(val_losses))\n",
    "print(f'Best model at index: {best_index}')\n",
    "\n",
    "'''copy the best model from model file to the model_test file'''\n",
    "shutil.copyfile(f'Target_model/net_parameters{best_index}.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "'''Reload the best model''' \n",
    "model = Net()\n",
    "checkpoint = torch.load('Best_target_model/net_parameters.pkl')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.14870961\n",
      "mpe_val: 0.13430652\n",
      "mpe_a: 0.14546513817506954\n",
      "mpe_b: 0.12298336232504156\n",
      "rmse_train: 224.06447\n",
      "rmse_val: 186.85628\n",
      "rmse_a: 233.8179076834971\n",
      "rmse_b: 221.11512838338314\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.6538496017456055\n",
      "r2_val: 0.6645767688751221\n",
      "r2_a: 0.6417636985700611\n",
      "r2_b: 0.3985379421089278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
