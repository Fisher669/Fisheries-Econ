{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型效果\n",
    "对于googlenet baseline 的修改，详细见md文件\n",
    "\n",
    "\n",
    "## 对比（非lstm架构）\n",
    "r2_train: 0.9947811961174011\n",
    "r2_val: 0.9749422669410706\n",
    "r2_a: 0.9464100454957816\n",
    "r2_b: 0.6836000724611211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 公用的前3层卷积层\n",
    "        self.shared_cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(8, 16, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # 插入Google net Inception模块\n",
    "        self.inceptions = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True),\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=1, padding=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True),\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(5, 5), stride=1, padding=2),  # Adjusted padding to 2\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "            )\n",
    "        ])\n",
    "        self.lstm = torch.nn.LSTM(input_size=32, hidden_size=128, num_layers=2, batch_first=True)\n",
    "        # Input size for fc1 will be calculated dynamically\n",
    "        # self.fc1 = None  \n",
    "        self.drop_layer = torch.nn.Dropout(p=0.2)\n",
    "        self.fc2 = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            # 公用卷积层\n",
    "            shared_out = self.shared_cnn(x)\n",
    "            \n",
    "            # Inception outputs\n",
    "            inception_outputs = [inception(shared_out) for inception in self.inceptions]\n",
    "            combined = torch.cat(inception_outputs, dim=1)  # Concatenate along channel dimension (dim=1)\n",
    "\n",
    "            # Reshape for LSTM: (batch_size, sequence_length, input_size)\n",
    "            combined = combined.permute(0, 2, 3, 1)  # (batch_size, height, width, channels)\n",
    "            combined = combined.contiguous().view(combined.size(0), -1, 32)  # Flatten height & width into sequence length\n",
    "\n",
    "            # Pass through LSTM\n",
    "            lstm_out, (hn, cn) = self.lstm(combined)\n",
    "\n",
    "            # Use LSTM output at the final time step\n",
    "            lstm_out_last = lstm_out[:, -1, :]  # (batch_size, hidden_size)\n",
    "\n",
    "            # # Calculate input size for fully connected layer dynamically\n",
    "            # if self.fc1 is None:\n",
    "            #     self.fc1 = torch.nn.Linear(lstm_out_last.size(1), 1000)\n",
    "\n",
    "            # Fully connected layers\n",
    "            x = torch.relu(lstm_out_last)\n",
    "            x = self.drop_layer(x)\n",
    "            x = self.fc2(x)\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during forward pass: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (shared_cnn): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  )\n",
      "  (inceptions): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(32, 128, num_layers=2, batch_first=True)\n",
      "  (drop_layer): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Step = 0 train_loss: 0.062209778 val_loss: 0.05956042\n",
      "Step = 1 train_loss: 0.062317587 val_loss: 0.059592247\n",
      "Step = 2 train_loss: 0.062170677 val_loss: 0.059621353\n",
      "Step = 3 train_loss: 0.062393546 val_loss: 0.059649043\n",
      "Step = 4 train_loss: 0.06254788 val_loss: 0.059675507\n",
      "Step = 5 train_loss: 0.062786594 val_loss: 0.059696704\n",
      "Step = 6 train_loss: 0.06163252 val_loss: 0.059711352\n",
      "Step = 7 train_loss: 0.06214843 val_loss: 0.059723422\n",
      "Step = 8 train_loss: 0.06233913 val_loss: 0.059734207\n",
      "Step = 9 train_loss: 0.06154001 val_loss: 0.059743997\n",
      "Step = 10 train_loss: 0.06180478 val_loss: 0.059751626\n",
      "Step = 11 train_loss: 0.061423007 val_loss: 0.05975621\n",
      "Step = 12 train_loss: 0.061668776 val_loss: 0.059759554\n",
      "Step = 13 train_loss: 0.061818514 val_loss: 0.05976083\n",
      "Step = 14 train_loss: 0.0611775 val_loss: 0.059759803\n",
      "Step = 15 train_loss: 0.06159958 val_loss: 0.05975691\n",
      "Step = 16 train_loss: 0.06182125 val_loss: 0.059753727\n",
      "Step = 17 train_loss: 0.061294913 val_loss: 0.059759255\n",
      "Step = 18 train_loss: 0.06093421 val_loss: 0.059756134\n",
      "Step = 19 train_loss: 0.061153565 val_loss: 0.059746247\n",
      "Step = 20 train_loss: 0.060828626 val_loss: 0.059731465\n",
      "Step = 21 train_loss: 0.060810547 val_loss: 0.059717994\n",
      "Step = 22 train_loss: 0.061157856 val_loss: 0.059704788\n",
      "Step = 23 train_loss: 0.061068468 val_loss: 0.05969142\n",
      "Step = 24 train_loss: 0.061331026 val_loss: 0.059675105\n",
      "Step = 25 train_loss: 0.060943875 val_loss: 0.059656892\n",
      "Step = 26 train_loss: 0.060760424 val_loss: 0.059635814\n",
      "Step = 27 train_loss: 0.060576137 val_loss: 0.05961409\n",
      "Step = 28 train_loss: 0.06073344 val_loss: 0.05959006\n",
      "Step = 29 train_loss: 0.060547948 val_loss: 0.059559967\n",
      "Step = 30 train_loss: 0.06071365 val_loss: 0.05952895\n",
      "Step = 31 train_loss: 0.06067972 val_loss: 0.05949981\n",
      "Step = 32 train_loss: 0.060313053 val_loss: 0.059467122\n",
      "Step = 33 train_loss: 0.06057537 val_loss: 0.05943264\n",
      "Step = 34 train_loss: 0.05993271 val_loss: 0.05939158\n",
      "Step = 35 train_loss: 0.06040558 val_loss: 0.05934628\n",
      "Step = 36 train_loss: 0.060453 val_loss: 0.05928868\n",
      "Step = 37 train_loss: 0.059942756 val_loss: 0.059228785\n",
      "Step = 38 train_loss: 0.05999897 val_loss: 0.059169855\n",
      "Step = 39 train_loss: 0.05982473 val_loss: 0.059107855\n",
      "Step = 40 train_loss: 0.060341835 val_loss: 0.05904226\n",
      "Step = 41 train_loss: 0.05957891 val_loss: 0.058980018\n",
      "Step = 42 train_loss: 0.06009644 val_loss: 0.058914114\n",
      "Step = 43 train_loss: 0.059670884 val_loss: 0.058853194\n",
      "Step = 44 train_loss: 0.059698444 val_loss: 0.058791194\n",
      "Step = 45 train_loss: 0.059595294 val_loss: 0.058724713\n",
      "Step = 46 train_loss: 0.059831288 val_loss: 0.05867101\n",
      "Step = 47 train_loss: 0.059532348 val_loss: 0.058632903\n",
      "Step = 48 train_loss: 0.05897977 val_loss: 0.05861376\n",
      "Step = 49 train_loss: 0.059544142 val_loss: 0.0585887\n",
      "Step = 50 train_loss: 0.059832435 val_loss: 0.058539424\n",
      "Step = 51 train_loss: 0.058827873 val_loss: 0.058474258\n",
      "Step = 52 train_loss: 0.0591685 val_loss: 0.058397118\n",
      "Step = 53 train_loss: 0.059195206 val_loss: 0.058315933\n",
      "Step = 54 train_loss: 0.059389755 val_loss: 0.05824168\n",
      "Step = 55 train_loss: 0.059499986 val_loss: 0.058170877\n",
      "Step = 56 train_loss: 0.059138037 val_loss: 0.058102284\n",
      "Step = 57 train_loss: 0.05864352 val_loss: 0.058013752\n",
      "Step = 58 train_loss: 0.05922726 val_loss: 0.05792428\n",
      "Step = 59 train_loss: 0.058988195 val_loss: 0.05782685\n",
      "Step = 60 train_loss: 0.05845757 val_loss: 0.057730123\n",
      "Step = 61 train_loss: 0.058672216 val_loss: 0.05763861\n",
      "Step = 62 train_loss: 0.058330256 val_loss: 0.057537567\n",
      "Step = 63 train_loss: 0.058632568 val_loss: 0.057435967\n",
      "Step = 64 train_loss: 0.058729805 val_loss: 0.057335384\n",
      "Step = 65 train_loss: 0.05875792 val_loss: 0.05722072\n",
      "Step = 66 train_loss: 0.058050632 val_loss: 0.057107832\n",
      "Step = 67 train_loss: 0.058373928 val_loss: 0.056982353\n",
      "Step = 68 train_loss: 0.058135703 val_loss: 0.056874234\n",
      "Step = 69 train_loss: 0.058658104 val_loss: 0.056765046\n",
      "Step = 70 train_loss: 0.058322523 val_loss: 0.05665687\n",
      "Step = 71 train_loss: 0.058759592 val_loss: 0.056566715\n",
      "Step = 72 train_loss: 0.05786006 val_loss: 0.05646549\n",
      "Step = 73 train_loss: 0.057275083 val_loss: 0.056364734\n",
      "Step = 74 train_loss: 0.058382772 val_loss: 0.056259695\n",
      "Step = 75 train_loss: 0.05768357 val_loss: 0.05616272\n",
      "Step = 76 train_loss: 0.05792817 val_loss: 0.05606299\n",
      "Step = 77 train_loss: 0.057847746 val_loss: 0.05596899\n",
      "Step = 78 train_loss: 0.05750213 val_loss: 0.05587952\n",
      "Step = 79 train_loss: 0.057164982 val_loss: 0.05582235\n",
      "Step = 80 train_loss: 0.057842538 val_loss: 0.055765275\n",
      "Step = 81 train_loss: 0.05763832 val_loss: 0.055695407\n",
      "Step = 82 train_loss: 0.057181213 val_loss: 0.055620506\n",
      "Step = 83 train_loss: 0.057316046 val_loss: 0.05553257\n",
      "Step = 84 train_loss: 0.057692416 val_loss: 0.055446904\n",
      "Step = 85 train_loss: 0.05754703 val_loss: 0.05531646\n",
      "Step = 86 train_loss: 0.05742327 val_loss: 0.055190813\n",
      "Step = 87 train_loss: 0.05751175 val_loss: 0.055098888\n",
      "Step = 88 train_loss: 0.057168067 val_loss: 0.055021584\n",
      "Step = 89 train_loss: 0.05672541 val_loss: 0.054930292\n",
      "Step = 90 train_loss: 0.057058033 val_loss: 0.054869052\n",
      "Step = 91 train_loss: 0.056754205 val_loss: 0.054857854\n",
      "Step = 92 train_loss: 0.056613866 val_loss: 0.054811973\n",
      "Step = 93 train_loss: 0.056168143 val_loss: 0.054854333\n",
      "Step = 94 train_loss: 0.05590761 val_loss: 0.05490642\n",
      "Step = 95 train_loss: 0.056204855 val_loss: 0.054889593\n",
      "Step = 96 train_loss: 0.056058004 val_loss: 0.05482425\n",
      "Step = 97 train_loss: 0.056501005 val_loss: 0.05474169\n",
      "Step = 98 train_loss: 0.05613039 val_loss: 0.054634526\n",
      "Step = 99 train_loss: 0.05636836 val_loss: 0.05451713\n",
      "Step = 100 train_loss: 0.05645051 val_loss: 0.054395676\n",
      "Step = 101 train_loss: 0.056374647 val_loss: 0.054281652\n",
      "Step = 102 train_loss: 0.05592386 val_loss: 0.054180656\n",
      "Step = 103 train_loss: 0.056493327 val_loss: 0.054100588\n",
      "Step = 104 train_loss: 0.055766646 val_loss: 0.0541672\n",
      "Step = 105 train_loss: 0.05582792 val_loss: 0.054143913\n",
      "Step = 106 train_loss: 0.05566935 val_loss: 0.054081142\n",
      "Step = 107 train_loss: 0.05531273 val_loss: 0.054002542\n",
      "Step = 108 train_loss: 0.055402856 val_loss: 0.053904533\n",
      "Step = 109 train_loss: 0.05565021 val_loss: 0.05382653\n",
      "Step = 110 train_loss: 0.055673018 val_loss: 0.05374281\n",
      "Step = 111 train_loss: 0.055636916 val_loss: 0.053644303\n",
      "Step = 112 train_loss: 0.05585193 val_loss: 0.053551685\n",
      "Step = 113 train_loss: 0.054888643 val_loss: 0.053474408\n",
      "Step = 114 train_loss: 0.054742694 val_loss: 0.053428516\n",
      "Step = 115 train_loss: 0.054356042 val_loss: 0.053436767\n",
      "Step = 116 train_loss: 0.05525533 val_loss: 0.053462394\n",
      "Step = 117 train_loss: 0.05407161 val_loss: 0.05351748\n",
      "Step = 118 train_loss: 0.054536514 val_loss: 0.053529624\n",
      "Step = 119 train_loss: 0.054617103 val_loss: 0.053505667\n",
      "Step = 120 train_loss: 0.054671627 val_loss: 0.05343285\n",
      "Step = 121 train_loss: 0.053864874 val_loss: 0.053328123\n",
      "Step = 122 train_loss: 0.05410657 val_loss: 0.05318451\n",
      "Step = 123 train_loss: 0.05437356 val_loss: 0.053037684\n",
      "Step = 124 train_loss: 0.05397201 val_loss: 0.052876547\n",
      "Step = 125 train_loss: 0.05478837 val_loss: 0.05273197\n",
      "Step = 126 train_loss: 0.05389651 val_loss: 0.052551072\n",
      "Step = 127 train_loss: 0.05456222 val_loss: 0.05249034\n",
      "Step = 128 train_loss: 0.054171488 val_loss: 0.05247233\n",
      "Step = 129 train_loss: 0.0539538 val_loss: 0.05242843\n",
      "Step = 130 train_loss: 0.05388198 val_loss: 0.052360337\n",
      "Step = 131 train_loss: 0.053053454 val_loss: 0.05232016\n",
      "Step = 132 train_loss: 0.052944787 val_loss: 0.052293055\n",
      "Step = 133 train_loss: 0.052996818 val_loss: 0.052256748\n",
      "Step = 134 train_loss: 0.053132676 val_loss: 0.052205853\n",
      "Step = 135 train_loss: 0.053605042 val_loss: 0.05212117\n",
      "Step = 136 train_loss: 0.053260054 val_loss: 0.052010026\n",
      "Step = 137 train_loss: 0.05334753 val_loss: 0.051874366\n",
      "Step = 138 train_loss: 0.052599587 val_loss: 0.051714513\n",
      "Step = 139 train_loss: 0.052633148 val_loss: 0.05156892\n",
      "Step = 140 train_loss: 0.05248136 val_loss: 0.05140483\n",
      "Step = 141 train_loss: 0.052837893 val_loss: 0.051204465\n",
      "Step = 142 train_loss: 0.05270128 val_loss: 0.051037967\n",
      "Step = 143 train_loss: 0.05368233 val_loss: 0.050857794\n",
      "Step = 144 train_loss: 0.052194998 val_loss: 0.050690528\n",
      "Step = 145 train_loss: 0.053041928 val_loss: 0.050554097\n",
      "Step = 146 train_loss: 0.051881872 val_loss: 0.050461467\n",
      "Step = 147 train_loss: 0.0527469 val_loss: 0.05040794\n",
      "Step = 148 train_loss: 0.052488983 val_loss: 0.050292514\n",
      "Step = 149 train_loss: 0.05220224 val_loss: 0.05016963\n",
      "Step = 150 train_loss: 0.051458243 val_loss: 0.050009552\n",
      "Step = 151 train_loss: 0.051615797 val_loss: 0.049856838\n",
      "Step = 152 train_loss: 0.051769946 val_loss: 0.049724277\n",
      "Step = 153 train_loss: 0.052278135 val_loss: 0.049575962\n",
      "Step = 154 train_loss: 0.051909465 val_loss: 0.049466785\n",
      "Step = 155 train_loss: 0.05170811 val_loss: 0.04941502\n",
      "Step = 156 train_loss: 0.051878616 val_loss: 0.049423344\n",
      "Step = 157 train_loss: 0.05141604 val_loss: 0.049450576\n",
      "Step = 158 train_loss: 0.051313657 val_loss: 0.04944655\n",
      "Step = 159 train_loss: 0.05101283 val_loss: 0.0494952\n",
      "Step = 160 train_loss: 0.050083876 val_loss: 0.049509794\n",
      "Step = 161 train_loss: 0.050931536 val_loss: 0.049549162\n",
      "Step = 162 train_loss: 0.050882462 val_loss: 0.049528476\n",
      "Step = 163 train_loss: 0.050660662 val_loss: 0.049475588\n",
      "Step = 164 train_loss: 0.050047852 val_loss: 0.049351316\n",
      "Step = 165 train_loss: 0.04980986 val_loss: 0.04924086\n",
      "Step = 166 train_loss: 0.051195458 val_loss: 0.04911743\n",
      "Step = 167 train_loss: 0.050515294 val_loss: 0.048985038\n",
      "Step = 168 train_loss: 0.049885295 val_loss: 0.04883316\n",
      "Step = 169 train_loss: 0.05015343 val_loss: 0.04874297\n",
      "Step = 170 train_loss: 0.050167684 val_loss: 0.048622936\n",
      "Step = 171 train_loss: 0.04929234 val_loss: 0.04858258\n",
      "Step = 172 train_loss: 0.04964654 val_loss: 0.04848824\n",
      "Step = 173 train_loss: 0.05036292 val_loss: 0.0484249\n",
      "Step = 174 train_loss: 0.04846292 val_loss: 0.048333082\n",
      "Step = 175 train_loss: 0.04876003 val_loss: 0.048198525\n",
      "Step = 176 train_loss: 0.04938733 val_loss: 0.048065618\n",
      "Step = 177 train_loss: 0.048646472 val_loss: 0.047903128\n",
      "Step = 178 train_loss: 0.048895296 val_loss: 0.047791485\n",
      "Step = 179 train_loss: 0.048916735 val_loss: 0.04764953\n",
      "Step = 180 train_loss: 0.048494633 val_loss: 0.047517344\n",
      "Step = 181 train_loss: 0.04878915 val_loss: 0.047356546\n",
      "Step = 182 train_loss: 0.048337743 val_loss: 0.047219224\n",
      "Step = 183 train_loss: 0.048454262 val_loss: 0.047056414\n",
      "Step = 184 train_loss: 0.04898673 val_loss: 0.046890326\n",
      "Step = 185 train_loss: 0.048027515 val_loss: 0.046722855\n",
      "Step = 186 train_loss: 0.04722204 val_loss: 0.046526633\n",
      "Step = 187 train_loss: 0.04804649 val_loss: 0.04631642\n",
      "Step = 188 train_loss: 0.047589857 val_loss: 0.046159342\n",
      "Step = 189 train_loss: 0.04611899 val_loss: 0.0460157\n",
      "Step = 190 train_loss: 0.04758162 val_loss: 0.04592513\n",
      "Step = 191 train_loss: 0.045888383 val_loss: 0.045776837\n",
      "Step = 192 train_loss: 0.047622286 val_loss: 0.0457804\n",
      "Step = 193 train_loss: 0.046668753 val_loss: 0.045895558\n",
      "Step = 194 train_loss: 0.046380088 val_loss: 0.04597121\n",
      "Step = 195 train_loss: 0.046345353 val_loss: 0.04588708\n",
      "Step = 196 train_loss: 0.045120534 val_loss: 0.045709763\n",
      "Step = 197 train_loss: 0.046246443 val_loss: 0.04551647\n",
      "Step = 198 train_loss: 0.046038117 val_loss: 0.04538434\n",
      "Step = 199 train_loss: 0.044820845 val_loss: 0.045253213\n",
      "Step = 200 train_loss: 0.045488045 val_loss: 0.04514525\n",
      "Step = 201 train_loss: 0.04668305 val_loss: 0.04499114\n",
      "Step = 202 train_loss: 0.046150506 val_loss: 0.044802558\n",
      "Step = 203 train_loss: 0.045959458 val_loss: 0.044573076\n",
      "Step = 204 train_loss: 0.045018546 val_loss: 0.044299833\n",
      "Step = 205 train_loss: 0.045161374 val_loss: 0.04402559\n",
      "Step = 206 train_loss: 0.045362454 val_loss: 0.04364862\n",
      "Step = 207 train_loss: 0.04529477 val_loss: 0.04322715\n",
      "Step = 208 train_loss: 0.045699418 val_loss: 0.042843733\n",
      "Step = 209 train_loss: 0.04450147 val_loss: 0.042777337\n",
      "Step = 210 train_loss: 0.045250647 val_loss: 0.042508412\n",
      "Step = 211 train_loss: 0.04409538 val_loss: 0.042243302\n",
      "Step = 212 train_loss: 0.044885594 val_loss: 0.04207957\n",
      "Step = 213 train_loss: 0.044545785 val_loss: 0.04198087\n",
      "Step = 214 train_loss: 0.04269221 val_loss: 0.042035017\n",
      "Step = 215 train_loss: 0.0446471 val_loss: 0.042037018\n",
      "Step = 216 train_loss: 0.042712606 val_loss: 0.042014413\n",
      "Step = 217 train_loss: 0.04494051 val_loss: 0.042079005\n",
      "Step = 218 train_loss: 0.043618735 val_loss: 0.042153973\n",
      "Step = 219 train_loss: 0.042647794 val_loss: 0.04232263\n",
      "Step = 220 train_loss: 0.043850128 val_loss: 0.042311348\n",
      "Step = 221 train_loss: 0.043999914 val_loss: 0.042304046\n",
      "Step = 222 train_loss: 0.04333023 val_loss: 0.04223148\n",
      "Step = 223 train_loss: 0.042825308 val_loss: 0.042143486\n",
      "Step = 224 train_loss: 0.042371932 val_loss: 0.042055152\n",
      "Step = 225 train_loss: 0.043651942 val_loss: 0.041928183\n",
      "Step = 226 train_loss: 0.04243844 val_loss: 0.04180714\n",
      "Step = 227 train_loss: 0.042105034 val_loss: 0.041641984\n",
      "Step = 228 train_loss: 0.042520422 val_loss: 0.041412726\n",
      "Step = 229 train_loss: 0.041804373 val_loss: 0.041184317\n",
      "Step = 230 train_loss: 0.042238653 val_loss: 0.04089339\n",
      "Step = 231 train_loss: 0.041692555 val_loss: 0.040615458\n",
      "Step = 232 train_loss: 0.043270662 val_loss: 0.04048868\n",
      "Step = 233 train_loss: 0.04261094 val_loss: 0.040205825\n",
      "Step = 234 train_loss: 0.043043587 val_loss: 0.04004784\n",
      "Step = 235 train_loss: 0.041463576 val_loss: 0.039920803\n",
      "Step = 236 train_loss: 0.041422375 val_loss: 0.0398284\n",
      "Step = 237 train_loss: 0.042135473 val_loss: 0.03976943\n",
      "Step = 238 train_loss: 0.041474182 val_loss: 0.03978934\n",
      "Step = 239 train_loss: 0.041067142 val_loss: 0.04002189\n",
      "Step = 240 train_loss: 0.04120304 val_loss: 0.04019004\n",
      "Step = 241 train_loss: 0.039997254 val_loss: 0.04031978\n",
      "Step = 242 train_loss: 0.04117712 val_loss: 0.040348265\n",
      "Step = 243 train_loss: 0.042398836 val_loss: 0.04038247\n",
      "Step = 244 train_loss: 0.04138214 val_loss: 0.04031762\n",
      "Step = 245 train_loss: 0.04182635 val_loss: 0.040173937\n",
      "Step = 246 train_loss: 0.040764138 val_loss: 0.040065505\n",
      "Step = 247 train_loss: 0.03995931 val_loss: 0.039905995\n",
      "Step = 248 train_loss: 0.038994238 val_loss: 0.039606657\n",
      "Step = 249 train_loss: 0.038563833 val_loss: 0.039383184\n",
      "Step = 250 train_loss: 0.040275875 val_loss: 0.03919341\n",
      "Step = 251 train_loss: 0.038301334 val_loss: 0.039088067\n",
      "Step = 252 train_loss: 0.040064752 val_loss: 0.039030626\n",
      "Step = 253 train_loss: 0.039846383 val_loss: 0.039069932\n",
      "Step = 254 train_loss: 0.03884586 val_loss: 0.039073747\n",
      "Step = 255 train_loss: 0.040914495 val_loss: 0.0392854\n",
      "Step = 256 train_loss: 0.04038824 val_loss: 0.03942009\n",
      "Step = 257 train_loss: 0.038296178 val_loss: 0.039315708\n",
      "Step = 258 train_loss: 0.039395086 val_loss: 0.03908373\n",
      "Step = 259 train_loss: 0.039994337 val_loss: 0.03882851\n",
      "Step = 260 train_loss: 0.038868587 val_loss: 0.038590617\n",
      "Step = 261 train_loss: 0.037742004 val_loss: 0.03834835\n",
      "Step = 262 train_loss: 0.037630294 val_loss: 0.038144827\n",
      "Step = 263 train_loss: 0.03797754 val_loss: 0.037948724\n",
      "Step = 264 train_loss: 0.039265797 val_loss: 0.037692606\n",
      "Step = 265 train_loss: 0.037167016 val_loss: 0.03747023\n",
      "Step = 266 train_loss: 0.03678758 val_loss: 0.037259348\n",
      "Step = 267 train_loss: 0.038246438 val_loss: 0.036977317\n",
      "Step = 268 train_loss: 0.038469978 val_loss: 0.03678086\n",
      "Step = 269 train_loss: 0.03739199 val_loss: 0.036482494\n",
      "Step = 270 train_loss: 0.037418433 val_loss: 0.036166575\n",
      "Step = 271 train_loss: 0.035917185 val_loss: 0.035827048\n",
      "Step = 272 train_loss: 0.035245497 val_loss: 0.035544045\n",
      "Step = 273 train_loss: 0.037493043 val_loss: 0.03511668\n",
      "Step = 274 train_loss: 0.03792627 val_loss: 0.034770865\n",
      "Step = 275 train_loss: 0.0356104 val_loss: 0.03459628\n",
      "Step = 276 train_loss: 0.03705657 val_loss: 0.0351039\n",
      "Step = 277 train_loss: 0.035777207 val_loss: 0.035539296\n",
      "Step = 278 train_loss: 0.035535526 val_loss: 0.03566354\n",
      "Step = 279 train_loss: 0.03718993 val_loss: 0.035667967\n",
      "Step = 280 train_loss: 0.03472404 val_loss: 0.035586394\n",
      "Step = 281 train_loss: 0.036392145 val_loss: 0.035518672\n",
      "Step = 282 train_loss: 0.036255673 val_loss: 0.035396725\n",
      "Step = 283 train_loss: 0.03432859 val_loss: 0.035261717\n",
      "Step = 284 train_loss: 0.036207788 val_loss: 0.035124976\n",
      "Step = 285 train_loss: 0.035825923 val_loss: 0.034942716\n",
      "Step = 286 train_loss: 0.036627013 val_loss: 0.034765877\n",
      "Step = 287 train_loss: 0.033701744 val_loss: 0.034633007\n",
      "Step = 288 train_loss: 0.033678576 val_loss: 0.034467723\n",
      "Step = 289 train_loss: 0.035019405 val_loss: 0.03434334\n",
      "Step = 290 train_loss: 0.03390077 val_loss: 0.03421499\n",
      "Step = 291 train_loss: 0.034001745 val_loss: 0.03407683\n",
      "Step = 292 train_loss: 0.034302928 val_loss: 0.034017846\n",
      "Step = 293 train_loss: 0.03324163 val_loss: 0.034039356\n",
      "Step = 294 train_loss: 0.03508871 val_loss: 0.03399999\n",
      "Step = 295 train_loss: 0.03497482 val_loss: 0.033688404\n",
      "Step = 296 train_loss: 0.03505273 val_loss: 0.033316787\n",
      "Step = 297 train_loss: 0.034092583 val_loss: 0.032924935\n",
      "Step = 298 train_loss: 0.033864144 val_loss: 0.032537717\n",
      "Step = 299 train_loss: 0.033390574 val_loss: 0.032209333\n",
      "Step = 300 train_loss: 0.03402059 val_loss: 0.031971592\n",
      "Step = 301 train_loss: 0.03503085 val_loss: 0.031789865\n",
      "Step = 302 train_loss: 0.033919837 val_loss: 0.031627968\n",
      "Step = 303 train_loss: 0.034002576 val_loss: 0.03141509\n",
      "Step = 304 train_loss: 0.03173179 val_loss: 0.031118808\n",
      "Step = 305 train_loss: 0.032336958 val_loss: 0.030896408\n",
      "Step = 306 train_loss: 0.0327326 val_loss: 0.030653294\n",
      "Step = 307 train_loss: 0.03360887 val_loss: 0.030391935\n",
      "Step = 308 train_loss: 0.03298923 val_loss: 0.030220736\n",
      "Step = 309 train_loss: 0.033498604 val_loss: 0.030043114\n",
      "Step = 310 train_loss: 0.03226072 val_loss: 0.029894626\n",
      "Step = 311 train_loss: 0.032351933 val_loss: 0.029823018\n",
      "Step = 312 train_loss: 0.03192501 val_loss: 0.029717876\n",
      "Step = 313 train_loss: 0.03274002 val_loss: 0.02968072\n",
      "Step = 314 train_loss: 0.034612514 val_loss: 0.029642532\n",
      "Step = 315 train_loss: 0.033350095 val_loss: 0.02960188\n",
      "Step = 316 train_loss: 0.034326933 val_loss: 0.029428491\n",
      "Step = 317 train_loss: 0.031210948 val_loss: 0.029271739\n",
      "Step = 318 train_loss: 0.03224913 val_loss: 0.029142601\n",
      "Step = 319 train_loss: 0.029754868 val_loss: 0.02895021\n",
      "Step = 320 train_loss: 0.03146995 val_loss: 0.02869942\n",
      "Step = 321 train_loss: 0.033557203 val_loss: 0.028446803\n",
      "Step = 322 train_loss: 0.03183038 val_loss: 0.02821572\n",
      "Step = 323 train_loss: 0.02955837 val_loss: 0.027895195\n",
      "Step = 324 train_loss: 0.031009953 val_loss: 0.027621869\n",
      "Step = 325 train_loss: 0.031083597 val_loss: 0.027172918\n",
      "Step = 326 train_loss: 0.031958748 val_loss: 0.026847534\n",
      "Step = 327 train_loss: 0.032278422 val_loss: 0.026623333\n",
      "Step = 328 train_loss: 0.032138016 val_loss: 0.02645651\n",
      "Step = 329 train_loss: 0.031307045 val_loss: 0.02635551\n",
      "Step = 330 train_loss: 0.033600688 val_loss: 0.026303453\n",
      "Step = 331 train_loss: 0.031092841 val_loss: 0.026333965\n",
      "Step = 332 train_loss: 0.030946668 val_loss: 0.026396746\n",
      "Step = 333 train_loss: 0.032191675 val_loss: 0.026411656\n",
      "Step = 334 train_loss: 0.030566623 val_loss: 0.02653709\n",
      "Step = 335 train_loss: 0.031491153 val_loss: 0.026585404\n",
      "Step = 336 train_loss: 0.029316906 val_loss: 0.0266951\n",
      "Step = 337 train_loss: 0.032247093 val_loss: 0.02616473\n",
      "Step = 338 train_loss: 0.031618617 val_loss: 0.025779625\n",
      "Step = 339 train_loss: 0.032425914 val_loss: 0.025510719\n",
      "Step = 340 train_loss: 0.031223645 val_loss: 0.025285497\n",
      "Step = 341 train_loss: 0.030342745 val_loss: 0.025050413\n",
      "Step = 342 train_loss: 0.032665357 val_loss: 0.024940971\n",
      "Step = 343 train_loss: 0.030966096 val_loss: 0.02485379\n",
      "Step = 344 train_loss: 0.030446716 val_loss: 0.02476203\n",
      "Step = 345 train_loss: 0.03006078 val_loss: 0.024731558\n",
      "Step = 346 train_loss: 0.030944638 val_loss: 0.024719777\n",
      "Step = 347 train_loss: 0.03111313 val_loss: 0.024677556\n",
      "Step = 348 train_loss: 0.029489994 val_loss: 0.024720958\n",
      "Step = 349 train_loss: 0.0316645 val_loss: 0.024750533\n",
      "Step = 350 train_loss: 0.03135417 val_loss: 0.024764175\n",
      "Step = 351 train_loss: 0.030916782 val_loss: 0.024723718\n",
      "Step = 352 train_loss: 0.030981133 val_loss: 0.024755152\n",
      "Step = 353 train_loss: 0.031225888 val_loss: 0.024732694\n",
      "Step = 354 train_loss: 0.028782947 val_loss: 0.024786312\n",
      "Step = 355 train_loss: 0.03170321 val_loss: 0.024886675\n",
      "Step = 356 train_loss: 0.031527136 val_loss: 0.024961244\n",
      "Step = 357 train_loss: 0.028393546 val_loss: 0.024956772\n",
      "Step = 358 train_loss: 0.029378368 val_loss: 0.02494106\n",
      "Step = 359 train_loss: 0.03084931 val_loss: 0.024926133\n",
      "Step = 360 train_loss: 0.026295708 val_loss: 0.024893295\n",
      "Step = 361 train_loss: 0.029345904 val_loss: 0.02488169\n",
      "Step = 362 train_loss: 0.029083563 val_loss: 0.024869869\n",
      "Step = 363 train_loss: 0.028904397 val_loss: 0.02483923\n",
      "Step = 364 train_loss: 0.029547768 val_loss: 0.02483777\n",
      "Step = 365 train_loss: 0.028309418 val_loss: 0.024782643\n",
      "Step = 366 train_loss: 0.03158624 val_loss: 0.024712568\n",
      "Step = 367 train_loss: 0.029468032 val_loss: 0.024621285\n",
      "Step = 368 train_loss: 0.029796313 val_loss: 0.024538387\n",
      "Step = 369 train_loss: 0.028854689 val_loss: 0.024437122\n",
      "Step = 370 train_loss: 0.03188608 val_loss: 0.02436044\n",
      "Step = 371 train_loss: 0.028336314 val_loss: 0.024263056\n",
      "Step = 372 train_loss: 0.030314859 val_loss: 0.024192533\n",
      "Step = 373 train_loss: 0.028510105 val_loss: 0.024109162\n",
      "Step = 374 train_loss: 0.029094772 val_loss: 0.024041072\n",
      "Step = 375 train_loss: 0.028753119 val_loss: 0.02397172\n",
      "Step = 376 train_loss: 0.028791163 val_loss: 0.0240226\n",
      "Step = 377 train_loss: 0.029250966 val_loss: 0.024036594\n",
      "Step = 378 train_loss: 0.02880225 val_loss: 0.024015471\n",
      "Step = 379 train_loss: 0.031302042 val_loss: 0.023953866\n",
      "Step = 380 train_loss: 0.03124197 val_loss: 0.023883931\n",
      "Step = 381 train_loss: 0.030485447 val_loss: 0.023799043\n",
      "Step = 382 train_loss: 0.02904858 val_loss: 0.023731174\n",
      "Step = 383 train_loss: 0.028781932 val_loss: 0.023657031\n",
      "Step = 384 train_loss: 0.030188283 val_loss: 0.023577556\n",
      "Step = 385 train_loss: 0.028701223 val_loss: 0.023473334\n",
      "Step = 386 train_loss: 0.030154295 val_loss: 0.02340194\n",
      "Step = 387 train_loss: 0.029569002 val_loss: 0.023298992\n",
      "Step = 388 train_loss: 0.02976001 val_loss: 0.02320551\n",
      "Step = 389 train_loss: 0.029709028 val_loss: 0.023098774\n",
      "Step = 390 train_loss: 0.027055304 val_loss: 0.022987489\n",
      "Step = 391 train_loss: 0.028161269 val_loss: 0.022902166\n",
      "Step = 392 train_loss: 0.030206067 val_loss: 0.022849863\n",
      "Step = 393 train_loss: 0.029262276 val_loss: 0.022810908\n",
      "Step = 394 train_loss: 0.030881844 val_loss: 0.022796007\n",
      "Step = 395 train_loss: 0.02779582 val_loss: 0.022748653\n",
      "Step = 396 train_loss: 0.028686222 val_loss: 0.022697035\n",
      "Step = 397 train_loss: 0.028769834 val_loss: 0.022617893\n",
      "Step = 398 train_loss: 0.028747793 val_loss: 0.022562282\n",
      "Step = 399 train_loss: 0.029421471 val_loss: 0.022476379\n",
      "Step = 400 train_loss: 0.02951762 val_loss: 0.022377964\n",
      "Step = 401 train_loss: 0.029477878 val_loss: 0.022288669\n",
      "Step = 402 train_loss: 0.03047092 val_loss: 0.02226291\n",
      "Step = 403 train_loss: 0.025977304 val_loss: 0.022192447\n",
      "Step = 404 train_loss: 0.029045364 val_loss: 0.022202702\n",
      "Step = 405 train_loss: 0.029035628 val_loss: 0.022617415\n",
      "Step = 406 train_loss: 0.029141644 val_loss: 0.022823757\n",
      "Step = 407 train_loss: 0.030877424 val_loss: 0.022877742\n",
      "Step = 408 train_loss: 0.029029919 val_loss: 0.022909936\n",
      "Step = 409 train_loss: 0.030831654 val_loss: 0.022940269\n",
      "Step = 410 train_loss: 0.029614966 val_loss: 0.022971557\n",
      "Step = 411 train_loss: 0.028894924 val_loss: 0.023020357\n",
      "Step = 412 train_loss: 0.029826066 val_loss: 0.023093106\n",
      "Step = 413 train_loss: 0.032810215 val_loss: 0.023112698\n",
      "Step = 414 train_loss: 0.030475616 val_loss: 0.023105528\n",
      "Step = 415 train_loss: 0.029937925 val_loss: 0.023106072\n",
      "Step = 416 train_loss: 0.028214622 val_loss: 0.023094209\n",
      "Step = 417 train_loss: 0.029056499 val_loss: 0.023087747\n",
      "Step = 418 train_loss: 0.02856535 val_loss: 0.023069812\n",
      "Step = 419 train_loss: 0.029190393 val_loss: 0.023024507\n",
      "Step = 420 train_loss: 0.028237592 val_loss: 0.022992574\n",
      "Step = 421 train_loss: 0.028979467 val_loss: 0.022969626\n",
      "Step = 422 train_loss: 0.028753627 val_loss: 0.022926403\n",
      "Step = 423 train_loss: 0.029018246 val_loss: 0.022874683\n",
      "Step = 424 train_loss: 0.028785823 val_loss: 0.022853555\n",
      "Step = 425 train_loss: 0.029401692 val_loss: 0.022823788\n",
      "Step = 426 train_loss: 0.028415592 val_loss: 0.02280986\n",
      "Step = 427 train_loss: 0.029226858 val_loss: 0.022746123\n",
      "Step = 428 train_loss: 0.026847826 val_loss: 0.022698225\n",
      "Step = 429 train_loss: 0.030033175 val_loss: 0.022670902\n",
      "Step = 430 train_loss: 0.028750164 val_loss: 0.022642259\n",
      "Step = 431 train_loss: 0.028752001 val_loss: 0.022610554\n",
      "Step = 432 train_loss: 0.02870647 val_loss: 0.02261241\n",
      "Step = 433 train_loss: 0.027886141 val_loss: 0.022578673\n",
      "Step = 434 train_loss: 0.030715125 val_loss: 0.022564497\n",
      "Step = 435 train_loss: 0.030540168 val_loss: 0.022573799\n",
      "Step = 436 train_loss: 0.029440634 val_loss: 0.022542499\n",
      "Step = 437 train_loss: 0.028116833 val_loss: 0.02256824\n",
      "Step = 438 train_loss: 0.027467858 val_loss: 0.022607006\n",
      "Step = 439 train_loss: 0.030531667 val_loss: 0.022654083\n",
      "Step = 440 train_loss: 0.027838053 val_loss: 0.022726439\n",
      "Step = 441 train_loss: 0.029138308 val_loss: 0.022783654\n",
      "Step = 442 train_loss: 0.025850387 val_loss: 0.022862498\n",
      "Step = 443 train_loss: 0.02755451 val_loss: 0.023057384\n",
      "Step = 444 train_loss: 0.028742867 val_loss: 0.023434242\n",
      "Step = 445 train_loss: 0.028037073 val_loss: 0.023976877\n",
      "Step = 446 train_loss: 0.029995369 val_loss: 0.024380531\n",
      "Step = 447 train_loss: 0.028466603 val_loss: 0.024655484\n",
      "Step = 448 train_loss: 0.028746027 val_loss: 0.024811978\n",
      "Step = 449 train_loss: 0.030491484 val_loss: 0.02486356\n",
      "Step = 450 train_loss: 0.030371143 val_loss: 0.02474784\n",
      "Step = 451 train_loss: 0.02868567 val_loss: 0.024542067\n",
      "Step = 452 train_loss: 0.030357946 val_loss: 0.02432287\n",
      "Step = 453 train_loss: 0.029287545 val_loss: 0.02417128\n",
      "Step = 454 train_loss: 0.02975391 val_loss: 0.024057236\n",
      "Step = 455 train_loss: 0.030232511 val_loss: 0.023882492\n",
      "Step = 456 train_loss: 0.030300071 val_loss: 0.02384709\n",
      "Step = 457 train_loss: 0.027318995 val_loss: 0.023773314\n",
      "Step = 458 train_loss: 0.030255703 val_loss: 0.023721129\n",
      "Step = 459 train_loss: 0.028540842 val_loss: 0.023615463\n",
      "Step = 460 train_loss: 0.02924915 val_loss: 0.02356476\n",
      "Step = 461 train_loss: 0.028558599 val_loss: 0.023520032\n",
      "Step = 462 train_loss: 0.028540662 val_loss: 0.023468034\n",
      "Step = 463 train_loss: 0.030857418 val_loss: 0.02342654\n",
      "Step = 464 train_loss: 0.030091051 val_loss: 0.023412677\n",
      "Step = 465 train_loss: 0.029578583 val_loss: 0.023386428\n",
      "Step = 466 train_loss: 0.03084514 val_loss: 0.023367442\n",
      "Step = 467 train_loss: 0.029261034 val_loss: 0.023319447\n",
      "Step = 468 train_loss: 0.029265357 val_loss: 0.023289068\n",
      "Step = 469 train_loss: 0.027246846 val_loss: 0.023263339\n",
      "Step = 470 train_loss: 0.030423544 val_loss: 0.023222405\n",
      "Step = 471 train_loss: 0.028261092 val_loss: 0.023159092\n",
      "Step = 472 train_loss: 0.030006953 val_loss: 0.023168877\n",
      "Step = 473 train_loss: 0.029348962 val_loss: 0.02322391\n",
      "Step = 474 train_loss: 0.030136574 val_loss: 0.023272611\n",
      "Step = 475 train_loss: 0.030356957 val_loss: 0.023329018\n",
      "Step = 476 train_loss: 0.028452476 val_loss: 0.023328738\n",
      "Step = 477 train_loss: 0.027904417 val_loss: 0.023361417\n",
      "Step = 478 train_loss: 0.028961042 val_loss: 0.023407834\n",
      "Step = 479 train_loss: 0.028756173 val_loss: 0.023431424\n",
      "Step = 480 train_loss: 0.02972273 val_loss: 0.02347258\n",
      "Step = 481 train_loss: 0.028659515 val_loss: 0.023524217\n",
      "Step = 482 train_loss: 0.029523749 val_loss: 0.023545643\n",
      "Step = 483 train_loss: 0.028307823 val_loss: 0.023566721\n",
      "Step = 484 train_loss: 0.02962774 val_loss: 0.023575412\n",
      "Step = 485 train_loss: 0.029745106 val_loss: 0.02357236\n",
      "Step = 486 train_loss: 0.029298006 val_loss: 0.023594333\n",
      "Step = 487 train_loss: 0.02936547 val_loss: 0.023594124\n",
      "Step = 488 train_loss: 0.02910769 val_loss: 0.023628851\n",
      "Step = 489 train_loss: 0.02949667 val_loss: 0.023666939\n",
      "Step = 490 train_loss: 0.029789003 val_loss: 0.023628298\n",
      "Step = 491 train_loss: 0.027436556 val_loss: 0.023637997\n",
      "Step = 492 train_loss: 0.028475413 val_loss: 0.023617642\n",
      "Step = 493 train_loss: 0.029013475 val_loss: 0.023591878\n",
      "Step = 494 train_loss: 0.028797345 val_loss: 0.023588654\n",
      "Step = 495 train_loss: 0.030144494 val_loss: 0.02357238\n",
      "Step = 496 train_loss: 0.030703647 val_loss: 0.023589512\n",
      "Step = 497 train_loss: 0.029211877 val_loss: 0.023596406\n",
      "Step = 498 train_loss: 0.02745166 val_loss: 0.023575062\n",
      "Step = 499 train_loss: 0.028799562 val_loss: 0.02355122\n",
      "Step = 500 train_loss: 0.030710556 val_loss: 0.023513913\n",
      "Step = 501 train_loss: 0.03160921 val_loss: 0.023173321\n",
      "Step = 502 train_loss: 0.026301187 val_loss: 0.022786967\n",
      "Step = 503 train_loss: 0.030698268 val_loss: 0.022535704\n",
      "Step = 504 train_loss: 0.0274542 val_loss: 0.022377348\n",
      "Step = 505 train_loss: 0.03127822 val_loss: 0.022375487\n",
      "Step = 506 train_loss: 0.028453259 val_loss: 0.02242904\n",
      "Step = 507 train_loss: 0.030905776 val_loss: 0.022519499\n",
      "Step = 508 train_loss: 0.030228117 val_loss: 0.022597041\n",
      "Step = 509 train_loss: 0.029721169 val_loss: 0.022693008\n",
      "Step = 510 train_loss: 0.02947952 val_loss: 0.02279067\n",
      "Step = 511 train_loss: 0.027874775 val_loss: 0.022820601\n",
      "Step = 512 train_loss: 0.027861264 val_loss: 0.022853384\n",
      "Step = 513 train_loss: 0.029454919 val_loss: 0.022866976\n",
      "Step = 514 train_loss: 0.030297378 val_loss: 0.02291909\n",
      "Step = 515 train_loss: 0.029582206 val_loss: 0.022934569\n",
      "Step = 516 train_loss: 0.02910557 val_loss: 0.022972481\n",
      "Step = 517 train_loss: 0.02962863 val_loss: 0.023000022\n",
      "Step = 518 train_loss: 0.027136415 val_loss: 0.023016881\n",
      "Step = 519 train_loss: 0.030915182 val_loss: 0.0230275\n",
      "Step = 520 train_loss: 0.028930135 val_loss: 0.023043144\n",
      "Step = 521 train_loss: 0.029900035 val_loss: 0.023070173\n",
      "Step = 522 train_loss: 0.028229676 val_loss: 0.023065\n",
      "Step = 523 train_loss: 0.028145911 val_loss: 0.023068465\n",
      "Step = 524 train_loss: 0.02636245 val_loss: 0.023088545\n",
      "Step = 525 train_loss: 0.02905202 val_loss: 0.023084411\n",
      "Step = 526 train_loss: 0.029055681 val_loss: 0.02309046\n",
      "Step = 527 train_loss: 0.02958968 val_loss: 0.023106782\n",
      "Step = 528 train_loss: 0.029329704 val_loss: 0.023129385\n",
      "Step = 529 train_loss: 0.030173825 val_loss: 0.023147956\n",
      "Step = 530 train_loss: 0.02826816 val_loss: 0.023168579\n",
      "Step = 531 train_loss: 0.029311093 val_loss: 0.023187922\n",
      "Step = 532 train_loss: 0.030771302 val_loss: 0.023224685\n",
      "Step = 533 train_loss: 0.030569991 val_loss: 0.02323989\n",
      "Step = 534 train_loss: 0.02983075 val_loss: 0.023241311\n",
      "Step = 535 train_loss: 0.02726638 val_loss: 0.02323168\n",
      "Step = 536 train_loss: 0.028872956 val_loss: 0.023218926\n",
      "Step = 537 train_loss: 0.027387084 val_loss: 0.023221081\n",
      "Step = 538 train_loss: 0.029264655 val_loss: 0.023253795\n",
      "Step = 539 train_loss: 0.029453553 val_loss: 0.023303894\n",
      "Step = 540 train_loss: 0.029636374 val_loss: 0.023346703\n",
      "Step = 541 train_loss: 0.027355392 val_loss: 0.023387346\n",
      "Step = 542 train_loss: 0.028361352 val_loss: 0.023389217\n",
      "Step = 543 train_loss: 0.027586864 val_loss: 0.023372946\n",
      "Step = 544 train_loss: 0.028800895 val_loss: 0.023376131\n",
      "Step = 545 train_loss: 0.028928924 val_loss: 0.023345977\n",
      "Step = 546 train_loss: 0.02870486 val_loss: 0.023316948\n",
      "Step = 547 train_loss: 0.029922638 val_loss: 0.023288079\n",
      "Step = 548 train_loss: 0.029629959 val_loss: 0.02326359\n",
      "Step = 549 train_loss: 0.027886234 val_loss: 0.02324243\n",
      "Step = 550 train_loss: 0.028912358 val_loss: 0.02322069\n",
      "Step = 551 train_loss: 0.027971823 val_loss: 0.02321175\n",
      "Step = 552 train_loss: 0.0308001 val_loss: 0.023197714\n",
      "Step = 553 train_loss: 0.029766668 val_loss: 0.023191264\n",
      "Step = 554 train_loss: 0.029055793 val_loss: 0.023177564\n",
      "Step = 555 train_loss: 0.02862473 val_loss: 0.023169419\n",
      "Step = 556 train_loss: 0.031278715 val_loss: 0.023157492\n",
      "Step = 557 train_loss: 0.028977912 val_loss: 0.023123456\n",
      "Step = 558 train_loss: 0.027294837 val_loss: 0.023083657\n",
      "Step = 559 train_loss: 0.029944178 val_loss: 0.023108896\n",
      "Step = 560 train_loss: 0.029939115 val_loss: 0.023131026\n",
      "Step = 561 train_loss: 0.028793078 val_loss: 0.023129996\n",
      "Step = 562 train_loss: 0.028794635 val_loss: 0.023141963\n",
      "Step = 563 train_loss: 0.029920788 val_loss: 0.023113148\n",
      "Step = 564 train_loss: 0.030176627 val_loss: 0.02307947\n",
      "Step = 565 train_loss: 0.027758788 val_loss: 0.023082336\n",
      "Step = 566 train_loss: 0.029101333 val_loss: 0.023090241\n",
      "Step = 567 train_loss: 0.030041981 val_loss: 0.023088206\n",
      "Step = 568 train_loss: 0.026987275 val_loss: 0.023087591\n",
      "Step = 569 train_loss: 0.029149504 val_loss: 0.023089191\n",
      "Step = 570 train_loss: 0.027098028 val_loss: 0.023130324\n",
      "Step = 571 train_loss: 0.029895104 val_loss: 0.023159107\n",
      "Step = 572 train_loss: 0.027502887 val_loss: 0.023173802\n",
      "Step = 573 train_loss: 0.031170834 val_loss: 0.023176096\n",
      "Step = 574 train_loss: 0.02867308 val_loss: 0.02316615\n",
      "Step = 575 train_loss: 0.029244287 val_loss: 0.023141554\n",
      "Step = 576 train_loss: 0.028157622 val_loss: 0.02311526\n",
      "Step = 577 train_loss: 0.029738447 val_loss: 0.023089949\n",
      "Step = 578 train_loss: 0.028065473 val_loss: 0.023034384\n",
      "Step = 579 train_loss: 0.030188017 val_loss: 0.023020096\n",
      "Step = 580 train_loss: 0.030378573 val_loss: 0.022999845\n",
      "Step = 581 train_loss: 0.029346142 val_loss: 0.022953115\n",
      "Step = 582 train_loss: 0.030918987 val_loss: 0.022933967\n",
      "Step = 583 train_loss: 0.029912706 val_loss: 0.022874806\n",
      "Step = 584 train_loss: 0.027682396 val_loss: 0.022803523\n",
      "Step = 585 train_loss: 0.028388985 val_loss: 0.022759294\n",
      "Step = 586 train_loss: 0.028702585 val_loss: 0.0227746\n",
      "Step = 587 train_loss: 0.027750127 val_loss: 0.022663493\n",
      "Step = 588 train_loss: 0.028876338 val_loss: 0.022659753\n",
      "Step = 589 train_loss: 0.029411692 val_loss: 0.02278165\n",
      "Step = 590 train_loss: 0.030185562 val_loss: 0.022918785\n",
      "Step = 591 train_loss: 0.027173035 val_loss: 0.0231818\n",
      "Step = 592 train_loss: 0.030325962 val_loss: 0.023349678\n",
      "Step = 593 train_loss: 0.03132379 val_loss: 0.023370976\n",
      "Step = 594 train_loss: 0.02928942 val_loss: 0.023378074\n",
      "Step = 595 train_loss: 0.029480807 val_loss: 0.02339438\n",
      "Step = 596 train_loss: 0.029630188 val_loss: 0.023326917\n",
      "Step = 597 train_loss: 0.028976798 val_loss: 0.023249516\n",
      "Step = 598 train_loss: 0.03109572 val_loss: 0.023253186\n",
      "Step = 599 train_loss: 0.027035428 val_loss: 0.023268878\n",
      "Step = 600 train_loss: 0.029248102 val_loss: 0.023282016\n",
      "Step = 601 train_loss: 0.029429294 val_loss: 0.02325597\n",
      "Step = 602 train_loss: 0.028831065 val_loss: 0.023243738\n",
      "Step = 603 train_loss: 0.029384647 val_loss: 0.023233132\n",
      "Step = 604 train_loss: 0.030860143 val_loss: 0.023250675\n",
      "Step = 605 train_loss: 0.02870007 val_loss: 0.023290507\n",
      "Step = 606 train_loss: 0.02946666 val_loss: 0.023264484\n",
      "Step = 607 train_loss: 0.026138356 val_loss: 0.023259707\n",
      "Step = 608 train_loss: 0.027626706 val_loss: 0.023279369\n",
      "Step = 609 train_loss: 0.029860806 val_loss: 0.023283822\n",
      "Step = 610 train_loss: 0.027129084 val_loss: 0.023309598\n",
      "Step = 611 train_loss: 0.029382441 val_loss: 0.023306541\n",
      "Step = 612 train_loss: 0.025916435 val_loss: 0.023318341\n",
      "Step = 613 train_loss: 0.027120449 val_loss: 0.023270888\n",
      "Step = 614 train_loss: 0.028512584 val_loss: 0.023243688\n",
      "Step = 615 train_loss: 0.028616728 val_loss: 0.023232127\n",
      "Step = 616 train_loss: 0.028225172 val_loss: 0.023209626\n",
      "Step = 617 train_loss: 0.030343056 val_loss: 0.023185894\n",
      "Step = 618 train_loss: 0.027230121 val_loss: 0.023169445\n",
      "Step = 619 train_loss: 0.03171998 val_loss: 0.023123994\n",
      "Step = 620 train_loss: 0.027485099 val_loss: 0.02308927\n",
      "Step = 621 train_loss: 0.029867765 val_loss: 0.023033453\n",
      "Step = 622 train_loss: 0.028733203 val_loss: 0.023010654\n",
      "Step = 623 train_loss: 0.028155439 val_loss: 0.023007838\n",
      "Step = 624 train_loss: 0.029840754 val_loss: 0.02297023\n",
      "Step = 625 train_loss: 0.029811174 val_loss: 0.022921372\n",
      "Step = 626 train_loss: 0.031164907 val_loss: 0.022927709\n",
      "Step = 627 train_loss: 0.029102001 val_loss: 0.022953486\n",
      "Step = 628 train_loss: 0.030548811 val_loss: 0.02302488\n",
      "Step = 629 train_loss: 0.028909406 val_loss: 0.023019284\n",
      "Step = 630 train_loss: 0.03046964 val_loss: 0.02302736\n",
      "Step = 631 train_loss: 0.028144158 val_loss: 0.023031758\n",
      "Step = 632 train_loss: 0.028988184 val_loss: 0.023014132\n",
      "Step = 633 train_loss: 0.030452963 val_loss: 0.023009589\n",
      "Step = 634 train_loss: 0.031112658 val_loss: 0.022990923\n",
      "Step = 635 train_loss: 0.02848227 val_loss: 0.022947468\n",
      "Step = 636 train_loss: 0.028056404 val_loss: 0.022905601\n",
      "Step = 637 train_loss: 0.02954456 val_loss: 0.022916337\n",
      "Step = 638 train_loss: 0.028230641 val_loss: 0.022912456\n",
      "Step = 639 train_loss: 0.029570095 val_loss: 0.022948995\n",
      "Step = 640 train_loss: 0.027622925 val_loss: 0.022939991\n",
      "Step = 641 train_loss: 0.02929441 val_loss: 0.022945693\n",
      "Step = 642 train_loss: 0.030175315 val_loss: 0.022977302\n",
      "Step = 643 train_loss: 0.030798199 val_loss: 0.023008198\n",
      "Step = 644 train_loss: 0.030233141 val_loss: 0.023021556\n",
      "Step = 645 train_loss: 0.028911404 val_loss: 0.023020694\n",
      "Step = 646 train_loss: 0.029480169 val_loss: 0.023017755\n",
      "Step = 647 train_loss: 0.029806118 val_loss: 0.022976123\n",
      "Step = 648 train_loss: 0.030201394 val_loss: 0.022946032\n",
      "Step = 649 train_loss: 0.030257104 val_loss: 0.022933433\n",
      "Step = 650 train_loss: 0.026620232 val_loss: 0.022948489\n",
      "Step = 651 train_loss: 0.030681202 val_loss: 0.022931442\n",
      "Step = 652 train_loss: 0.030570965 val_loss: 0.022915313\n",
      "Step = 653 train_loss: 0.028502319 val_loss: 0.02290335\n",
      "Step = 654 train_loss: 0.030606521 val_loss: 0.022862194\n",
      "Step = 655 train_loss: 0.028203068 val_loss: 0.022810899\n",
      "Step = 656 train_loss: 0.03055593 val_loss: 0.0227838\n",
      "Step = 657 train_loss: 0.02857056 val_loss: 0.022759862\n",
      "Step = 658 train_loss: 0.028147053 val_loss: 0.022733314\n",
      "Step = 659 train_loss: 0.030443508 val_loss: 0.02268845\n",
      "Step = 660 train_loss: 0.027937893 val_loss: 0.022659726\n",
      "Step = 661 train_loss: 0.030369 val_loss: 0.022641722\n",
      "Step = 662 train_loss: 0.02793078 val_loss: 0.022638373\n",
      "Step = 663 train_loss: 0.030383475 val_loss: 0.022654321\n",
      "Step = 664 train_loss: 0.02689646 val_loss: 0.022677135\n",
      "Step = 665 train_loss: 0.02931899 val_loss: 0.022640279\n",
      "Step = 666 train_loss: 0.027284406 val_loss: 0.022626806\n",
      "Step = 667 train_loss: 0.03014576 val_loss: 0.02257349\n",
      "Step = 668 train_loss: 0.028831955 val_loss: 0.022558259\n",
      "Step = 669 train_loss: 0.028597513 val_loss: 0.02257175\n",
      "Step = 670 train_loss: 0.02828029 val_loss: 0.02248978\n",
      "Step = 671 train_loss: 0.02887446 val_loss: 0.02232872\n",
      "Step = 672 train_loss: 0.02938803 val_loss: 0.022204896\n",
      "Step = 673 train_loss: 0.02700653 val_loss: 0.022247536\n",
      "Step = 674 train_loss: 0.03122097 val_loss: 0.022395322\n",
      "Step = 675 train_loss: 0.027228532 val_loss: 0.022653073\n",
      "Step = 676 train_loss: 0.028019475 val_loss: 0.023031304\n",
      "Step = 677 train_loss: 0.026957877 val_loss: 0.023349218\n",
      "Step = 678 train_loss: 0.02946104 val_loss: 0.023581255\n",
      "Step = 679 train_loss: 0.03060784 val_loss: 0.023849912\n",
      "Step = 680 train_loss: 0.027814917 val_loss: 0.024153678\n",
      "Step = 681 train_loss: 0.028735992 val_loss: 0.024300849\n",
      "Step = 682 train_loss: 0.028217955 val_loss: 0.024262909\n",
      "Step = 683 train_loss: 0.02851795 val_loss: 0.024290446\n",
      "Step = 684 train_loss: 0.029416738 val_loss: 0.024280034\n",
      "Step = 685 train_loss: 0.027556041 val_loss: 0.024127528\n",
      "Step = 686 train_loss: 0.029281426 val_loss: 0.023869522\n",
      "Step = 687 train_loss: 0.026761867 val_loss: 0.023783617\n",
      "Step = 688 train_loss: 0.026548434 val_loss: 0.02380099\n",
      "Step = 689 train_loss: 0.027506564 val_loss: 0.02397745\n",
      "Step = 690 train_loss: 0.028930021 val_loss: 0.024377488\n",
      "Step = 691 train_loss: 0.028302195 val_loss: 0.024610095\n",
      "Step = 692 train_loss: 0.027962623 val_loss: 0.024796331\n",
      "Step = 693 train_loss: 0.028422117 val_loss: 0.02479448\n",
      "Step = 694 train_loss: 0.027897278 val_loss: 0.024532579\n",
      "Step = 695 train_loss: 0.029578937 val_loss: 0.02432042\n",
      "Step = 696 train_loss: 0.02907757 val_loss: 0.023946695\n",
      "Step = 697 train_loss: 0.027983587 val_loss: 0.023582011\n",
      "Step = 698 train_loss: 0.029768365 val_loss: 0.023440992\n",
      "Step = 699 train_loss: 0.029535826 val_loss: 0.023295518\n",
      "Step = 700 train_loss: 0.027963804 val_loss: 0.023167677\n",
      "Step = 701 train_loss: 0.02877495 val_loss: 0.023059372\n",
      "Step = 702 train_loss: 0.028695274 val_loss: 0.02316294\n",
      "Step = 703 train_loss: 0.026416095 val_loss: 0.02339576\n",
      "Step = 704 train_loss: 0.030530708 val_loss: 0.023739196\n",
      "Step = 705 train_loss: 0.029369878 val_loss: 0.024065427\n",
      "Step = 706 train_loss: 0.028369505 val_loss: 0.024120452\n",
      "Step = 707 train_loss: 0.029337991 val_loss: 0.023999378\n",
      "Step = 708 train_loss: 0.024899589 val_loss: 0.02374782\n",
      "Step = 709 train_loss: 0.030649798 val_loss: 0.023542793\n",
      "Step = 710 train_loss: 0.03011348 val_loss: 0.023247793\n",
      "Step = 711 train_loss: 0.028718876 val_loss: 0.023118328\n",
      "Step = 712 train_loss: 0.025826693 val_loss: 0.023096804\n",
      "Step = 713 train_loss: 0.028486868 val_loss: 0.023175484\n",
      "Step = 714 train_loss: 0.028488753 val_loss: 0.02333403\n",
      "Step = 715 train_loss: 0.028497415 val_loss: 0.023596449\n",
      "Step = 716 train_loss: 0.027624197 val_loss: 0.023663627\n",
      "Step = 717 train_loss: 0.027973019 val_loss: 0.023755481\n",
      "Step = 718 train_loss: 0.027330419 val_loss: 0.023620276\n",
      "Step = 719 train_loss: 0.027819114 val_loss: 0.023169527\n",
      "Step = 720 train_loss: 0.027188944 val_loss: 0.022910869\n",
      "Step = 721 train_loss: 0.028485429 val_loss: 0.022798246\n",
      "Step = 722 train_loss: 0.025851201 val_loss: 0.02270746\n",
      "Step = 723 train_loss: 0.028222265 val_loss: 0.022790061\n",
      "Step = 724 train_loss: 0.026758807 val_loss: 0.022962663\n",
      "Step = 725 train_loss: 0.02826939 val_loss: 0.02311232\n",
      "Step = 726 train_loss: 0.027314143 val_loss: 0.02348671\n",
      "Step = 727 train_loss: 0.027203234 val_loss: 0.023878898\n",
      "Step = 728 train_loss: 0.029272316 val_loss: 0.02410715\n",
      "Step = 729 train_loss: 0.02767003 val_loss: 0.02400508\n",
      "Step = 730 train_loss: 0.028320758 val_loss: 0.02382626\n",
      "Step = 731 train_loss: 0.026937852 val_loss: 0.023796031\n",
      "Step = 732 train_loss: 0.025134636 val_loss: 0.023727791\n",
      "Step = 733 train_loss: 0.029098818 val_loss: 0.023708636\n",
      "Step = 734 train_loss: 0.02639066 val_loss: 0.02376847\n",
      "Step = 735 train_loss: 0.026221305 val_loss: 0.023533013\n",
      "Step = 736 train_loss: 0.02827533 val_loss: 0.023163628\n",
      "Step = 737 train_loss: 0.028049279 val_loss: 0.022618532\n",
      "Step = 738 train_loss: 0.027333826 val_loss: 0.022799237\n",
      "Step = 739 train_loss: 0.026709743 val_loss: 0.023537355\n",
      "Step = 740 train_loss: 0.025509508 val_loss: 0.024767911\n",
      "Step = 741 train_loss: 0.02669213 val_loss: 0.025270797\n",
      "Step = 742 train_loss: 0.029535957 val_loss: 0.025114428\n",
      "Step = 743 train_loss: 0.02808845 val_loss: 0.024731372\n",
      "Step = 744 train_loss: 0.02896942 val_loss: 0.024229039\n",
      "Step = 745 train_loss: 0.027550194 val_loss: 0.023292243\n",
      "Step = 746 train_loss: 0.027495323 val_loss: 0.022271983\n",
      "Step = 747 train_loss: 0.027287453 val_loss: 0.02176089\n",
      "Step = 748 train_loss: 0.027748777 val_loss: 0.021688195\n",
      "Step = 749 train_loss: 0.026925812 val_loss: 0.022139065\n",
      "Step = 750 train_loss: 0.02601397 val_loss: 0.022777075\n",
      "Step = 751 train_loss: 0.027505953 val_loss: 0.02363256\n",
      "Step = 752 train_loss: 0.027965756 val_loss: 0.024456497\n",
      "Step = 753 train_loss: 0.025824858 val_loss: 0.024734415\n",
      "Step = 754 train_loss: 0.02925041 val_loss: 0.024570031\n",
      "Step = 755 train_loss: 0.02827811 val_loss: 0.024044838\n",
      "Step = 756 train_loss: 0.028905045 val_loss: 0.023254892\n",
      "Step = 757 train_loss: 0.026927022 val_loss: 0.022107499\n",
      "Step = 758 train_loss: 0.026675176 val_loss: 0.021636195\n",
      "Step = 759 train_loss: 0.02693175 val_loss: 0.02189661\n",
      "Step = 760 train_loss: 0.025734626 val_loss: 0.02267524\n",
      "Step = 761 train_loss: 0.025420507 val_loss: 0.023317082\n",
      "Step = 762 train_loss: 0.028398119 val_loss: 0.023738705\n",
      "Step = 763 train_loss: 0.028502941 val_loss: 0.02354852\n",
      "Step = 764 train_loss: 0.02770694 val_loss: 0.022658655\n",
      "Step = 765 train_loss: 0.026320914 val_loss: 0.021866377\n",
      "Step = 766 train_loss: 0.02579976 val_loss: 0.021285156\n",
      "Step = 767 train_loss: 0.02860085 val_loss: 0.0212288\n",
      "Step = 768 train_loss: 0.029139241 val_loss: 0.021636333\n",
      "Step = 769 train_loss: 0.026782576 val_loss: 0.022608647\n",
      "Step = 770 train_loss: 0.028241638 val_loss: 0.02359677\n",
      "Step = 771 train_loss: 0.026412215 val_loss: 0.02422158\n",
      "Step = 772 train_loss: 0.027205061 val_loss: 0.024290621\n",
      "Step = 773 train_loss: 0.02915869 val_loss: 0.024056917\n",
      "Step = 774 train_loss: 0.027215205 val_loss: 0.02363888\n",
      "Step = 775 train_loss: 0.028551258 val_loss: 0.022694832\n",
      "Step = 776 train_loss: 0.026898375 val_loss: 0.021579795\n",
      "Step = 777 train_loss: 0.026761748 val_loss: 0.02108212\n",
      "Step = 778 train_loss: 0.028411506 val_loss: 0.020878403\n",
      "Step = 779 train_loss: 0.028647134 val_loss: 0.021166485\n",
      "Step = 780 train_loss: 0.026308026 val_loss: 0.021462549\n",
      "Step = 781 train_loss: 0.024979832 val_loss: 0.02196372\n",
      "Step = 782 train_loss: 0.026347775 val_loss: 0.022008002\n",
      "Step = 783 train_loss: 0.027356734 val_loss: 0.021950612\n",
      "Step = 784 train_loss: 0.02388824 val_loss: 0.021544125\n",
      "Step = 785 train_loss: 0.026846401 val_loss: 0.02177648\n",
      "Step = 786 train_loss: 0.026655627 val_loss: 0.024146283\n",
      "Step = 787 train_loss: 0.025979588 val_loss: 0.024803435\n",
      "Step = 788 train_loss: 0.02611034 val_loss: 0.02438976\n",
      "Step = 789 train_loss: 0.027559143 val_loss: 0.021845993\n",
      "Step = 790 train_loss: 0.026741313 val_loss: 0.020648675\n",
      "Step = 791 train_loss: 0.02878826 val_loss: 0.02059761\n",
      "Step = 792 train_loss: 0.02724455 val_loss: 0.021384807\n",
      "Step = 793 train_loss: 0.027241694 val_loss: 0.02442651\n",
      "Step = 794 train_loss: 0.02667157 val_loss: 0.028589964\n",
      "Step = 795 train_loss: 0.02640044 val_loss: 0.029465405\n",
      "Step = 796 train_loss: 0.027612757 val_loss: 0.029276263\n",
      "Step = 797 train_loss: 0.027741263 val_loss: 0.028298613\n",
      "Step = 798 train_loss: 0.027700063 val_loss: 0.026776724\n",
      "Step = 799 train_loss: 0.028172271 val_loss: 0.02366318\n",
      "Step = 800 train_loss: 0.026181985 val_loss: 0.020525126\n",
      "Step = 801 train_loss: 0.027804606 val_loss: 0.020011634\n",
      "Step = 802 train_loss: 0.02750035 val_loss: 0.020607715\n",
      "Step = 803 train_loss: 0.025027305 val_loss: 0.0231162\n",
      "Step = 804 train_loss: 0.02423855 val_loss: 0.02546995\n",
      "Step = 805 train_loss: 0.024488034 val_loss: 0.025171854\n",
      "Step = 806 train_loss: 0.023360057 val_loss: 0.022198653\n",
      "Step = 807 train_loss: 0.024213903 val_loss: 0.021193461\n",
      "Step = 808 train_loss: 0.02632885 val_loss: 0.024885187\n",
      "Step = 809 train_loss: 0.025080396 val_loss: 0.02643787\n",
      "Step = 810 train_loss: 0.0238405 val_loss: 0.02364842\n",
      "Step = 811 train_loss: 0.028506944 val_loss: 0.024703752\n",
      "Step = 812 train_loss: 0.027410021 val_loss: 0.0231556\n",
      "Step = 813 train_loss: 0.024511976 val_loss: 0.023875173\n",
      "Step = 814 train_loss: 0.022146132 val_loss: 0.02585722\n",
      "Step = 815 train_loss: 0.025365045 val_loss: 0.027089788\n",
      "Step = 816 train_loss: 0.024619808 val_loss: 0.026194092\n",
      "Step = 817 train_loss: 0.02466331 val_loss: 0.024446007\n",
      "Step = 818 train_loss: 0.025155712 val_loss: 0.02542542\n",
      "Step = 819 train_loss: 0.022011677 val_loss: 0.02529421\n",
      "Step = 820 train_loss: 0.023877366 val_loss: 0.023670096\n",
      "Step = 821 train_loss: 0.023814 val_loss: 0.022604415\n",
      "Step = 822 train_loss: 0.027219286 val_loss: 0.0289681\n",
      "Step = 823 train_loss: 0.02505817 val_loss: 0.028386395\n",
      "Step = 824 train_loss: 0.0268718 val_loss: 0.026743002\n",
      "Step = 825 train_loss: 0.025695521 val_loss: 0.019352768\n",
      "Step = 826 train_loss: 0.029145606 val_loss: 0.019164255\n",
      "Step = 827 train_loss: 0.022747986 val_loss: 0.022545703\n",
      "Step = 828 train_loss: 0.026185947 val_loss: 0.030582536\n",
      "Step = 829 train_loss: 0.022501804 val_loss: 0.033148758\n",
      "Step = 830 train_loss: 0.023902414 val_loss: 0.031525142\n",
      "Step = 831 train_loss: 0.027752625 val_loss: 0.025474485\n",
      "Step = 832 train_loss: 0.02211848 val_loss: 0.024137877\n",
      "Step = 833 train_loss: 0.024764933 val_loss: 0.029409856\n",
      "Step = 834 train_loss: 0.026003446 val_loss: 0.027357843\n",
      "Step = 835 train_loss: 0.025226615 val_loss: 0.021146936\n",
      "Step = 836 train_loss: 0.023130419 val_loss: 0.021898605\n",
      "Step = 837 train_loss: 0.02430896 val_loss: 0.027839174\n",
      "Step = 838 train_loss: 0.023452481 val_loss: 0.025979735\n",
      "Step = 839 train_loss: 0.02718106 val_loss: 0.021346407\n",
      "Step = 840 train_loss: 0.022999533 val_loss: 0.018708723\n",
      "Step = 841 train_loss: 0.022146188 val_loss: 0.02326014\n",
      "Step = 842 train_loss: 0.024198739 val_loss: 0.027494691\n",
      "Step = 843 train_loss: 0.026193272 val_loss: 0.024192126\n",
      "Step = 844 train_loss: 0.025128642 val_loss: 0.017672453\n",
      "Step = 845 train_loss: 0.02558139 val_loss: 0.017433021\n",
      "Step = 846 train_loss: 0.025080519 val_loss: 0.021907939\n",
      "Step = 847 train_loss: 0.024246098 val_loss: 0.029351238\n",
      "Step = 848 train_loss: 0.0278498 val_loss: 0.027958455\n",
      "Step = 849 train_loss: 0.023327695 val_loss: 0.019909147\n",
      "Step = 850 train_loss: 0.025964085 val_loss: 0.016011788\n",
      "Step = 851 train_loss: 0.028303195 val_loss: 0.015556334\n",
      "Step = 852 train_loss: 0.026430853 val_loss: 0.01674637\n",
      "Step = 853 train_loss: 0.022879822 val_loss: 0.026822759\n",
      "Step = 854 train_loss: 0.027745474 val_loss: 0.02550263\n",
      "Step = 855 train_loss: 0.024800725 val_loss: 0.01982937\n",
      "Step = 856 train_loss: 0.023838669 val_loss: 0.015500587\n",
      "Step = 857 train_loss: 0.02287062 val_loss: 0.016262362\n",
      "Step = 858 train_loss: 0.022667617 val_loss: 0.015854148\n",
      "Step = 859 train_loss: 0.022997452 val_loss: 0.016007163\n",
      "Step = 860 train_loss: 0.025454806 val_loss: 0.018490877\n",
      "Step = 861 train_loss: 0.019664152 val_loss: 0.019103484\n",
      "Step = 862 train_loss: 0.024472237 val_loss: 0.01690642\n",
      "Step = 863 train_loss: 0.02191301 val_loss: 0.016899997\n",
      "Step = 864 train_loss: 0.02158561 val_loss: 0.019318065\n",
      "Step = 865 train_loss: 0.021245021 val_loss: 0.020059522\n",
      "Step = 866 train_loss: 0.022901902 val_loss: 0.022837741\n",
      "Step = 867 train_loss: 0.020678937 val_loss: 0.017967165\n",
      "Step = 868 train_loss: 0.020943843 val_loss: 0.01693972\n",
      "Step = 869 train_loss: 0.019962918 val_loss: 0.02021331\n",
      "Step = 870 train_loss: 0.022378547 val_loss: 0.025500266\n",
      "Step = 871 train_loss: 0.02177614 val_loss: 0.025737997\n",
      "Step = 872 train_loss: 0.021469405 val_loss: 0.021223614\n",
      "Step = 873 train_loss: 0.022276819 val_loss: 0.016259646\n",
      "Step = 874 train_loss: 0.024634907 val_loss: 0.019105682\n",
      "Step = 875 train_loss: 0.020090727 val_loss: 0.022495164\n",
      "Step = 876 train_loss: 0.021726271 val_loss: 0.022017207\n",
      "Step = 877 train_loss: 0.020749114 val_loss: 0.024321852\n",
      "Step = 878 train_loss: 0.02345293 val_loss: 0.026919484\n",
      "Step = 879 train_loss: 0.02218146 val_loss: 0.02781148\n",
      "Step = 880 train_loss: 0.02158748 val_loss: 0.023149652\n",
      "Step = 881 train_loss: 0.020746242 val_loss: 0.017210422\n",
      "Step = 882 train_loss: 0.02513381 val_loss: 0.017581776\n",
      "Step = 883 train_loss: 0.02425736 val_loss: 0.025873495\n",
      "Step = 884 train_loss: 0.020194236 val_loss: 0.031550117\n",
      "Step = 885 train_loss: 0.022451151 val_loss: 0.031596247\n",
      "Step = 886 train_loss: 0.022322308 val_loss: 0.027121285\n",
      "Step = 887 train_loss: 0.020411257 val_loss: 0.026709646\n",
      "Step = 888 train_loss: 0.023918044 val_loss: 0.030810071\n",
      "Step = 889 train_loss: 0.020077776 val_loss: 0.034948908\n",
      "Step = 890 train_loss: 0.020541642 val_loss: 0.035489645\n",
      "Step = 891 train_loss: 0.020373639 val_loss: 0.028691951\n",
      "Step = 892 train_loss: 0.022133786 val_loss: 0.02879543\n",
      "Step = 893 train_loss: 0.021407563 val_loss: 0.03555554\n",
      "Step = 894 train_loss: 0.024829276 val_loss: 0.035305284\n",
      "Step = 895 train_loss: 0.021228394 val_loss: 0.0290948\n",
      "Step = 896 train_loss: 0.01873284 val_loss: 0.025049945\n",
      "Step = 897 train_loss: 0.024240676 val_loss: 0.029864974\n",
      "Step = 898 train_loss: 0.020299025 val_loss: 0.03409233\n",
      "Step = 899 train_loss: 0.020820567 val_loss: 0.030332213\n",
      "Step = 900 train_loss: 0.021098305 val_loss: 0.020238033\n",
      "Step = 901 train_loss: 0.020846404 val_loss: 0.018581685\n",
      "Step = 902 train_loss: 0.019974988 val_loss: 0.019713001\n",
      "Step = 903 train_loss: 0.018506823 val_loss: 0.01985172\n",
      "Step = 904 train_loss: 0.019511402 val_loss: 0.016991278\n",
      "Step = 905 train_loss: 0.019314323 val_loss: 0.015202079\n",
      "Step = 906 train_loss: 0.019500125 val_loss: 0.014993188\n",
      "Step = 907 train_loss: 0.017781934 val_loss: 0.014920774\n",
      "Step = 908 train_loss: 0.018971661 val_loss: 0.01571347\n",
      "Step = 909 train_loss: 0.019538637 val_loss: 0.014213556\n",
      "Step = 910 train_loss: 0.019726675 val_loss: 0.014871476\n",
      "Step = 911 train_loss: 0.019780505 val_loss: 0.018891389\n",
      "Step = 912 train_loss: 0.02264354 val_loss: 0.016154181\n",
      "Step = 913 train_loss: 0.019710077 val_loss: 0.017288199\n",
      "Step = 914 train_loss: 0.021565814 val_loss: 0.016990758\n",
      "Step = 915 train_loss: 0.019190867 val_loss: 0.02124402\n",
      "Step = 916 train_loss: 0.017614998 val_loss: 0.02327516\n",
      "Step = 917 train_loss: 0.019747287 val_loss: 0.016569763\n",
      "Step = 918 train_loss: 0.020868141 val_loss: 0.015678965\n",
      "Step = 919 train_loss: 0.019007878 val_loss: 0.025337225\n",
      "Step = 920 train_loss: 0.017538523 val_loss: 0.025282947\n",
      "Step = 921 train_loss: 0.021458996 val_loss: 0.021028126\n",
      "Step = 922 train_loss: 0.019752957 val_loss: 0.016947215\n",
      "Step = 923 train_loss: 0.020079058 val_loss: 0.022323452\n",
      "Step = 924 train_loss: 0.020330938 val_loss: 0.027819715\n",
      "Step = 925 train_loss: 0.021062214 val_loss: 0.025161847\n",
      "Step = 926 train_loss: 0.017796068 val_loss: 0.020097425\n",
      "Step = 927 train_loss: 0.015782276 val_loss: 0.021879394\n",
      "Step = 928 train_loss: 0.019050322 val_loss: 0.03155159\n",
      "Step = 929 train_loss: 0.019034706 val_loss: 0.030237125\n",
      "Step = 930 train_loss: 0.021079453 val_loss: 0.02150264\n",
      "Step = 931 train_loss: 0.016615957 val_loss: 0.018819263\n",
      "Step = 932 train_loss: 0.018721541 val_loss: 0.02750487\n",
      "Step = 933 train_loss: 0.020071216 val_loss: 0.035142597\n",
      "Step = 934 train_loss: 0.020085951 val_loss: 0.029241983\n",
      "Step = 935 train_loss: 0.019036582 val_loss: 0.020657565\n",
      "Step = 936 train_loss: 0.018402843 val_loss: 0.019458033\n",
      "Step = 937 train_loss: 0.020972319 val_loss: 0.02983197\n",
      "Step = 938 train_loss: 0.017874988 val_loss: 0.031966865\n",
      "Step = 939 train_loss: 0.019216247 val_loss: 0.019558657\n",
      "Step = 940 train_loss: 0.01716585 val_loss: 0.014234502\n",
      "Step = 941 train_loss: 0.021086838 val_loss: 0.02148497\n",
      "Step = 942 train_loss: 0.016577108 val_loss: 0.03425335\n",
      "Step = 943 train_loss: 0.024713306 val_loss: 0.020530833\n",
      "Step = 944 train_loss: 0.018099884 val_loss: 0.014283847\n",
      "Step = 945 train_loss: 0.02012221 val_loss: 0.020486588\n",
      "Step = 946 train_loss: 0.01767125 val_loss: 0.028848883\n",
      "Step = 947 train_loss: 0.021001901 val_loss: 0.02282643\n",
      "Step = 948 train_loss: 0.017023398 val_loss: 0.023271544\n",
      "Step = 949 train_loss: 0.020403769 val_loss: 0.025763966\n",
      "Step = 950 train_loss: 0.01718225 val_loss: 0.031534947\n",
      "Step = 951 train_loss: 0.018607115 val_loss: 0.030286105\n",
      "Step = 952 train_loss: 0.01977139 val_loss: 0.020775696\n",
      "Step = 953 train_loss: 0.015728317 val_loss: 0.016416932\n",
      "Step = 954 train_loss: 0.021102797 val_loss: 0.02478635\n",
      "Step = 955 train_loss: 0.019301694 val_loss: 0.035533383\n",
      "Step = 956 train_loss: 0.017639074 val_loss: 0.031312484\n",
      "Step = 957 train_loss: 0.018019358 val_loss: 0.017349739\n",
      "Step = 958 train_loss: 0.016857808 val_loss: 0.013999648\n",
      "Step = 959 train_loss: 0.018587412 val_loss: 0.021106392\n",
      "Step = 960 train_loss: 0.01787762 val_loss: 0.034331284\n",
      "Step = 961 train_loss: 0.017695416 val_loss: 0.035119846\n",
      "Step = 962 train_loss: 0.02051882 val_loss: 0.019951168\n",
      "Step = 963 train_loss: 0.017935766 val_loss: 0.014038101\n",
      "Step = 964 train_loss: 0.018036157 val_loss: 0.020882117\n",
      "Step = 965 train_loss: 0.01764455 val_loss: 0.03136575\n",
      "Step = 966 train_loss: 0.017093925 val_loss: 0.030770041\n",
      "Step = 967 train_loss: 0.01869071 val_loss: 0.0210934\n",
      "Step = 968 train_loss: 0.018667514 val_loss: 0.017804783\n",
      "Step = 969 train_loss: 0.017799087 val_loss: 0.023200057\n",
      "Step = 970 train_loss: 0.022946924 val_loss: 0.029720824\n",
      "Step = 971 train_loss: 0.019014679 val_loss: 0.024325795\n",
      "Step = 972 train_loss: 0.019050816 val_loss: 0.024222765\n",
      "Step = 973 train_loss: 0.015933506 val_loss: 0.028732661\n",
      "Step = 974 train_loss: 0.016083486 val_loss: 0.031809412\n",
      "Step = 975 train_loss: 0.017860053 val_loss: 0.024224719\n",
      "Step = 976 train_loss: 0.017859261 val_loss: 0.022000972\n",
      "Step = 977 train_loss: 0.01987103 val_loss: 0.023972414\n",
      "Step = 978 train_loss: 0.016694807 val_loss: 0.029149454\n",
      "Step = 979 train_loss: 0.016095433 val_loss: 0.02837419\n",
      "Step = 980 train_loss: 0.016731374 val_loss: 0.026160147\n",
      "Step = 981 train_loss: 0.015461669 val_loss: 0.024611712\n",
      "Step = 982 train_loss: 0.018543655 val_loss: 0.026795033\n",
      "Step = 983 train_loss: 0.016088294 val_loss: 0.026458615\n",
      "Step = 984 train_loss: 0.017295264 val_loss: 0.023418918\n",
      "Step = 985 train_loss: 0.018610833 val_loss: 0.02215344\n",
      "Step = 986 train_loss: 0.01776908 val_loss: 0.030981256\n",
      "Step = 987 train_loss: 0.018165857 val_loss: 0.03703469\n",
      "Step = 988 train_loss: 0.019068532 val_loss: 0.036962703\n",
      "Step = 989 train_loss: 0.014726094 val_loss: 0.033542134\n",
      "Step = 990 train_loss: 0.017171135 val_loss: 0.018675681\n",
      "Step = 991 train_loss: 0.01610219 val_loss: 0.020331651\n",
      "Step = 992 train_loss: 0.01631738 val_loss: 0.032257363\n",
      "Step = 993 train_loss: 0.018967431 val_loss: 0.031144032\n",
      "Step = 994 train_loss: 0.018613573 val_loss: 0.028073555\n",
      "Step = 995 train_loss: 0.016596712 val_loss: 0.01804559\n",
      "Step = 996 train_loss: 0.019631125 val_loss: 0.018812303\n",
      "Step = 997 train_loss: 0.018412627 val_loss: 0.025131142\n",
      "Step = 998 train_loss: 0.015917663 val_loss: 0.035295177\n",
      "Step = 999 train_loss: 0.022730548 val_loss: 0.02086496\n",
      "958\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "# summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "val_losses=[]\n",
    "\n",
    "# 用于保存模型的最大数量\n",
    "max_models_to_keep = 10\n",
    "saved_models = []\n",
    "\n",
    "for t in range(1000):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 保存模型\n",
    "    model_path = 'Target_model/net_parameters'+str(t)+'.pkl'\n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, model_path)\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print('Step = %d' % t, 'train_loss:', train_loss.data.numpy(), 'val_loss:', val_loss.data.numpy())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # 添加当前模型及其验证损失\n",
    "    saved_models.append((val_loss.item(), model_path))\n",
    "\n",
    "    # 保持模型数量不超过max_models_to_keep\n",
    "    if len(saved_models) > max_models_to_keep:\n",
    "        # 找到验证损失最大的一组模型并删除\n",
    "        saved_models.sort(key=lambda x: x[0])  # 排序，根据损失\n",
    "        os.remove(saved_models.pop()[1])  # 删除损失最大的模型\n",
    "\n",
    "# 'saved_models' 中现在只包含验证损失最小的前十个模型\n",
    "'''选择损失最小的模型'''\n",
    "best_index = val_losses.index(np.min(val_losses))\n",
    "print(best_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_34052\\399239800.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_34052\\399239800.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''复制验证损失最小的模型到最佳模型文件夹'''\n",
    "if 1:\n",
    "    shutil.copyfile('Target_model/net_parameters'+str(best_index)+'.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "\n",
    "'''重新加载最佳模型''' \n",
    "model = Net()\n",
    "a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
    "optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.35011673\n",
      "mpe_val: 0.32820684\n",
      "mpe_a: 0.3992127248083419\n",
      "mpe_b: 0.17420322061699195\n",
      "rmse_train: 301.24637\n",
      "rmse_val: 264.367\n",
      "rmse_a: 319.64962649896876\n",
      "rmse_b: 259.32455533558715\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.37430518865585327\n",
      "r2_val: 0.328583300113678\n",
      "r2_a: 0.3304816952119657\n",
      "r2_b: 0.1727084187753034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
