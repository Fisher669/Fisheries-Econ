{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型效果\n",
    "## 改用deep-wide模型架构\n",
    "r2_train: 0.6286368370056152\n",
    "r2_val: 0.6448044180870056\n",
    "r2_a: 0.6082580908824828\n",
    "r2_b: 0.09643709934666189\n",
    "## 对比（非lstm架构）\n",
    "r2_train: 0.9947811961174011\n",
    "r2_val: 0.9749422669410706\n",
    "r2_a: 0.9464100454957816\n",
    "r2_b: 0.6836000724611211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 修改模型需要改层数，回传部分，以及lstm部分\n",
    "'''Define the CNN model''' \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Deep部分 - 卷积层\n",
    "        self.deep_cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(8, 16, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),  # 增加池化层\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(32, 64, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "        )\n",
    "        \n",
    "        # Wide部分 - LSTM层\n",
    "        self.wide_lstm = torch.nn.LSTM(input_size=32, hidden_size=32, batch_first=True)\n",
    "        \n",
    "        # 融合后的全连接层\n",
    "        self.fc1 = torch.nn.Linear(4832, 100)  # Deep 和 Wide 部分的输出拼接，注意调整输入维度\n",
    "        self.drop_layer = torch.nn.Dropout(p=0.2)\n",
    "        self.fc2 = torch.nn.Linear(100, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            # Deep部分 (CNN)\n",
    "            deep_out = self.deep_cnn(x)\n",
    "            deep_out = deep_out.view(deep_out.size(0), -1)  # 展平为 (batch_size, flatten_size)\n",
    "            \n",
    "            # Wide部分 (LSTM)\n",
    "            wide_input = x.view(x.size(0), -1, 32)  # 调整为 (batch_size, sequence_length, input_size)\n",
    "            wide_out, _ = self.wide_lstm(wide_input)\n",
    "            wide_out = wide_out[:, -1, :]  # 获取 LSTM 的最后一个时间步输出\n",
    "            \n",
    "            # 拼接 Deep 和 Wide 部分的输出\n",
    "            combined = torch.cat((deep_out, wide_out), dim=1)  # 在最后一个维度拼接\n",
    "            \n",
    "            # 全连接层\n",
    "            x = torch.relu(self.fc1(combined))\n",
    "            x = self.drop_layer(x)\n",
    "            x = self.fc2(x)\n",
    "            \n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during forward pass: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (deep_cnn): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (13): Dropout(p=0.2, inplace=False)\n",
      "    (14): Conv2d(32, 64, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (17): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (18): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (wide_lstm): LSTM(32, 32, batch_first=True)\n",
      "  (fc1): Linear(in_features=4832, out_features=100, bias=True)\n",
      "  (drop_layer): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "Step = 0 train_loss: 0.13412279 val_loss: 0.06612679\n",
      "Step = 1 train_loss: 0.08418961 val_loss: 0.06725972\n",
      "Step = 2 train_loss: 0.073112175 val_loss: 0.06646371\n",
      "Step = 3 train_loss: 0.06836365 val_loss: 0.065431714\n",
      "Step = 4 train_loss: 0.09347085 val_loss: 0.06488052\n",
      "Step = 5 train_loss: 0.08049024 val_loss: 0.06452315\n",
      "Step = 6 train_loss: 0.05454659 val_loss: 0.06402645\n",
      "Step = 7 train_loss: 0.088095255 val_loss: 0.063167915\n",
      "Step = 8 train_loss: 0.055560324 val_loss: 0.06197689\n",
      "Step = 9 train_loss: 0.04198817 val_loss: 0.060163967\n",
      "Step = 10 train_loss: 0.083872646 val_loss: 0.057629567\n",
      "Step = 11 train_loss: 0.06445249 val_loss: 0.055171773\n",
      "Step = 12 train_loss: 0.065168075 val_loss: 0.052811455\n",
      "Step = 13 train_loss: 0.07879912 val_loss: 0.050467763\n",
      "Step = 14 train_loss: 0.07715642 val_loss: 0.048234552\n",
      "Step = 15 train_loss: 0.070703685 val_loss: 0.046719108\n",
      "Step = 16 train_loss: 0.061373845 val_loss: 0.045246944\n",
      "Step = 17 train_loss: 0.07682522 val_loss: 0.044121616\n",
      "Step = 18 train_loss: 0.0947908 val_loss: 0.0434657\n",
      "Step = 19 train_loss: 0.04309865 val_loss: 0.042912807\n",
      "Step = 20 train_loss: 0.10028489 val_loss: 0.04304301\n",
      "Step = 21 train_loss: 0.05861783 val_loss: 0.043280635\n",
      "Step = 22 train_loss: 0.10925479 val_loss: 0.04400401\n",
      "Step = 23 train_loss: 0.05154664 val_loss: 0.04406823\n",
      "Step = 24 train_loss: 0.068023026 val_loss: 0.04342486\n",
      "Step = 25 train_loss: 0.058456298 val_loss: 0.04267608\n",
      "Step = 26 train_loss: 0.032921817 val_loss: 0.041980494\n",
      "Step = 27 train_loss: 0.05174603 val_loss: 0.041316807\n",
      "Step = 28 train_loss: 0.05400991 val_loss: 0.04043536\n",
      "Step = 29 train_loss: 0.039869126 val_loss: 0.038590405\n",
      "Step = 30 train_loss: 0.079116955 val_loss: 0.036882408\n",
      "Step = 31 train_loss: 0.08452872 val_loss: 0.035217337\n",
      "Step = 32 train_loss: 0.089106485 val_loss: 0.034019344\n",
      "Step = 33 train_loss: 0.056578774 val_loss: 0.032966293\n",
      "Step = 34 train_loss: 0.087090656 val_loss: 0.03138799\n",
      "Step = 35 train_loss: 0.08778463 val_loss: 0.029991155\n",
      "Step = 36 train_loss: 0.065647244 val_loss: 0.02867958\n",
      "Step = 37 train_loss: 0.06923549 val_loss: 0.027445193\n",
      "Step = 38 train_loss: 0.097824335 val_loss: 0.026568271\n",
      "Step = 39 train_loss: 0.047770627 val_loss: 0.025870068\n",
      "Step = 40 train_loss: 0.06416783 val_loss: 0.025178706\n",
      "Step = 41 train_loss: 0.067814276 val_loss: 0.024564283\n",
      "Step = 42 train_loss: 0.06604569 val_loss: 0.02381421\n",
      "Step = 43 train_loss: 0.025708212 val_loss: 0.023218278\n",
      "Step = 44 train_loss: 0.05488222 val_loss: 0.022684485\n",
      "Step = 45 train_loss: 0.0810239 val_loss: 0.022368142\n",
      "Step = 46 train_loss: 0.03525628 val_loss: 0.022051418\n",
      "Step = 47 train_loss: 0.07487007 val_loss: 0.022353614\n",
      "Step = 48 train_loss: 0.03526625 val_loss: 0.02268291\n",
      "Step = 49 train_loss: 0.04310979 val_loss: 0.02269431\n",
      "Step = 50 train_loss: 0.053258773 val_loss: 0.022633556\n",
      "Step = 51 train_loss: 0.078993544 val_loss: 0.022686776\n",
      "Step = 52 train_loss: 0.05955129 val_loss: 0.022842757\n",
      "Step = 53 train_loss: 0.06424207 val_loss: 0.023042733\n",
      "Step = 54 train_loss: 0.052077785 val_loss: 0.023616636\n",
      "Step = 55 train_loss: 0.04878732 val_loss: 0.02418674\n",
      "Step = 56 train_loss: 0.060973994 val_loss: 0.02463016\n",
      "Step = 57 train_loss: 0.08075837 val_loss: 0.024975171\n",
      "Step = 58 train_loss: 0.05851415 val_loss: 0.024989977\n",
      "Step = 59 train_loss: 0.04509705 val_loss: 0.024714932\n",
      "Step = 60 train_loss: 0.066743106 val_loss: 0.024703918\n",
      "Step = 61 train_loss: 0.053587474 val_loss: 0.024484327\n",
      "Step = 62 train_loss: 0.023088187 val_loss: 0.024259226\n",
      "Step = 63 train_loss: 0.06670383 val_loss: 0.024163518\n",
      "Step = 64 train_loss: 0.0604074 val_loss: 0.024372535\n",
      "Step = 65 train_loss: 0.042486332 val_loss: 0.02448521\n",
      "Step = 66 train_loss: 0.03703151 val_loss: 0.024440387\n",
      "Step = 67 train_loss: 0.045142695 val_loss: 0.024514396\n",
      "Step = 68 train_loss: 0.0602903 val_loss: 0.024553236\n",
      "Step = 69 train_loss: 0.064114474 val_loss: 0.024607737\n",
      "Step = 70 train_loss: 0.061172068 val_loss: 0.024481634\n",
      "Step = 71 train_loss: 0.04453551 val_loss: 0.024344586\n",
      "Step = 72 train_loss: 0.043255966 val_loss: 0.0242763\n",
      "Step = 73 train_loss: 0.04416392 val_loss: 0.024497084\n",
      "Step = 74 train_loss: 0.051320117 val_loss: 0.024683787\n",
      "Step = 75 train_loss: 0.055928197 val_loss: 0.024754731\n",
      "Step = 76 train_loss: 0.059408423 val_loss: 0.024938209\n",
      "Step = 77 train_loss: 0.047844265 val_loss: 0.025073005\n",
      "Step = 78 train_loss: 0.043069147 val_loss: 0.0251424\n",
      "Step = 79 train_loss: 0.046159767 val_loss: 0.025129408\n",
      "Step = 80 train_loss: 0.04130262 val_loss: 0.025149584\n",
      "Step = 81 train_loss: 0.05584958 val_loss: 0.025154416\n",
      "Step = 82 train_loss: 0.047210734 val_loss: 0.025348715\n",
      "Step = 83 train_loss: 0.038681857 val_loss: 0.025646133\n",
      "Step = 84 train_loss: 0.033654597 val_loss: 0.026387602\n",
      "Step = 85 train_loss: 0.03809836 val_loss: 0.029217385\n",
      "Step = 86 train_loss: 0.05017634 val_loss: 0.05533369\n",
      "Step = 87 train_loss: 0.03812272 val_loss: 0.060051035\n",
      "Step = 88 train_loss: 0.02261862 val_loss: 0.049807303\n",
      "Step = 89 train_loss: 0.053890232 val_loss: 0.051631074\n",
      "Step = 90 train_loss: 0.027090004 val_loss: 0.06136292\n",
      "Step = 91 train_loss: 0.057492394 val_loss: 0.056857046\n",
      "Step = 92 train_loss: 0.06141756 val_loss: 0.054921262\n",
      "Step = 93 train_loss: 0.056634434 val_loss: 0.057567857\n",
      "Step = 94 train_loss: 0.040406093 val_loss: 0.06333756\n",
      "Step = 95 train_loss: 0.03698254 val_loss: 0.057837352\n",
      "Step = 96 train_loss: 0.041668076 val_loss: 0.044276755\n",
      "Step = 97 train_loss: 0.042685624 val_loss: 0.025734838\n",
      "Step = 98 train_loss: 0.03432106 val_loss: 0.0215434\n",
      "Step = 99 train_loss: 0.033902857 val_loss: 0.021784306\n",
      "Step = 100 train_loss: 0.038244717 val_loss: 0.021889117\n",
      "Step = 101 train_loss: 0.058965478 val_loss: 0.021907443\n",
      "Step = 102 train_loss: 0.04080958 val_loss: 0.021732127\n",
      "Step = 103 train_loss: 0.0437415 val_loss: 0.021593884\n",
      "Step = 104 train_loss: 0.034776308 val_loss: 0.021797884\n",
      "Step = 105 train_loss: 0.0471493 val_loss: 0.022223514\n",
      "Step = 106 train_loss: 0.059699174 val_loss: 0.022783719\n",
      "Step = 107 train_loss: 0.04613076 val_loss: 0.023263576\n",
      "Step = 108 train_loss: 0.059834965 val_loss: 0.023692608\n",
      "Step = 109 train_loss: 0.031932585 val_loss: 0.024231115\n",
      "Step = 110 train_loss: 0.03591385 val_loss: 0.02480663\n",
      "Step = 111 train_loss: 0.033032957 val_loss: 0.025395\n",
      "Step = 112 train_loss: 0.03939525 val_loss: 0.02601306\n",
      "Step = 113 train_loss: 0.056681193 val_loss: 0.0265498\n",
      "Step = 114 train_loss: 0.0683024 val_loss: 0.027277997\n",
      "Step = 115 train_loss: 0.05949119 val_loss: 0.027982308\n",
      "Step = 116 train_loss: 0.03630098 val_loss: 0.028690085\n",
      "Step = 117 train_loss: 0.04171483 val_loss: 0.029144198\n",
      "Step = 118 train_loss: 0.04448319 val_loss: 0.029523829\n",
      "Step = 119 train_loss: 0.03705344 val_loss: 0.029757656\n",
      "Step = 120 train_loss: 0.029777471 val_loss: 0.030025538\n",
      "Step = 121 train_loss: 0.03896269 val_loss: 0.030444922\n",
      "Step = 122 train_loss: 0.04716985 val_loss: 0.030612376\n",
      "Step = 123 train_loss: 0.032143854 val_loss: 0.030921262\n",
      "Step = 124 train_loss: 0.041522603 val_loss: 0.03079877\n",
      "Step = 125 train_loss: 0.041628756 val_loss: 0.030589709\n",
      "Step = 126 train_loss: 0.038834635 val_loss: 0.030483775\n",
      "Step = 127 train_loss: 0.04393849 val_loss: 0.030406702\n",
      "Step = 128 train_loss: 0.0326741 val_loss: 0.030371256\n",
      "Step = 129 train_loss: 0.04779575 val_loss: 0.030241236\n",
      "Step = 130 train_loss: 0.037370183 val_loss: 0.029843124\n",
      "Step = 131 train_loss: 0.038294334 val_loss: 0.02963271\n",
      "Step = 132 train_loss: 0.035564885 val_loss: 0.029569652\n",
      "Step = 133 train_loss: 0.04848277 val_loss: 0.029530587\n",
      "Step = 134 train_loss: 0.04401246 val_loss: 0.02971414\n",
      "Step = 135 train_loss: 0.04373316 val_loss: 0.029961482\n",
      "Step = 136 train_loss: 0.044473276 val_loss: 0.030117972\n",
      "Step = 137 train_loss: 0.014127066 val_loss: 0.030394273\n",
      "Step = 138 train_loss: 0.044860527 val_loss: 0.03077521\n",
      "Step = 139 train_loss: 0.047315735 val_loss: 0.031164201\n",
      "Step = 140 train_loss: 0.04871539 val_loss: 0.031561047\n",
      "Step = 141 train_loss: 0.033285387 val_loss: 0.03197725\n",
      "Step = 142 train_loss: 0.058929585 val_loss: 0.03237053\n",
      "Step = 143 train_loss: 0.034196522 val_loss: 0.03266329\n",
      "Step = 144 train_loss: 0.034121305 val_loss: 0.032981627\n",
      "Step = 145 train_loss: 0.035177287 val_loss: 0.033311784\n",
      "Step = 146 train_loss: 0.035105344 val_loss: 0.03374658\n",
      "Step = 147 train_loss: 0.0315114 val_loss: 0.034031805\n",
      "Step = 148 train_loss: 0.02674923 val_loss: 0.034380957\n",
      "Step = 149 train_loss: 0.038853243 val_loss: 0.034894343\n",
      "Step = 150 train_loss: 0.039033644 val_loss: 0.03550657\n",
      "Step = 151 train_loss: 0.03886989 val_loss: 0.036197446\n",
      "Step = 152 train_loss: 0.047114864 val_loss: 0.037212368\n",
      "Step = 153 train_loss: 0.049735565 val_loss: 0.0388677\n",
      "Step = 154 train_loss: 0.037918355 val_loss: 0.04076156\n",
      "Step = 155 train_loss: 0.030852368 val_loss: 0.042232975\n",
      "Step = 156 train_loss: 0.037447788 val_loss: 0.0439851\n",
      "Step = 157 train_loss: 0.026675222 val_loss: 0.045881808\n",
      "Step = 158 train_loss: 0.03960897 val_loss: 0.05043809\n",
      "Step = 159 train_loss: 0.032887332 val_loss: 0.05704182\n",
      "Step = 160 train_loss: 0.03418673 val_loss: 0.06336583\n",
      "Step = 161 train_loss: 0.04151506 val_loss: 0.06463504\n",
      "Step = 162 train_loss: 0.031405475 val_loss: 0.06067661\n",
      "Step = 163 train_loss: 0.036169086 val_loss: 0.049091943\n",
      "Step = 164 train_loss: 0.01893383 val_loss: 0.046739638\n",
      "Step = 165 train_loss: 0.028673502 val_loss: 0.045869667\n",
      "Step = 166 train_loss: 0.024130585 val_loss: 0.04526083\n",
      "Step = 167 train_loss: 0.025861312 val_loss: 0.04581284\n",
      "Step = 168 train_loss: 0.039953314 val_loss: 0.053048633\n",
      "Step = 169 train_loss: 0.037297577 val_loss: 0.05130795\n",
      "Step = 170 train_loss: 0.024156015 val_loss: 0.0455985\n",
      "Step = 171 train_loss: 0.028652702 val_loss: 0.04565436\n",
      "Step = 172 train_loss: 0.024330553 val_loss: 0.04544802\n",
      "Step = 173 train_loss: 0.044865415 val_loss: 0.047083918\n",
      "Step = 174 train_loss: 0.036771797 val_loss: 0.04442337\n",
      "Step = 175 train_loss: 0.041390933 val_loss: 0.023067882\n",
      "Step = 176 train_loss: 0.04639214 val_loss: 0.023250272\n",
      "Step = 177 train_loss: 0.041602697 val_loss: 0.023548514\n",
      "Step = 178 train_loss: 0.035331946 val_loss: 0.023863614\n",
      "Step = 179 train_loss: 0.04287935 val_loss: 0.024204105\n",
      "Step = 180 train_loss: 0.043648496 val_loss: 0.024460081\n",
      "Step = 181 train_loss: 0.028354462 val_loss: 0.024833048\n",
      "Step = 182 train_loss: 0.036995705 val_loss: 0.025327642\n",
      "Step = 183 train_loss: 0.040168226 val_loss: 0.025943253\n",
      "Step = 184 train_loss: 0.034886613 val_loss: 0.026584683\n",
      "Step = 185 train_loss: 0.05180729 val_loss: 0.027297415\n",
      "Step = 186 train_loss: 0.03385004 val_loss: 0.027968599\n",
      "Step = 187 train_loss: 0.032313015 val_loss: 0.028677013\n",
      "Step = 188 train_loss: 0.03578121 val_loss: 0.029472264\n",
      "Step = 189 train_loss: 0.025406448 val_loss: 0.030460466\n",
      "Step = 190 train_loss: 0.027658923 val_loss: 0.031310376\n",
      "Step = 191 train_loss: 0.03808876 val_loss: 0.032042406\n",
      "Step = 192 train_loss: 0.039496824 val_loss: 0.032681257\n",
      "Step = 193 train_loss: 0.032285165 val_loss: 0.0331346\n",
      "Step = 194 train_loss: 0.036021505 val_loss: 0.0335735\n",
      "Step = 195 train_loss: 0.048306145 val_loss: 0.03380661\n",
      "Step = 196 train_loss: 0.025639612 val_loss: 0.034180336\n",
      "Step = 197 train_loss: 0.028059723 val_loss: 0.034436416\n",
      "Step = 198 train_loss: 0.042500734 val_loss: 0.034598365\n",
      "Step = 199 train_loss: 0.043928165 val_loss: 0.03481708\n",
      "Step = 200 train_loss: 0.036101267 val_loss: 0.034972668\n",
      "Step = 201 train_loss: 0.026821056 val_loss: 0.035299905\n",
      "Step = 202 train_loss: 0.035213523 val_loss: 0.03542457\n",
      "Step = 203 train_loss: 0.039548792 val_loss: 0.035792846\n",
      "Step = 204 train_loss: 0.036448844 val_loss: 0.03590206\n",
      "Step = 205 train_loss: 0.03776153 val_loss: 0.036078174\n",
      "Step = 206 train_loss: 0.033605855 val_loss: 0.036265437\n",
      "Step = 207 train_loss: 0.032834686 val_loss: 0.036392294\n",
      "Step = 208 train_loss: 0.042797107 val_loss: 0.036326572\n",
      "Step = 209 train_loss: 0.028725926 val_loss: 0.03641271\n",
      "Step = 210 train_loss: 0.032890894 val_loss: 0.03658835\n",
      "Step = 211 train_loss: 0.03334924 val_loss: 0.03663619\n",
      "Step = 212 train_loss: 0.043040674 val_loss: 0.03666161\n",
      "Step = 213 train_loss: 0.027482625 val_loss: 0.03667105\n",
      "Step = 214 train_loss: 0.034417123 val_loss: 0.036709335\n",
      "Step = 215 train_loss: 0.029526459 val_loss: 0.03682354\n",
      "Step = 216 train_loss: 0.029670307 val_loss: 0.036970653\n",
      "Step = 217 train_loss: 0.044541065 val_loss: 0.037099354\n",
      "Step = 218 train_loss: 0.028120358 val_loss: 0.037164204\n",
      "Step = 219 train_loss: 0.027757166 val_loss: 0.03710974\n",
      "Step = 220 train_loss: 0.038186967 val_loss: 0.037241716\n",
      "Step = 221 train_loss: 0.02971067 val_loss: 0.037365\n",
      "Step = 222 train_loss: 0.03158542 val_loss: 0.03714385\n",
      "Step = 223 train_loss: 0.03893737 val_loss: 0.03735331\n",
      "Step = 224 train_loss: 0.03360708 val_loss: 0.037478454\n",
      "Step = 225 train_loss: 0.038785245 val_loss: 0.03784211\n",
      "Step = 226 train_loss: 0.034485675 val_loss: 0.03797354\n",
      "Step = 227 train_loss: 0.038947165 val_loss: 0.038100064\n",
      "Step = 228 train_loss: 0.033303138 val_loss: 0.03827642\n",
      "Step = 229 train_loss: 0.03625891 val_loss: 0.03848933\n",
      "Step = 230 train_loss: 0.032750007 val_loss: 0.038837463\n",
      "Step = 231 train_loss: 0.022624103 val_loss: 0.039118387\n",
      "Step = 232 train_loss: 0.041720424 val_loss: 0.039376438\n",
      "Step = 233 train_loss: 0.02863589 val_loss: 0.03965887\n",
      "Step = 234 train_loss: 0.029731162 val_loss: 0.039972525\n",
      "Step = 235 train_loss: 0.03222227 val_loss: 0.04035909\n",
      "Step = 236 train_loss: 0.03278986 val_loss: 0.04046242\n",
      "Step = 237 train_loss: 0.03397063 val_loss: 0.04053062\n",
      "Step = 238 train_loss: 0.030127205 val_loss: 0.040725995\n",
      "Step = 239 train_loss: 0.025969964 val_loss: 0.04097088\n",
      "Step = 240 train_loss: 0.028988121 val_loss: 0.041056123\n",
      "Step = 241 train_loss: 0.030496636 val_loss: 0.041178387\n",
      "Step = 242 train_loss: 0.044377014 val_loss: 0.041530296\n",
      "Step = 243 train_loss: 0.028479937 val_loss: 0.042055864\n",
      "Step = 244 train_loss: 0.03951095 val_loss: 0.042203438\n",
      "Step = 245 train_loss: 0.03960216 val_loss: 0.042427942\n",
      "Step = 246 train_loss: 0.035322264 val_loss: 0.042541258\n",
      "Step = 247 train_loss: 0.029616611 val_loss: 0.042744115\n",
      "Step = 248 train_loss: 0.032362215 val_loss: 0.043044545\n",
      "Step = 249 train_loss: 0.034283645 val_loss: 0.043413915\n",
      "Step = 250 train_loss: 0.03175731 val_loss: 0.043860376\n",
      "Step = 251 train_loss: 0.026288422 val_loss: 0.044234924\n",
      "Step = 252 train_loss: 0.038589083 val_loss: 0.044283133\n",
      "Step = 253 train_loss: 0.043386705 val_loss: 0.04420673\n",
      "Step = 254 train_loss: 0.034481235 val_loss: 0.044107344\n",
      "Step = 255 train_loss: 0.026171971 val_loss: 0.044009782\n",
      "Step = 256 train_loss: 0.03866226 val_loss: 0.044035714\n",
      "Step = 257 train_loss: 0.04188446 val_loss: 0.044145435\n",
      "Step = 258 train_loss: 0.032052033 val_loss: 0.044134576\n",
      "Step = 259 train_loss: 0.031504307 val_loss: 0.044367757\n",
      "Step = 260 train_loss: 0.02707981 val_loss: 0.044156726\n",
      "Step = 261 train_loss: 0.033899367 val_loss: 0.043394655\n",
      "Step = 262 train_loss: 0.03778533 val_loss: 0.042511243\n",
      "Step = 263 train_loss: 0.027267076 val_loss: 0.041875694\n",
      "Step = 264 train_loss: 0.026843041 val_loss: 0.041895766\n",
      "Step = 265 train_loss: 0.04491294 val_loss: 0.04198692\n",
      "Step = 266 train_loss: 0.041314296 val_loss: 0.04220979\n",
      "Step = 267 train_loss: 0.02372371 val_loss: 0.04235769\n",
      "Step = 268 train_loss: 0.031804714 val_loss: 0.042454693\n",
      "Step = 269 train_loss: 0.03103963 val_loss: 0.042443436\n",
      "Step = 270 train_loss: 0.018223654 val_loss: 0.04247936\n",
      "Step = 271 train_loss: 0.030480277 val_loss: 0.04250749\n",
      "Step = 272 train_loss: 0.033176906 val_loss: 0.042611953\n",
      "Step = 273 train_loss: 0.023769973 val_loss: 0.042773943\n",
      "Step = 274 train_loss: 0.025317762 val_loss: 0.042845953\n",
      "Step = 275 train_loss: 0.025452826 val_loss: 0.042791784\n",
      "Step = 276 train_loss: 0.034567878 val_loss: 0.04295987\n",
      "Step = 277 train_loss: 0.026087124 val_loss: 0.043202765\n",
      "Step = 278 train_loss: 0.03144698 val_loss: 0.04341065\n",
      "Step = 279 train_loss: 0.024889763 val_loss: 0.043673746\n",
      "Step = 280 train_loss: 0.031612344 val_loss: 0.043917026\n",
      "Step = 281 train_loss: 0.030159088 val_loss: 0.044357702\n",
      "Step = 282 train_loss: 0.026199477 val_loss: 0.045103215\n",
      "Step = 283 train_loss: 0.03536538 val_loss: 0.046616353\n",
      "Step = 284 train_loss: 0.029370936 val_loss: 0.048253007\n",
      "Step = 285 train_loss: 0.035242226 val_loss: 0.05044608\n",
      "Step = 286 train_loss: 0.038849894 val_loss: 0.05259036\n",
      "Step = 287 train_loss: 0.025213532 val_loss: 0.053942885\n",
      "Step = 288 train_loss: 0.037229978 val_loss: 0.054064453\n",
      "Step = 289 train_loss: 0.022212349 val_loss: 0.04802057\n",
      "Step = 290 train_loss: 0.016936349 val_loss: 0.040727675\n",
      "Step = 291 train_loss: 0.024079172 val_loss: 0.037502497\n",
      "Step = 292 train_loss: 0.029949747 val_loss: 0.04035977\n",
      "Step = 293 train_loss: 0.024923481 val_loss: 0.045160975\n",
      "Step = 294 train_loss: 0.020299396 val_loss: 0.03701981\n",
      "Step = 295 train_loss: 0.032145612 val_loss: 0.033996206\n",
      "Step = 296 train_loss: 0.025596984 val_loss: 0.033004228\n",
      "Step = 297 train_loss: 0.036636945 val_loss: 0.03256147\n",
      "Step = 298 train_loss: 0.03237216 val_loss: 0.03251956\n",
      "Step = 299 train_loss: 0.03732782 val_loss: 0.032497875\n",
      "Step = 300 train_loss: 0.03632087 val_loss: 0.032746345\n",
      "Step = 301 train_loss: 0.03573841 val_loss: 0.03303617\n",
      "Step = 302 train_loss: 0.041750904 val_loss: 0.03350748\n",
      "Step = 303 train_loss: 0.028733253 val_loss: 0.034094892\n",
      "Step = 304 train_loss: 0.0285075 val_loss: 0.034667723\n",
      "Step = 305 train_loss: 0.038586594 val_loss: 0.03507292\n",
      "Step = 306 train_loss: 0.032309987 val_loss: 0.03554955\n",
      "Step = 307 train_loss: 0.033575747 val_loss: 0.035902523\n",
      "Step = 308 train_loss: 0.023573272 val_loss: 0.036222152\n",
      "Step = 309 train_loss: 0.0241735 val_loss: 0.036428656\n",
      "Step = 310 train_loss: 0.030862888 val_loss: 0.03654045\n",
      "Step = 311 train_loss: 0.04604475 val_loss: 0.03666659\n",
      "Step = 312 train_loss: 0.03677799 val_loss: 0.036873333\n",
      "Step = 313 train_loss: 0.030754616 val_loss: 0.036940128\n",
      "Step = 314 train_loss: 0.026827808 val_loss: 0.037127778\n",
      "Step = 315 train_loss: 0.046438854 val_loss: 0.03745389\n",
      "Step = 316 train_loss: 0.029724361 val_loss: 0.037783697\n",
      "Step = 317 train_loss: 0.026865572 val_loss: 0.0380638\n",
      "Step = 318 train_loss: 0.028554063 val_loss: 0.03847844\n",
      "Step = 319 train_loss: 0.03913035 val_loss: 0.038682543\n",
      "Step = 320 train_loss: 0.030682612 val_loss: 0.03913931\n",
      "Step = 321 train_loss: 0.032793067 val_loss: 0.039848384\n",
      "Step = 322 train_loss: 0.028997203 val_loss: 0.04057358\n",
      "Step = 323 train_loss: 0.03040276 val_loss: 0.04161577\n",
      "Step = 324 train_loss: 0.034555886 val_loss: 0.042477775\n",
      "Step = 325 train_loss: 0.033483677 val_loss: 0.044057243\n",
      "Step = 326 train_loss: 0.038453944 val_loss: 0.04570682\n",
      "Step = 327 train_loss: 0.02420683 val_loss: 0.046384603\n",
      "Step = 328 train_loss: 0.03069884 val_loss: 0.0439423\n",
      "Step = 329 train_loss: 0.029614963 val_loss: 0.03771969\n",
      "Step = 330 train_loss: 0.02241844 val_loss: 0.031991143\n",
      "Step = 331 train_loss: 0.02454853 val_loss: 0.029280461\n",
      "Step = 332 train_loss: 0.02083118 val_loss: 0.03335393\n",
      "Step = 333 train_loss: 0.02363472 val_loss: 0.035197612\n",
      "Step = 334 train_loss: 0.02456969 val_loss: 0.0439427\n",
      "Step = 335 train_loss: 0.026430411 val_loss: 0.04579953\n",
      "Step = 336 train_loss: 0.019065255 val_loss: 0.046191663\n",
      "Step = 337 train_loss: 0.028153248 val_loss: 0.045109123\n",
      "Step = 338 train_loss: 0.022953957 val_loss: 0.04231179\n",
      "Step = 339 train_loss: 0.015255417 val_loss: 0.039101217\n",
      "Step = 340 train_loss: 0.025628347 val_loss: 0.045954276\n",
      "Step = 341 train_loss: 0.03255698 val_loss: 0.036549732\n",
      "Step = 342 train_loss: 0.021690167 val_loss: 0.029278921\n",
      "Step = 343 train_loss: 0.02418408 val_loss: 0.029345382\n",
      "Step = 344 train_loss: 0.026705299 val_loss: 0.028231755\n",
      "Step = 345 train_loss: 0.032318305 val_loss: 0.03945781\n",
      "Step = 346 train_loss: 0.025592621 val_loss: 0.046655294\n",
      "Step = 347 train_loss: 0.01782311 val_loss: 0.04269459\n",
      "Step = 348 train_loss: 0.027366476 val_loss: 0.03523082\n",
      "Step = 349 train_loss: 0.029768052 val_loss: 0.034387026\n",
      "Step = 350 train_loss: 0.028452575 val_loss: 0.037065547\n",
      "Step = 351 train_loss: 0.024630535 val_loss: 0.041064918\n",
      "Step = 352 train_loss: 0.035559185 val_loss: 0.04503428\n",
      "Step = 353 train_loss: 0.022618668 val_loss: 0.051408466\n",
      "Step = 354 train_loss: 0.034464993 val_loss: 0.053664144\n",
      "Step = 355 train_loss: 0.03533922 val_loss: 0.029159889\n",
      "Step = 356 train_loss: 0.03230071 val_loss: 0.023183288\n",
      "Step = 357 train_loss: 0.020489287 val_loss: 0.023503175\n",
      "Step = 358 train_loss: 0.026602319 val_loss: 0.023907512\n",
      "Step = 359 train_loss: 0.021565568 val_loss: 0.050978165\n",
      "Step = 360 train_loss: 0.020876694 val_loss: 0.055217057\n",
      "Step = 361 train_loss: 0.027649926 val_loss: 0.052535087\n",
      "Step = 362 train_loss: 0.034724355 val_loss: 0.051712014\n",
      "Step = 363 train_loss: 0.032905314 val_loss: 0.052004576\n",
      "Step = 364 train_loss: 0.033047583 val_loss: 0.0525793\n",
      "Step = 365 train_loss: 0.03236611 val_loss: 0.053424172\n",
      "Step = 366 train_loss: 0.028498892 val_loss: 0.054309316\n",
      "Step = 367 train_loss: 0.036937222 val_loss: 0.05558727\n",
      "Step = 368 train_loss: 0.03292673 val_loss: 0.057733472\n",
      "Step = 369 train_loss: 0.026931453 val_loss: 0.059079945\n",
      "Step = 370 train_loss: 0.03229989 val_loss: 0.060794853\n",
      "Step = 371 train_loss: 0.02996588 val_loss: 0.061837904\n",
      "Step = 372 train_loss: 0.027015366 val_loss: 0.058524877\n",
      "Step = 373 train_loss: 0.030031618 val_loss: 0.053141944\n",
      "Step = 374 train_loss: 0.03612494 val_loss: 0.04824768\n",
      "Step = 375 train_loss: 0.028513102 val_loss: 0.042726744\n",
      "Step = 376 train_loss: 0.037570883 val_loss: 0.033627328\n",
      "Step = 377 train_loss: 0.030928873 val_loss: 0.02510077\n",
      "Step = 378 train_loss: 0.027336152 val_loss: 0.02168907\n",
      "Step = 379 train_loss: 0.022971852 val_loss: 0.021661859\n",
      "Step = 380 train_loss: 0.0252893 val_loss: 0.02160193\n",
      "Step = 381 train_loss: 0.033137243 val_loss: 0.021476652\n",
      "Step = 382 train_loss: 0.014045808 val_loss: 0.021327797\n",
      "Step = 383 train_loss: 0.027802553 val_loss: 0.021806916\n",
      "Step = 384 train_loss: 0.026375975 val_loss: 0.022519955\n",
      "Step = 385 train_loss: 0.020139877 val_loss: 0.02291971\n",
      "Step = 386 train_loss: 0.032985263 val_loss: 0.023410931\n",
      "Step = 387 train_loss: 0.027906477 val_loss: 0.02367284\n",
      "Step = 388 train_loss: 0.016719596 val_loss: 0.023643035\n",
      "Step = 389 train_loss: 0.016923252 val_loss: 0.023063367\n",
      "Step = 390 train_loss: 0.01802177 val_loss: 0.022667496\n",
      "Step = 391 train_loss: 0.015388373 val_loss: 0.022344593\n",
      "Step = 392 train_loss: 0.010050405 val_loss: 0.022078995\n",
      "Step = 393 train_loss: 0.022160437 val_loss: 0.0219685\n",
      "Step = 394 train_loss: 0.018057792 val_loss: 0.021934412\n",
      "Step = 395 train_loss: 0.028815467 val_loss: 0.022065047\n",
      "Step = 396 train_loss: 0.022876631 val_loss: 0.02212345\n",
      "Step = 397 train_loss: 0.015972957 val_loss: 0.021930303\n",
      "Step = 398 train_loss: 0.026176687 val_loss: 0.021625094\n",
      "Step = 399 train_loss: 0.020710062 val_loss: 0.03786059\n",
      "Step = 400 train_loss: 0.016097419 val_loss: 0.036022075\n",
      "Step = 401 train_loss: 0.02005534 val_loss: 0.020019177\n",
      "Step = 402 train_loss: 0.013667667 val_loss: 0.019791063\n",
      "Step = 403 train_loss: 0.024450766 val_loss: 0.022152815\n",
      "Step = 404 train_loss: 0.017465157 val_loss: 0.020583369\n",
      "Step = 405 train_loss: 0.0122411 val_loss: 0.019866075\n",
      "Step = 406 train_loss: 0.014548226 val_loss: 0.03806374\n",
      "Step = 407 train_loss: 0.018232718 val_loss: 0.019522093\n",
      "Step = 408 train_loss: 0.027665433 val_loss: 0.01997152\n",
      "Step = 409 train_loss: 0.022671562 val_loss: 0.020085229\n",
      "Step = 410 train_loss: 0.031659678 val_loss: 0.022470292\n",
      "Step = 411 train_loss: 0.01062045 val_loss: 0.06053463\n",
      "Step = 412 train_loss: 0.017938726 val_loss: 0.05969711\n",
      "Step = 413 train_loss: 0.023488127 val_loss: 0.030274967\n",
      "Step = 414 train_loss: 0.015267802 val_loss: 0.019331578\n",
      "Step = 415 train_loss: 0.0158426 val_loss: 0.019556142\n",
      "Step = 416 train_loss: 0.024533153 val_loss: 0.019798674\n",
      "Step = 417 train_loss: 0.021355173 val_loss: 0.04086801\n",
      "Step = 418 train_loss: 0.021632003 val_loss: 0.05987043\n",
      "Step = 419 train_loss: 0.021383265 val_loss: 0.063867174\n",
      "Step = 420 train_loss: 0.019612815 val_loss: 0.046183288\n",
      "Step = 421 train_loss: 0.020925798 val_loss: 0.03381021\n",
      "Step = 422 train_loss: 0.01844527 val_loss: 0.019398065\n",
      "Step = 423 train_loss: 0.019553326 val_loss: 0.019731719\n",
      "Step = 424 train_loss: 0.022951497 val_loss: 0.031845264\n",
      "Step = 425 train_loss: 0.02208791 val_loss: 0.06307235\n",
      "Step = 426 train_loss: 0.020293117 val_loss: 0.02660642\n",
      "Step = 427 train_loss: 0.016847113 val_loss: 0.020769704\n",
      "Step = 428 train_loss: 0.023505868 val_loss: 0.019921849\n",
      "Step = 429 train_loss: 0.02211872 val_loss: 0.021774054\n",
      "Step = 430 train_loss: 0.014688208 val_loss: 0.06844815\n",
      "Step = 431 train_loss: 0.01216114 val_loss: 0.07265909\n",
      "Step = 432 train_loss: 0.02158212 val_loss: 0.07242568\n",
      "Step = 433 train_loss: 0.020260604 val_loss: 0.056806926\n",
      "Step = 434 train_loss: 0.019124957 val_loss: 0.019004028\n",
      "Step = 435 train_loss: 0.01705629 val_loss: 0.019277945\n",
      "Step = 436 train_loss: 0.016580608 val_loss: 0.019570975\n",
      "Step = 437 train_loss: 0.011536868 val_loss: 0.019693376\n",
      "Step = 438 train_loss: 0.011879999 val_loss: 0.019801203\n",
      "Step = 439 train_loss: 0.015642637 val_loss: 0.020036187\n",
      "Step = 440 train_loss: 0.03025232 val_loss: 0.022680506\n",
      "Step = 441 train_loss: 0.010980701 val_loss: 0.04343094\n",
      "Step = 442 train_loss: 0.017011322 val_loss: 0.07351208\n",
      "Step = 443 train_loss: 0.019732144 val_loss: 0.07214309\n",
      "Step = 444 train_loss: 0.023753343 val_loss: 0.07250537\n",
      "Step = 445 train_loss: 0.01953467 val_loss: 0.03116145\n",
      "Step = 446 train_loss: 0.018024268 val_loss: 0.01871099\n",
      "Step = 447 train_loss: 0.013626283 val_loss: 0.019060932\n",
      "Step = 448 train_loss: 0.019270185 val_loss: 0.019019833\n",
      "Step = 449 train_loss: 0.019756356 val_loss: 0.036913097\n",
      "Step = 450 train_loss: 0.009724781 val_loss: 0.06665769\n",
      "Step = 451 train_loss: 0.017902466 val_loss: 0.056693234\n",
      "Step = 452 train_loss: 0.013792876 val_loss: 0.019246368\n",
      "Step = 453 train_loss: 0.012126626 val_loss: 0.019824049\n",
      "Step = 454 train_loss: 0.012709494 val_loss: 0.075754486\n",
      "Step = 455 train_loss: 0.010458848 val_loss: 0.078079924\n",
      "Step = 456 train_loss: 0.018872928 val_loss: 0.07784399\n",
      "Step = 457 train_loss: 0.01865701 val_loss: 0.0769967\n",
      "Step = 458 train_loss: 0.014266413 val_loss: 0.059844483\n",
      "Step = 459 train_loss: 0.018355725 val_loss: 0.019105446\n",
      "Step = 460 train_loss: 0.01731196 val_loss: 0.01967322\n",
      "Step = 461 train_loss: 0.016815577 val_loss: 0.01996783\n",
      "Step = 462 train_loss: 0.025647182 val_loss: 0.020180184\n",
      "Step = 463 train_loss: 0.02574856 val_loss: 0.020254085\n",
      "Step = 464 train_loss: 0.017614722 val_loss: 0.022529969\n",
      "Step = 465 train_loss: 0.007564964 val_loss: 0.07640394\n",
      "Step = 466 train_loss: 0.015501697 val_loss: 0.08009999\n",
      "Step = 467 train_loss: 0.019278098 val_loss: 0.07584366\n",
      "Step = 468 train_loss: 0.01680121 val_loss: 0.03273011\n",
      "Step = 469 train_loss: 0.012806211 val_loss: 0.01931275\n",
      "Step = 470 train_loss: 0.015647799 val_loss: 0.019651927\n",
      "Step = 471 train_loss: 0.023338152 val_loss: 0.019805307\n",
      "Step = 472 train_loss: 0.025324421 val_loss: 0.019991923\n",
      "Step = 473 train_loss: 0.020953132 val_loss: 0.020177262\n",
      "Step = 474 train_loss: 0.02191031 val_loss: 0.028856885\n",
      "Step = 475 train_loss: 0.017161457 val_loss: 0.07891896\n",
      "Step = 476 train_loss: 0.020813921 val_loss: 0.07436567\n",
      "Step = 477 train_loss: 0.018851874 val_loss: 0.07512872\n",
      "Step = 478 train_loss: 0.020426668 val_loss: 0.07796499\n",
      "Step = 479 train_loss: 0.024460955 val_loss: 0.07197506\n",
      "Step = 480 train_loss: 0.013917191 val_loss: 0.018854218\n",
      "Step = 481 train_loss: 0.020447562 val_loss: 0.019805787\n",
      "Step = 482 train_loss: 0.00991573 val_loss: 0.020153796\n",
      "Step = 483 train_loss: 0.017395662 val_loss: 0.020599496\n",
      "Step = 484 train_loss: 0.019229287 val_loss: 0.021173935\n",
      "Step = 485 train_loss: 0.01697902 val_loss: 0.0217648\n",
      "Step = 486 train_loss: 0.016417217 val_loss: 0.022444239\n",
      "Step = 487 train_loss: 0.023693707 val_loss: 0.023819072\n",
      "Step = 488 train_loss: 0.018233767 val_loss: 0.03227759\n",
      "Step = 489 train_loss: 0.013919318 val_loss: 0.05340745\n",
      "Step = 490 train_loss: 0.014207109 val_loss: 0.0824193\n",
      "Step = 491 train_loss: 0.0120331235 val_loss: 0.07894843\n",
      "Step = 492 train_loss: 0.013105611 val_loss: 0.07781343\n",
      "Step = 493 train_loss: 0.021876145 val_loss: 0.079475895\n",
      "Step = 494 train_loss: 0.014912739 val_loss: 0.08047044\n",
      "Step = 495 train_loss: 0.01756651 val_loss: 0.042178843\n",
      "Step = 496 train_loss: 0.017883725 val_loss: 0.019530676\n",
      "Step = 497 train_loss: 0.013143221 val_loss: 0.019732248\n",
      "Step = 498 train_loss: 0.0149896685 val_loss: 0.032421395\n",
      "Step = 499 train_loss: 0.016618894 val_loss: 0.071138695\n",
      "Step = 500 train_loss: 0.016136512 val_loss: 0.056687396\n",
      "Step = 501 train_loss: 0.009832373 val_loss: 0.026855147\n",
      "Step = 502 train_loss: 0.02363327 val_loss: 0.023686765\n",
      "Step = 503 train_loss: 0.012631099 val_loss: 0.054200802\n",
      "Step = 504 train_loss: 0.006513438 val_loss: 0.059819624\n",
      "Step = 505 train_loss: 0.01029038 val_loss: 0.064366885\n",
      "Step = 506 train_loss: 0.011859785 val_loss: 0.05646637\n",
      "Step = 507 train_loss: 0.007008704 val_loss: 0.02736233\n",
      "Step = 508 train_loss: 0.013191085 val_loss: 0.025878124\n",
      "Step = 509 train_loss: 0.0135776345 val_loss: 0.027289446\n",
      "Step = 510 train_loss: 0.009980618 val_loss: 0.059495334\n",
      "Step = 511 train_loss: 0.013986081 val_loss: 0.057114568\n",
      "Step = 512 train_loss: 0.012424748 val_loss: 0.047071323\n",
      "Step = 513 train_loss: 0.01117692 val_loss: 0.033587974\n",
      "Step = 514 train_loss: 0.021162381 val_loss: 0.035085708\n",
      "Step = 515 train_loss: 0.0065838923 val_loss: 0.035008132\n",
      "Step = 516 train_loss: 0.013607366 val_loss: 0.04500816\n",
      "Step = 517 train_loss: 0.0100320075 val_loss: 0.06585999\n",
      "Step = 518 train_loss: 0.013138527 val_loss: 0.07415347\n",
      "Step = 519 train_loss: 0.008085325 val_loss: 0.07895325\n",
      "Step = 520 train_loss: 0.0068264413 val_loss: 0.07309214\n",
      "Step = 521 train_loss: 0.014379098 val_loss: 0.050997235\n",
      "Step = 522 train_loss: 0.012097081 val_loss: 0.020349089\n",
      "Step = 523 train_loss: 0.010046885 val_loss: 0.020069947\n",
      "Step = 524 train_loss: 0.0125460075 val_loss: 0.020682763\n",
      "Step = 525 train_loss: 0.011281742 val_loss: 0.0229475\n",
      "Step = 526 train_loss: 0.0115409745 val_loss: 0.05905053\n",
      "Step = 527 train_loss: 0.008197575 val_loss: 0.07793102\n",
      "Step = 528 train_loss: 0.008237715 val_loss: 0.07655539\n",
      "Step = 529 train_loss: 0.014928703 val_loss: 0.03827652\n",
      "Step = 530 train_loss: 0.013683115 val_loss: 0.020191938\n",
      "Step = 531 train_loss: 0.016570203 val_loss: 0.020288382\n",
      "Step = 532 train_loss: 0.0108967805 val_loss: 0.020711835\n",
      "Step = 533 train_loss: 0.017306192 val_loss: 0.021198908\n",
      "Step = 534 train_loss: 0.017240407 val_loss: 0.026107745\n",
      "Step = 535 train_loss: 0.020065727 val_loss: 0.07237585\n",
      "Step = 536 train_loss: 0.012149547 val_loss: 0.077091604\n",
      "Step = 537 train_loss: 0.009106054 val_loss: 0.074799344\n",
      "Step = 538 train_loss: 0.011173058 val_loss: 0.06824601\n",
      "Step = 539 train_loss: 0.01646304 val_loss: 0.012587755\n",
      "Step = 540 train_loss: 0.009924712 val_loss: 0.037825644\n",
      "Step = 541 train_loss: 0.0072915554 val_loss: 0.05082069\n",
      "Step = 542 train_loss: 0.014564972 val_loss: 0.05195898\n",
      "Step = 543 train_loss: 0.02145983 val_loss: 0.019598998\n",
      "Step = 544 train_loss: 0.0075478074 val_loss: 0.03171367\n",
      "Step = 545 train_loss: 0.017748466 val_loss: 0.04908282\n",
      "Step = 546 train_loss: 0.012742669 val_loss: 0.020907357\n",
      "Step = 547 train_loss: 0.014255917 val_loss: 0.03224517\n",
      "Step = 548 train_loss: 0.010521696 val_loss: 0.032571778\n",
      "Step = 549 train_loss: 0.010577662 val_loss: 0.01918414\n",
      "Step = 550 train_loss: 0.0073482348 val_loss: 0.012778057\n",
      "Step = 551 train_loss: 0.015354185 val_loss: 0.04312143\n",
      "Step = 552 train_loss: 0.008133929 val_loss: 0.03748113\n",
      "Step = 553 train_loss: 0.015516654 val_loss: 0.00955652\n",
      "Step = 554 train_loss: 0.015228506 val_loss: 0.020685999\n",
      "Step = 555 train_loss: 0.01653277 val_loss: 0.022307154\n",
      "Step = 556 train_loss: 0.012540897 val_loss: 0.017994283\n",
      "Step = 557 train_loss: 0.0117493225 val_loss: 0.01682903\n",
      "Step = 558 train_loss: 0.009481361 val_loss: 0.059789587\n",
      "Step = 559 train_loss: 0.0124032255 val_loss: 0.06943284\n",
      "Step = 560 train_loss: 0.016216366 val_loss: 0.059566334\n",
      "Step = 561 train_loss: 0.016158001 val_loss: 0.010265069\n",
      "Step = 562 train_loss: 0.011336936 val_loss: 0.025899103\n",
      "Step = 563 train_loss: 0.015369844 val_loss: 0.03441971\n",
      "Step = 564 train_loss: 0.013455317 val_loss: 0.007428441\n",
      "Step = 565 train_loss: 0.012938102 val_loss: 0.027989967\n",
      "Step = 566 train_loss: 0.0047391555 val_loss: 0.06803609\n",
      "Step = 567 train_loss: 0.008063207 val_loss: 0.07045457\n",
      "Step = 568 train_loss: 0.018034056 val_loss: 0.06494314\n",
      "Step = 569 train_loss: 0.00875543 val_loss: 0.046880286\n",
      "Step = 570 train_loss: 0.014465907 val_loss: 0.108869866\n",
      "Step = 571 train_loss: 0.011290979 val_loss: 0.1106807\n",
      "Step = 572 train_loss: 0.011910731 val_loss: 0.07387577\n",
      "Step = 573 train_loss: 0.01672311 val_loss: 0.030295111\n",
      "Step = 574 train_loss: 0.018346297 val_loss: 0.05311457\n",
      "Step = 575 train_loss: 0.0107245855 val_loss: 0.08199967\n",
      "Step = 576 train_loss: 0.017170565 val_loss: 0.08372849\n",
      "Step = 577 train_loss: 0.021611203 val_loss: 0.08361102\n",
      "Step = 578 train_loss: 0.015594757 val_loss: 0.07339768\n",
      "Step = 579 train_loss: 0.013810873 val_loss: 0.051152933\n",
      "Step = 580 train_loss: 0.009287825 val_loss: 0.015328838\n",
      "Step = 581 train_loss: 0.008357195 val_loss: 0.041152682\n",
      "Step = 582 train_loss: 0.012510154 val_loss: 0.081558265\n",
      "Step = 583 train_loss: 0.015163023 val_loss: 0.119704664\n",
      "Step = 584 train_loss: 0.014357931 val_loss: 0.13171226\n",
      "Step = 585 train_loss: 0.012544661 val_loss: 0.007415411\n",
      "Step = 586 train_loss: 0.007341348 val_loss: 0.05162013\n",
      "Step = 587 train_loss: 0.009283812 val_loss: 0.061395906\n",
      "Step = 588 train_loss: 0.008970363 val_loss: 0.062848225\n",
      "Step = 589 train_loss: 0.011708999 val_loss: 0.055301417\n",
      "Step = 590 train_loss: 0.017678326 val_loss: 0.022291547\n",
      "Step = 591 train_loss: 0.0150384605 val_loss: 0.029225955\n",
      "Step = 592 train_loss: 0.0052829986 val_loss: 0.07438873\n",
      "Step = 593 train_loss: 0.010467796 val_loss: 0.05944462\n",
      "Step = 594 train_loss: 0.010310981 val_loss: 0.042434715\n",
      "Step = 595 train_loss: 0.01423502 val_loss: 0.023405334\n",
      "Step = 596 train_loss: 0.01198719 val_loss: 0.052681003\n",
      "Step = 597 train_loss: 0.011755667 val_loss: 0.043761764\n",
      "Step = 598 train_loss: 0.01081657 val_loss: 0.022842268\n",
      "Step = 599 train_loss: 0.017228123 val_loss: 0.015965981\n",
      "Step = 600 train_loss: 0.010542953 val_loss: 0.01023162\n",
      "Step = 601 train_loss: 0.008816907 val_loss: 0.018951327\n",
      "Step = 602 train_loss: 0.0127563225 val_loss: 0.020343075\n",
      "Step = 603 train_loss: 0.0064320145 val_loss: 0.011491532\n",
      "Step = 604 train_loss: 0.007028087 val_loss: 0.03622883\n",
      "Step = 605 train_loss: 0.009100376 val_loss: 0.055472475\n",
      "Step = 606 train_loss: 0.006561882 val_loss: 0.05176219\n",
      "Step = 607 train_loss: 0.012245164 val_loss: 0.014096551\n",
      "Step = 608 train_loss: 0.009720562 val_loss: 0.019283293\n",
      "Step = 609 train_loss: 0.014963284 val_loss: 0.019137124\n",
      "Step = 610 train_loss: 0.017026644 val_loss: 0.017730193\n",
      "Step = 611 train_loss: 0.011094239 val_loss: 0.016438926\n",
      "Step = 612 train_loss: 0.00562652 val_loss: 0.06553863\n",
      "Step = 613 train_loss: 0.012283217 val_loss: 0.07492657\n",
      "Step = 614 train_loss: 0.0143490825 val_loss: 0.07408416\n",
      "Step = 615 train_loss: 0.013212777 val_loss: 0.061765995\n",
      "Step = 616 train_loss: 0.015558996 val_loss: 0.01819383\n",
      "Step = 617 train_loss: 0.007189234 val_loss: 0.018011667\n",
      "Step = 618 train_loss: 0.012717912 val_loss: 0.018603778\n",
      "Step = 619 train_loss: 0.011669075 val_loss: 0.01848286\n",
      "Step = 620 train_loss: 0.017726677 val_loss: 0.020683628\n",
      "Step = 621 train_loss: 0.010953882 val_loss: 0.06513868\n",
      "Step = 622 train_loss: 0.018048348 val_loss: 0.07575759\n",
      "Step = 623 train_loss: 0.012341901 val_loss: 0.07697788\n",
      "Step = 624 train_loss: 0.011115521 val_loss: 0.07619318\n",
      "Step = 625 train_loss: 0.012975394 val_loss: 0.06857792\n",
      "Step = 626 train_loss: 0.013615375 val_loss: 0.017267587\n",
      "Step = 627 train_loss: 0.0070161168 val_loss: 0.01909308\n",
      "Step = 628 train_loss: 0.015861563 val_loss: 0.0195371\n",
      "Step = 629 train_loss: 0.017595472 val_loss: 0.019930916\n",
      "Step = 630 train_loss: 0.00556572 val_loss: 0.037418514\n",
      "Step = 631 train_loss: 0.0041335826 val_loss: 0.06939248\n",
      "Step = 632 train_loss: 0.013458174 val_loss: 0.07146943\n",
      "Step = 633 train_loss: 0.0068161045 val_loss: 0.06734907\n",
      "Step = 634 train_loss: 0.00907304 val_loss: 0.041297484\n",
      "Step = 635 train_loss: 0.008757726 val_loss: 0.026513917\n",
      "Step = 636 train_loss: 0.006527176 val_loss: 0.021100888\n",
      "Step = 637 train_loss: 0.010267962 val_loss: 0.021476725\n",
      "Step = 638 train_loss: 0.0074937763 val_loss: 0.04373151\n",
      "Step = 639 train_loss: 0.011279253 val_loss: 0.058692187\n",
      "Step = 640 train_loss: 0.011097925 val_loss: 0.057478167\n",
      "Step = 641 train_loss: 0.011141652 val_loss: 0.028648553\n",
      "Step = 642 train_loss: 0.010021495 val_loss: 0.018719561\n",
      "Step = 643 train_loss: 0.0091896 val_loss: 0.018660635\n",
      "Step = 644 train_loss: 0.009519117 val_loss: 0.027492043\n",
      "Step = 645 train_loss: 0.00818922 val_loss: 0.044242628\n",
      "Step = 646 train_loss: 0.009595067 val_loss: 0.06597662\n",
      "Step = 647 train_loss: 0.009776102 val_loss: 0.06974213\n",
      "Step = 648 train_loss: 0.011748831 val_loss: 0.06587542\n",
      "Step = 649 train_loss: 0.014899903 val_loss: 0.054791044\n",
      "Step = 650 train_loss: 0.010068501 val_loss: 0.019966114\n",
      "Step = 651 train_loss: 0.004783903 val_loss: 0.01838812\n",
      "Step = 652 train_loss: 0.017725412 val_loss: 0.018819803\n",
      "Step = 653 train_loss: 0.010921811 val_loss: 0.019082349\n",
      "Step = 654 train_loss: 0.018199455 val_loss: 0.019995455\n",
      "Step = 655 train_loss: 0.0076723862 val_loss: 0.04886191\n",
      "Step = 656 train_loss: 0.0058687385 val_loss: 0.06813792\n",
      "Step = 657 train_loss: 0.0101687955 val_loss: 0.0710742\n",
      "Step = 658 train_loss: 0.01261671 val_loss: 0.06984852\n",
      "Step = 659 train_loss: 0.013503752 val_loss: 0.0607877\n",
      "Step = 660 train_loss: 0.009838421 val_loss: 0.018568313\n",
      "Step = 661 train_loss: 0.010335366 val_loss: 0.018591166\n",
      "Step = 662 train_loss: 0.015134147 val_loss: 0.018993696\n",
      "Step = 663 train_loss: 0.008627269 val_loss: 0.01934257\n",
      "Step = 664 train_loss: 0.012480786 val_loss: 0.023564275\n",
      "Step = 665 train_loss: 0.006860253 val_loss: 0.06029347\n",
      "Step = 666 train_loss: 0.0046478333 val_loss: 0.06611618\n",
      "Step = 667 train_loss: 0.013554239 val_loss: 0.05602172\n",
      "Step = 668 train_loss: 0.010568334 val_loss: 0.02899418\n",
      "Step = 669 train_loss: 0.011666084 val_loss: 0.03422395\n",
      "Step = 670 train_loss: 0.02017407 val_loss: 0.041737992\n",
      "Step = 671 train_loss: 0.00348275 val_loss: 0.044148132\n",
      "Step = 672 train_loss: 0.008454205 val_loss: 0.046394262\n",
      "Step = 673 train_loss: 0.0072283596 val_loss: 0.042458598\n",
      "Step = 674 train_loss: 0.007935595 val_loss: 0.036704544\n",
      "Step = 675 train_loss: 0.010570074 val_loss: 0.03234795\n",
      "Step = 676 train_loss: 0.009537753 val_loss: 0.036411367\n",
      "Step = 677 train_loss: 0.0071500177 val_loss: 0.024720507\n",
      "Step = 678 train_loss: 0.0077884197 val_loss: 0.040516835\n",
      "Step = 679 train_loss: 0.011723838 val_loss: 0.05929972\n",
      "Step = 680 train_loss: 0.010410199 val_loss: 0.06427207\n",
      "Step = 681 train_loss: 0.010087346 val_loss: 0.06277522\n",
      "Step = 682 train_loss: 0.006249805 val_loss: 0.05513752\n",
      "Step = 683 train_loss: 0.009424383 val_loss: 0.048059754\n",
      "Step = 684 train_loss: 0.006325014 val_loss: 0.022901604\n",
      "Step = 685 train_loss: 0.0070968047 val_loss: 0.019332781\n",
      "Step = 686 train_loss: 0.011001399 val_loss: 0.019836416\n",
      "Step = 687 train_loss: 0.008561268 val_loss: 0.020810736\n",
      "Step = 688 train_loss: 0.011640986 val_loss: 0.03355707\n",
      "Step = 689 train_loss: 0.011481962 val_loss: 0.06393993\n",
      "Step = 690 train_loss: 0.009144444 val_loss: 0.06601382\n",
      "Step = 691 train_loss: 0.0072914343 val_loss: 0.06596427\n",
      "Step = 692 train_loss: 0.0069117798 val_loss: 0.06363209\n",
      "Step = 693 train_loss: 0.0074011143 val_loss: 0.0555495\n",
      "Step = 694 train_loss: 0.0075405687 val_loss: 0.03167798\n",
      "Step = 695 train_loss: 0.006843873 val_loss: 0.022394244\n",
      "Step = 696 train_loss: 0.008977852 val_loss: 0.02257489\n",
      "Step = 697 train_loss: 0.008421055 val_loss: 0.024139551\n",
      "Step = 698 train_loss: 0.013914562 val_loss: 0.03796852\n",
      "Step = 699 train_loss: 0.013865139 val_loss: 0.06328402\n",
      "Step = 700 train_loss: 0.0050292243 val_loss: 0.06578577\n",
      "Step = 701 train_loss: 0.0034835162 val_loss: 0.06684736\n",
      "Step = 702 train_loss: 0.006753279 val_loss: 0.065734215\n",
      "Step = 703 train_loss: 0.010346938 val_loss: 0.060095344\n",
      "Step = 704 train_loss: 0.011214687 val_loss: 0.021671377\n",
      "Step = 705 train_loss: 0.006621098 val_loss: 0.020492386\n",
      "Step = 706 train_loss: 0.011117707 val_loss: 0.020679722\n",
      "Step = 707 train_loss: 0.007987361 val_loss: 0.033928193\n",
      "Step = 708 train_loss: 0.008102385 val_loss: 0.061004423\n",
      "Step = 709 train_loss: 0.0043781493 val_loss: 0.06155264\n",
      "Step = 710 train_loss: 0.013477235 val_loss: 0.056956552\n",
      "Step = 711 train_loss: 0.011888653 val_loss: 0.03564156\n",
      "Step = 712 train_loss: 0.008459947 val_loss: 0.028416038\n",
      "Step = 713 train_loss: 0.009492244 val_loss: 0.021833993\n",
      "Step = 714 train_loss: 0.008086349 val_loss: 0.039865263\n",
      "Step = 715 train_loss: 0.0066351946 val_loss: 0.040032133\n",
      "Step = 716 train_loss: 0.004939303 val_loss: 0.03796448\n",
      "Step = 717 train_loss: 0.011108213 val_loss: 0.022433087\n",
      "Step = 718 train_loss: 0.0072304383 val_loss: 0.01613252\n",
      "Step = 719 train_loss: 0.009614322 val_loss: 0.016598545\n",
      "Step = 720 train_loss: 0.0118356515 val_loss: 0.020472705\n",
      "Step = 721 train_loss: 0.013147288 val_loss: 0.057082642\n",
      "Step = 722 train_loss: 0.0062441104 val_loss: 0.062347814\n",
      "Step = 723 train_loss: 0.015677627 val_loss: 0.055364788\n",
      "Step = 724 train_loss: 0.008706159 val_loss: 0.02677286\n",
      "Step = 725 train_loss: 0.008723478 val_loss: 0.01875143\n",
      "Step = 726 train_loss: 0.0075916434 val_loss: 0.024018355\n",
      "Step = 727 train_loss: 0.006554174 val_loss: 0.022006078\n",
      "Step = 728 train_loss: 0.013996354 val_loss: 0.040007982\n",
      "Step = 729 train_loss: 0.009474991 val_loss: 0.056375183\n",
      "Step = 730 train_loss: 0.01531347 val_loss: 0.058815643\n",
      "Step = 731 train_loss: 0.0059150187 val_loss: 0.05865927\n",
      "Step = 732 train_loss: 0.0066390196 val_loss: 0.05566111\n",
      "Step = 733 train_loss: 0.009472838 val_loss: 0.023710921\n",
      "Step = 734 train_loss: 0.0031763944 val_loss: 0.015261236\n",
      "Step = 735 train_loss: 0.011189038 val_loss: 0.014280088\n",
      "Step = 736 train_loss: 0.009194757 val_loss: 0.036794357\n",
      "Step = 737 train_loss: 0.005976987 val_loss: 0.062345743\n",
      "Step = 738 train_loss: 0.005426961 val_loss: 0.06456279\n",
      "Step = 739 train_loss: 0.011286606 val_loss: 0.062242217\n",
      "Step = 740 train_loss: 0.013758966 val_loss: 0.050931565\n",
      "Step = 741 train_loss: 0.011491253 val_loss: 0.017034795\n",
      "Step = 742 train_loss: 0.005768608 val_loss: 0.014163515\n",
      "Step = 743 train_loss: 0.0073203873 val_loss: 0.01407731\n",
      "Step = 744 train_loss: 0.006559709 val_loss: 0.013688613\n",
      "Step = 745 train_loss: 0.008232209 val_loss: 0.027356409\n",
      "Step = 746 train_loss: 0.011253606 val_loss: 0.050939962\n",
      "Step = 747 train_loss: 0.0063772947 val_loss: 0.055179603\n",
      "Step = 748 train_loss: 0.010037994 val_loss: 0.048498143\n",
      "Step = 749 train_loss: 0.006179454 val_loss: 0.035652652\n",
      "Step = 750 train_loss: 0.010449663 val_loss: 0.014762303\n",
      "Step = 751 train_loss: 0.011025652 val_loss: 0.01638185\n",
      "Step = 752 train_loss: 0.011928699 val_loss: 0.01486829\n",
      "Step = 753 train_loss: 0.008179994 val_loss: 0.03513344\n",
      "Step = 754 train_loss: 0.007440928 val_loss: 0.059895534\n",
      "Step = 755 train_loss: 0.0071684625 val_loss: 0.060286388\n",
      "Step = 756 train_loss: 0.0050244387 val_loss: 0.054037414\n",
      "Step = 757 train_loss: 0.008391345 val_loss: 0.022015495\n",
      "Step = 758 train_loss: 0.009141529 val_loss: 0.015949782\n",
      "Step = 759 train_loss: 0.010713614 val_loss: 0.0162515\n",
      "Step = 760 train_loss: 0.007189001 val_loss: 0.015999535\n",
      "Step = 761 train_loss: 0.013174758 val_loss: 0.020994673\n",
      "Step = 762 train_loss: 0.008625976 val_loss: 0.055375993\n",
      "Step = 763 train_loss: 0.009299783 val_loss: 0.059585262\n",
      "Step = 764 train_loss: 0.0053635696 val_loss: 0.0598949\n",
      "Step = 765 train_loss: 0.005493078 val_loss: 0.048020218\n",
      "Step = 766 train_loss: 0.007846893 val_loss: 0.02138414\n",
      "Step = 767 train_loss: 0.012164186 val_loss: 0.02482293\n",
      "Step = 768 train_loss: 0.010298569 val_loss: 0.030966265\n",
      "Step = 769 train_loss: 0.005227874 val_loss: 0.044784863\n",
      "Step = 770 train_loss: 0.007122259 val_loss: 0.034080766\n",
      "Step = 771 train_loss: 0.0068708365 val_loss: 0.050718267\n",
      "Step = 772 train_loss: 0.012460322 val_loss: 0.03638642\n",
      "Step = 773 train_loss: 0.010370072 val_loss: 0.02232023\n",
      "Step = 774 train_loss: 0.008194441 val_loss: 0.029429043\n",
      "Step = 775 train_loss: 0.008852022 val_loss: 0.0492882\n",
      "Step = 776 train_loss: 0.011364305 val_loss: 0.038317077\n",
      "Step = 777 train_loss: 0.01387017 val_loss: 0.014125075\n",
      "Step = 778 train_loss: 0.0054656863 val_loss: 0.015410128\n",
      "Step = 779 train_loss: 0.015252455 val_loss: 0.017862061\n",
      "Step = 780 train_loss: 0.0077439123 val_loss: 0.06153902\n",
      "Step = 781 train_loss: 0.009133128 val_loss: 0.06715942\n",
      "Step = 782 train_loss: 0.012431468 val_loss: 0.064609684\n",
      "Step = 783 train_loss: 0.013538138 val_loss: 0.05159951\n",
      "Step = 784 train_loss: 0.009033347 val_loss: 0.013834251\n",
      "Step = 785 train_loss: 0.0045035193 val_loss: 0.016309218\n",
      "Step = 786 train_loss: 0.007282431 val_loss: 0.015940346\n",
      "Step = 787 train_loss: 0.0105846785 val_loss: 0.014530436\n",
      "Step = 788 train_loss: 0.008781744 val_loss: 0.0543218\n",
      "Step = 789 train_loss: 0.0065924944 val_loss: 0.06359866\n",
      "Step = 790 train_loss: 0.011328251 val_loss: 0.066701196\n",
      "Step = 791 train_loss: 0.007309163 val_loss: 0.063937776\n",
      "Step = 792 train_loss: 0.0030043595 val_loss: 0.05938272\n",
      "Step = 793 train_loss: 0.008040678 val_loss: 0.045685608\n",
      "Step = 794 train_loss: 0.018228441 val_loss: 0.0142397825\n",
      "Step = 795 train_loss: 0.008624141 val_loss: 0.015431856\n",
      "Step = 796 train_loss: 0.0061896737 val_loss: 0.016399791\n",
      "Step = 797 train_loss: 0.0068201977 val_loss: 0.01417643\n",
      "Step = 798 train_loss: 0.0109871905 val_loss: 0.05016676\n",
      "Step = 799 train_loss: 0.003528711 val_loss: 0.06419366\n",
      "Step = 800 train_loss: 0.009783575 val_loss: 0.06563224\n",
      "Step = 801 train_loss: 0.010076385 val_loss: 0.060738854\n",
      "Step = 802 train_loss: 0.008701095 val_loss: 0.037482344\n",
      "Step = 803 train_loss: 0.00881297 val_loss: 0.015047039\n",
      "Step = 804 train_loss: 0.0041034273 val_loss: 0.016963942\n",
      "Step = 805 train_loss: 0.0056804647 val_loss: 0.016453356\n",
      "Step = 806 train_loss: 0.008416247 val_loss: 0.01576551\n",
      "Step = 807 train_loss: 0.011209596 val_loss: 0.05571149\n",
      "Step = 808 train_loss: 0.0050209565 val_loss: 0.06250828\n",
      "Step = 809 train_loss: 0.0077962684 val_loss: 0.061753273\n",
      "Step = 810 train_loss: 0.007777364 val_loss: 0.05627201\n",
      "Step = 811 train_loss: 0.004252651 val_loss: 0.032327984\n",
      "Step = 812 train_loss: 0.008457006 val_loss: 0.017200591\n",
      "Step = 813 train_loss: 0.0076639736 val_loss: 0.017356798\n",
      "Step = 814 train_loss: 0.005682572 val_loss: 0.018017747\n",
      "Step = 815 train_loss: 0.012565896 val_loss: 0.017410776\n",
      "Step = 816 train_loss: 0.010272875 val_loss: 0.016703492\n",
      "Step = 817 train_loss: 0.008051666 val_loss: 0.015953645\n",
      "Step = 818 train_loss: 0.009854114 val_loss: 0.05773817\n",
      "Step = 819 train_loss: 0.010252984 val_loss: 0.066809125\n",
      "Step = 820 train_loss: 0.009781374 val_loss: 0.0682309\n",
      "Step = 821 train_loss: 0.008160174 val_loss: 0.066328056\n",
      "Step = 822 train_loss: 0.01138614 val_loss: 0.063479625\n",
      "Step = 823 train_loss: 0.008192213 val_loss: 0.056742314\n",
      "Step = 824 train_loss: 0.008826492 val_loss: 0.013325377\n",
      "Step = 825 train_loss: 0.0034588529 val_loss: 0.018082872\n",
      "Step = 826 train_loss: 0.0065139127 val_loss: 0.018751884\n",
      "Step = 827 train_loss: 0.008774602 val_loss: 0.017973384\n",
      "Step = 828 train_loss: 0.010023177 val_loss: 0.016975597\n",
      "Step = 829 train_loss: 0.008451402 val_loss: 0.020818984\n",
      "Step = 830 train_loss: 0.00464474 val_loss: 0.05571346\n",
      "Step = 831 train_loss: 0.0054718647 val_loss: 0.061140466\n",
      "Step = 832 train_loss: 0.008041387 val_loss: 0.06115686\n",
      "Step = 833 train_loss: 0.010651397 val_loss: 0.031229677\n",
      "Step = 834 train_loss: 0.0052854074 val_loss: 0.014772198\n",
      "Step = 835 train_loss: 0.008336823 val_loss: 0.016083704\n",
      "Step = 836 train_loss: 0.017647268 val_loss: 0.015836028\n",
      "Step = 837 train_loss: 0.010899191 val_loss: 0.015127438\n",
      "Step = 838 train_loss: 0.009720205 val_loss: 0.015221263\n",
      "Step = 839 train_loss: 0.006968144 val_loss: 0.020432753\n",
      "Step = 840 train_loss: 0.008048407 val_loss: 0.05786728\n",
      "Step = 841 train_loss: 0.008079876 val_loss: 0.06342711\n",
      "Step = 842 train_loss: 0.009639393 val_loss: 0.066109434\n",
      "Step = 843 train_loss: 0.0046173465 val_loss: 0.06671701\n",
      "Step = 844 train_loss: 0.007411698 val_loss: 0.064510085\n",
      "Step = 845 train_loss: 0.008671449 val_loss: 0.05679448\n",
      "Step = 846 train_loss: 0.009638955 val_loss: 0.01768103\n",
      "Step = 847 train_loss: 0.0035219698 val_loss: 0.015773604\n",
      "Step = 848 train_loss: 0.009837343 val_loss: 0.016899563\n",
      "Step = 849 train_loss: 0.0078302 val_loss: 0.016393267\n",
      "Step = 850 train_loss: 0.00734065 val_loss: 0.0151664745\n",
      "Step = 851 train_loss: 0.0066216225 val_loss: 0.02965773\n",
      "Step = 852 train_loss: 0.009678593 val_loss: 0.046452604\n",
      "Step = 853 train_loss: 0.006639839 val_loss: 0.05615259\n",
      "Step = 854 train_loss: 0.0062979087 val_loss: 0.05989584\n",
      "Step = 855 train_loss: 0.0106537575 val_loss: 0.0566238\n",
      "Step = 856 train_loss: 0.004977068 val_loss: 0.04900723\n",
      "Step = 857 train_loss: 0.011758542 val_loss: 0.03274216\n",
      "Step = 858 train_loss: 0.0034017419 val_loss: 0.030271692\n",
      "Step = 859 train_loss: 0.003685827 val_loss: 0.038056508\n",
      "Step = 860 train_loss: 0.0066689206 val_loss: 0.054485198\n",
      "Step = 861 train_loss: 0.0062760194 val_loss: 0.054484844\n",
      "Step = 862 train_loss: 0.007489163 val_loss: 0.044519678\n",
      "Step = 863 train_loss: 0.008288984 val_loss: 0.030152159\n",
      "Step = 864 train_loss: 0.0076529426 val_loss: 0.017146192\n",
      "Step = 865 train_loss: 0.01098186 val_loss: 0.021204438\n",
      "Step = 866 train_loss: 0.009275748 val_loss: 0.032489125\n",
      "Step = 867 train_loss: 0.0052267136 val_loss: 0.04912213\n",
      "Step = 868 train_loss: 0.0122787515 val_loss: 0.053884037\n",
      "Step = 869 train_loss: 0.005738241 val_loss: 0.055964667\n",
      "Step = 870 train_loss: 0.0061261314 val_loss: 0.050623115\n",
      "Step = 871 train_loss: 0.0056597465 val_loss: 0.01897447\n",
      "Step = 872 train_loss: 0.006692594 val_loss: 0.012169521\n",
      "Step = 873 train_loss: 0.0046029203 val_loss: 0.012512726\n",
      "Step = 874 train_loss: 0.006259736 val_loss: 0.013646994\n",
      "Step = 875 train_loss: 0.0039837547 val_loss: 0.030111436\n",
      "Step = 876 train_loss: 0.008207608 val_loss: 0.047873206\n",
      "Step = 877 train_loss: 0.0065667136 val_loss: 0.04693722\n",
      "Step = 878 train_loss: 0.007336468 val_loss: 0.042922605\n",
      "Step = 879 train_loss: 0.0057087066 val_loss: 0.029481847\n",
      "Step = 880 train_loss: 0.0058881086 val_loss: 0.028928606\n",
      "Step = 881 train_loss: 0.0039668297 val_loss: 0.029721685\n",
      "Step = 882 train_loss: 0.011607162 val_loss: 0.03024779\n",
      "Step = 883 train_loss: 0.009979654 val_loss: 0.043128684\n",
      "Step = 884 train_loss: 0.004753697 val_loss: 0.053382467\n",
      "Step = 885 train_loss: 0.0047572576 val_loss: 0.05458324\n",
      "Step = 886 train_loss: 0.0051299874 val_loss: 0.053880487\n",
      "Step = 887 train_loss: 0.006229907 val_loss: 0.046089113\n",
      "Step = 888 train_loss: 0.0053648828 val_loss: 0.026409589\n",
      "Step = 889 train_loss: 0.007849405 val_loss: 0.01165619\n",
      "Step = 890 train_loss: 0.009423998 val_loss: 0.011691202\n",
      "Step = 891 train_loss: 0.00915737 val_loss: 0.013517984\n",
      "Step = 892 train_loss: 0.0067198477 val_loss: 0.038302682\n",
      "Step = 893 train_loss: 0.006390218 val_loss: 0.058172576\n",
      "Step = 894 train_loss: 0.0036547033 val_loss: 0.061593816\n",
      "Step = 895 train_loss: 0.0061976844 val_loss: 0.062791914\n",
      "Step = 896 train_loss: 0.005120705 val_loss: 0.059326638\n",
      "Step = 897 train_loss: 0.01697216 val_loss: 0.04618023\n",
      "Step = 898 train_loss: 0.008518296 val_loss: 0.011455185\n",
      "Step = 899 train_loss: 0.0057988716 val_loss: 0.018172042\n",
      "Step = 900 train_loss: 0.008841548 val_loss: 0.011271369\n",
      "Step = 901 train_loss: 0.0046383166 val_loss: 0.015774779\n",
      "Step = 902 train_loss: 0.008229813 val_loss: 0.033742394\n",
      "Step = 903 train_loss: 0.00415771 val_loss: 0.044107012\n",
      "Step = 904 train_loss: 0.0059585525 val_loss: 0.04043407\n",
      "Step = 905 train_loss: 0.007353761 val_loss: 0.021725837\n",
      "Step = 906 train_loss: 0.009666215 val_loss: 0.021247892\n",
      "Step = 907 train_loss: 0.0036141218 val_loss: 0.012968436\n",
      "Step = 908 train_loss: 0.004186447 val_loss: 0.010934529\n",
      "Step = 909 train_loss: 0.009653584 val_loss: 0.013423793\n",
      "Step = 910 train_loss: 0.009076037 val_loss: 0.043331735\n",
      "Step = 911 train_loss: 0.004584278 val_loss: 0.052277774\n",
      "Step = 912 train_loss: 0.0056826402 val_loss: 0.047454152\n",
      "Step = 913 train_loss: 0.007909882 val_loss: 0.027452713\n",
      "Step = 914 train_loss: 0.0058415355 val_loss: 0.013805451\n",
      "Step = 915 train_loss: 0.0061013103 val_loss: 0.010970129\n",
      "Step = 916 train_loss: 0.0036857214 val_loss: 0.018067852\n",
      "Step = 917 train_loss: 0.008088613 val_loss: 0.041569054\n",
      "Step = 918 train_loss: 0.0050442223 val_loss: 0.05508656\n",
      "Step = 919 train_loss: 0.0066922577 val_loss: 0.056240372\n",
      "Step = 920 train_loss: 0.01686692 val_loss: 0.052849904\n",
      "Step = 921 train_loss: 0.00503618 val_loss: 0.032831766\n",
      "Step = 922 train_loss: 0.004704681 val_loss: 0.01575833\n",
      "Step = 923 train_loss: 0.00441211 val_loss: 0.010826013\n",
      "Step = 924 train_loss: 0.0060178386 val_loss: 0.018737\n",
      "Step = 925 train_loss: 0.0052864156 val_loss: 0.023457944\n",
      "Step = 926 train_loss: 0.009023331 val_loss: 0.020671636\n",
      "Step = 927 train_loss: 0.008953383 val_loss: 0.01500222\n",
      "Step = 928 train_loss: 0.008203339 val_loss: 0.014554848\n",
      "Step = 929 train_loss: 0.00820696 val_loss: 0.03852731\n",
      "Step = 930 train_loss: 0.003122073 val_loss: 0.05594044\n",
      "Step = 931 train_loss: 0.003924715 val_loss: 0.05901373\n",
      "Step = 932 train_loss: 0.006583437 val_loss: 0.059466135\n",
      "Step = 933 train_loss: 0.0038462286 val_loss: 0.05803331\n",
      "Step = 934 train_loss: 0.0042895093 val_loss: 0.055299852\n",
      "Step = 935 train_loss: 0.0049694073 val_loss: 0.048927493\n",
      "Step = 936 train_loss: 0.01022995 val_loss: 0.022618923\n",
      "Step = 937 train_loss: 0.007208169 val_loss: 0.014669033\n",
      "Step = 938 train_loss: 0.009070314 val_loss: 0.015012712\n",
      "Step = 939 train_loss: 0.0054052835 val_loss: 0.015867474\n",
      "Step = 940 train_loss: 0.009194562 val_loss: 0.02092581\n",
      "Step = 941 train_loss: 0.0033189184 val_loss: 0.030152597\n",
      "Step = 942 train_loss: 0.010611681 val_loss: 0.048994\n",
      "Step = 943 train_loss: 0.0062568313 val_loss: 0.053445797\n",
      "Step = 944 train_loss: 0.0036793882 val_loss: 0.055543467\n",
      "Step = 945 train_loss: 0.004147068 val_loss: 0.055347607\n",
      "Step = 946 train_loss: 0.003769238 val_loss: 0.05564567\n",
      "Step = 947 train_loss: 0.011338886 val_loss: 0.035311535\n",
      "Step = 948 train_loss: 0.006755598 val_loss: 0.012386032\n",
      "Step = 949 train_loss: 0.0061759325 val_loss: 0.014118531\n",
      "Step = 950 train_loss: 0.013649513 val_loss: 0.013090756\n",
      "Step = 951 train_loss: 0.007956388 val_loss: 0.029024974\n",
      "Step = 952 train_loss: 0.0030330913 val_loss: 0.05444701\n",
      "Step = 953 train_loss: 0.006064764 val_loss: 0.059864897\n",
      "Step = 954 train_loss: 0.009574402 val_loss: 0.059644584\n",
      "Step = 955 train_loss: 0.008721633 val_loss: 0.056785926\n",
      "Step = 956 train_loss: 0.0040378226 val_loss: 0.05343567\n",
      "Step = 957 train_loss: 0.0054669073 val_loss: 0.058055762\n",
      "Step = 958 train_loss: 0.004305588 val_loss: 0.055964954\n",
      "Step = 959 train_loss: 0.012009705 val_loss: 0.037457157\n",
      "Step = 960 train_loss: 0.0095791975 val_loss: 0.009895119\n",
      "Step = 961 train_loss: 0.0038743988 val_loss: 0.012290558\n",
      "Step = 962 train_loss: 0.007206362 val_loss: 0.016282747\n",
      "Step = 963 train_loss: 0.006625759 val_loss: 0.012972215\n",
      "Step = 964 train_loss: 0.0058419923 val_loss: 0.01852312\n",
      "Step = 965 train_loss: 0.0068630073 val_loss: 0.043000292\n",
      "Step = 966 train_loss: 0.0036295827 val_loss: 0.05106988\n",
      "Step = 967 train_loss: 0.005259499 val_loss: 0.04711419\n",
      "Step = 968 train_loss: 0.008357846 val_loss: 0.03435757\n",
      "Step = 969 train_loss: 0.00348621 val_loss: 0.025517525\n",
      "Step = 970 train_loss: 0.0056987656 val_loss: 0.020606805\n",
      "Step = 971 train_loss: 0.0033675137 val_loss: 0.020111326\n",
      "Step = 972 train_loss: 0.0032487342 val_loss: 0.023776064\n",
      "Step = 973 train_loss: 0.011453288 val_loss: 0.03426524\n",
      "Step = 974 train_loss: 0.0036887308 val_loss: 0.039114993\n",
      "Step = 975 train_loss: 0.007395591 val_loss: 0.019328726\n",
      "Step = 976 train_loss: 0.003615761 val_loss: 0.012542354\n",
      "Step = 977 train_loss: 0.007539717 val_loss: 0.014195023\n",
      "Step = 978 train_loss: 0.009683394 val_loss: 0.015669594\n",
      "Step = 979 train_loss: 0.008341033 val_loss: 0.027787145\n",
      "Step = 980 train_loss: 0.005173805 val_loss: 0.050792538\n",
      "Step = 981 train_loss: 0.00869698 val_loss: 0.061274838\n",
      "Step = 982 train_loss: 0.01670353 val_loss: 0.060840886\n",
      "Step = 983 train_loss: 0.0076798503 val_loss: 0.05665921\n",
      "Step = 984 train_loss: 0.0027217255 val_loss: 0.05054022\n",
      "Step = 985 train_loss: 0.004030401 val_loss: 0.032506775\n",
      "Step = 986 train_loss: 0.006375283 val_loss: 0.013956143\n",
      "Step = 987 train_loss: 0.0061187353 val_loss: 0.013409938\n",
      "Step = 988 train_loss: 0.0068410933 val_loss: 0.015584617\n",
      "Step = 989 train_loss: 0.005576802 val_loss: 0.04026901\n",
      "Step = 990 train_loss: 0.0038211884 val_loss: 0.05090749\n",
      "Step = 991 train_loss: 0.0051907827 val_loss: 0.050787054\n",
      "Step = 992 train_loss: 0.011362368 val_loss: 0.038605843\n",
      "Step = 993 train_loss: 0.0037118993 val_loss: 0.022539668\n",
      "Step = 994 train_loss: 0.0045904364 val_loss: 0.014324116\n",
      "Step = 995 train_loss: 0.008107046 val_loss: 0.01408607\n",
      "Step = 996 train_loss: 0.0073141456 val_loss: 0.014318754\n",
      "Step = 997 train_loss: 0.008093936 val_loss: 0.022499265\n",
      "Step = 998 train_loss: 0.0024743835 val_loss: 0.048981037\n",
      "Step = 999 train_loss: 0.008886684 val_loss: 0.052359104\n",
      "585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_5412\\3159843208.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_5412\\3159843208.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "# summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "val_losses=[]\n",
    "\n",
    "for t in range(1000):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    '''save all the model in each epoch'''    \n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, 'Target_model/net_parameters'+str(t)+'.pkl')\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print ('Step = %d' % t, 'train_loss:',train_loss.data.numpy(),'val_loss:',val_loss.data.numpy())\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "   \n",
    "'''find the best model'''\n",
    "best_index=val_losses.index(np.min(val_losses))\n",
    "print(best_index)\n",
    "\n",
    "'''copy the best model from model file to the model_test file'''\n",
    "if 1:\n",
    "    shutil.copyfile('Target_model/net_parameters'+str(best_index)+'.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "'''Reload the best model''' \n",
    "model = Net()\n",
    "a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
    "optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.15125354\n",
      "mpe_val: 0.13912944\n",
      "mpe_a: 0.14603851902578857\n",
      "mpe_b: 0.1445131909573912\n",
      "rmse_train: 232.08122\n",
      "rmse_val: 192.28477\n",
      "rmse_a: 244.50795392969064\n",
      "rmse_b: 271.01512872900656\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.6286368370056152\n",
      "r2_val: 0.6448044180870056\n",
      "r2_a: 0.6082580908824828\n",
      "r2_b: 0.09643709934666189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
