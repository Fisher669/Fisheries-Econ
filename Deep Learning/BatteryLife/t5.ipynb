{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型效果\n",
    "## 改用deep-wide模型架构\n",
    "### 三层卷积再分架构\n",
    "r2_train: 0.6515334844589233\n",
    "r2_val: 0.6068358421325684\n",
    "r2_a: 0.6229705199002005\n",
    "r2_b: 0.4175009753657175\n",
    "## 对比（非lstm架构）\n",
    "r2_train: 0.9947811961174011\n",
    "r2_val: 0.9749422669410706\n",
    "r2_a: 0.9464100454957816\n",
    "r2_b: 0.6836000724611211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "'''Define the CNN model''' \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 公用的前3层卷积层\n",
    "        self.shared_cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(8, 16, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Deep部分 - 完整卷积层\n",
    "        self.deep_cnn = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),  # 增加池化层\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(32, 64, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "        )\n",
    "\n",
    "        # Wide部分 - 包含公用层的卷积层\n",
    "        self.wide_cnn = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Conv2d(8, 16, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "        )\n",
    "        self.wide_lstm = torch.nn.LSTM(input_size=32, hidden_size=32, batch_first=True)\n",
    "\n",
    "        # 融合后的全连接层\n",
    "        self.fc1 = torch.nn.Linear(4832, 1000)  # Deep 和 Wide 部分的输出拼接\n",
    "        self.drop_layer = torch.nn.Dropout(p=0.2)\n",
    "        self.fc2 = torch.nn.Linear(1000, 100)\n",
    "        self.fc3 = torch.nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            # 先计算公用卷积层\n",
    "            shared_out = self.shared_cnn(x)\n",
    "\n",
    "            # Deep部分 (CNN)\n",
    "            deep_out = self.deep_cnn(shared_out)\n",
    "            deep_out = deep_out.view(deep_out.size(0), -1)  # 展平为 (batch_size, flatten_size)\n",
    "\n",
    "            # Wide部分 (LSTM)\n",
    "            wide_input = shared_out.view(shared_out.size(0), -1, 32)  # 调整为 (batch_size, sequence_length, input_size)\n",
    "            wide_out, _ = self.wide_lstm(wide_input)\n",
    "            wide_out = wide_out[:, -1, :]  # 获取 LSTM 的最后一个时间步输出\n",
    "\n",
    "            # 拼接 Deep 和 Wide 部分的输出\n",
    "            combined = torch.cat((deep_out, wide_out), dim=1)  # 在最后一个维度拼接\n",
    "\n",
    "            # 全连接层\n",
    "            x = torch.relu(self.fc1(combined))\n",
    "            x = self.drop_layer(x)\n",
    "            x = self.fc2(x)\n",
    "            x = torch.sigmoid(self.fc3(x))\n",
    "            return x\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during forward pass: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (shared_cnn): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  )\n",
      "  (deep_cnn): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): Conv2d(32, 64, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (wide_cnn): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (1): Dropout(p=0.2, inplace=False)\n",
      "    (2): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (5): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (wide_lstm): LSTM(32, 32, batch_first=True)\n",
      "  (fc1): Linear(in_features=4832, out_features=1000, bias=True)\n",
      "  (drop_layer): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (fc3): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "Step = 0 train_loss: 0.07627173 val_loss: 0.060444072\n",
      "Step = 1 train_loss: 0.056454536 val_loss: 0.059706572\n",
      "Step = 2 train_loss: 0.045987524 val_loss: 0.05876672\n",
      "Step = 3 train_loss: 0.03730243 val_loss: 0.057538997\n",
      "Step = 4 train_loss: 0.030139226 val_loss: 0.05608165\n",
      "Step = 5 train_loss: 0.030076005 val_loss: 0.05448064\n",
      "Step = 6 train_loss: 0.030510573 val_loss: 0.05280788\n",
      "Step = 7 train_loss: 0.031395856 val_loss: 0.051154435\n",
      "Step = 8 train_loss: 0.034579553 val_loss: 0.049496464\n",
      "Step = 9 train_loss: 0.031428594 val_loss: 0.04793609\n",
      "Step = 10 train_loss: 0.036104623 val_loss: 0.04648271\n",
      "Step = 11 train_loss: 0.03699052 val_loss: 0.045230873\n",
      "Step = 12 train_loss: 0.033684365 val_loss: 0.044184968\n",
      "Step = 13 train_loss: 0.03836662 val_loss: 0.0433709\n",
      "Step = 14 train_loss: 0.034185007 val_loss: 0.042712647\n",
      "Step = 15 train_loss: 0.03242213 val_loss: 0.042195547\n",
      "Step = 16 train_loss: 0.03194603 val_loss: 0.0417706\n",
      "Step = 17 train_loss: 0.03073589 val_loss: 0.04153707\n",
      "Step = 18 train_loss: 0.033250213 val_loss: 0.041356437\n",
      "Step = 19 train_loss: 0.03091045 val_loss: 0.04131894\n",
      "Step = 20 train_loss: 0.03195304 val_loss: 0.04131055\n",
      "Step = 21 train_loss: 0.026554793 val_loss: 0.04137281\n",
      "Step = 22 train_loss: 0.02958859 val_loss: 0.041379552\n",
      "Step = 23 train_loss: 0.025215289 val_loss: 0.041408755\n",
      "Step = 24 train_loss: 0.031573847 val_loss: 0.04126388\n",
      "Step = 25 train_loss: 0.028328588 val_loss: 0.041018683\n",
      "Step = 26 train_loss: 0.029616734 val_loss: 0.040693253\n",
      "Step = 27 train_loss: 0.03170529 val_loss: 0.040285967\n",
      "Step = 28 train_loss: 0.027686777 val_loss: 0.039916497\n",
      "Step = 29 train_loss: 0.031759262 val_loss: 0.039509006\n",
      "Step = 30 train_loss: 0.02986394 val_loss: 0.039103203\n",
      "Step = 31 train_loss: 0.030792637 val_loss: 0.038680136\n",
      "Step = 32 train_loss: 0.027581781 val_loss: 0.038304303\n",
      "Step = 33 train_loss: 0.029477863 val_loss: 0.037926264\n",
      "Step = 34 train_loss: 0.027862538 val_loss: 0.037447184\n",
      "Step = 35 train_loss: 0.026666565 val_loss: 0.037087157\n",
      "Step = 36 train_loss: 0.029867662 val_loss: 0.036778178\n",
      "Step = 37 train_loss: 0.023466479 val_loss: 0.036535226\n",
      "Step = 38 train_loss: 0.023020755 val_loss: 0.036305897\n",
      "Step = 39 train_loss: 0.02839412 val_loss: 0.036122877\n",
      "Step = 40 train_loss: 0.028879782 val_loss: 0.035989724\n",
      "Step = 41 train_loss: 0.029387893 val_loss: 0.0358916\n",
      "Step = 42 train_loss: 0.026666041 val_loss: 0.035835776\n",
      "Step = 43 train_loss: 0.027632307 val_loss: 0.03583402\n",
      "Step = 44 train_loss: 0.026380803 val_loss: 0.035843946\n",
      "Step = 45 train_loss: 0.023753276 val_loss: 0.035877336\n",
      "Step = 46 train_loss: 0.02686747 val_loss: 0.035792816\n",
      "Step = 47 train_loss: 0.025432719 val_loss: 0.03569629\n",
      "Step = 48 train_loss: 0.0262946 val_loss: 0.035539467\n",
      "Step = 49 train_loss: 0.02654237 val_loss: 0.0356029\n",
      "Step = 50 train_loss: 0.025576584 val_loss: 0.035604402\n",
      "Step = 51 train_loss: 0.027814683 val_loss: 0.03577732\n",
      "Step = 52 train_loss: 0.026324807 val_loss: 0.0358318\n",
      "Step = 53 train_loss: 0.026893433 val_loss: 0.03614926\n",
      "Step = 54 train_loss: 0.021511002 val_loss: 0.036687978\n",
      "Step = 55 train_loss: 0.023873312 val_loss: 0.037170433\n",
      "Step = 56 train_loss: 0.02295932 val_loss: 0.037631027\n",
      "Step = 57 train_loss: 0.02676778 val_loss: 0.03806046\n",
      "Step = 58 train_loss: 0.025521137 val_loss: 0.03866674\n",
      "Step = 59 train_loss: 0.023513757 val_loss: 0.038849127\n",
      "Step = 60 train_loss: 0.02439826 val_loss: 0.038999096\n",
      "Step = 61 train_loss: 0.025552867 val_loss: 0.03996218\n",
      "Step = 62 train_loss: 0.02494491 val_loss: 0.04151462\n",
      "Step = 63 train_loss: 0.021959351 val_loss: 0.043188456\n",
      "Step = 64 train_loss: 0.026182238 val_loss: 0.046381507\n",
      "Step = 65 train_loss: 0.022774657 val_loss: 0.049314883\n",
      "Step = 66 train_loss: 0.024061944 val_loss: 0.050928388\n",
      "Step = 67 train_loss: 0.026067752 val_loss: 0.052385576\n",
      "Step = 68 train_loss: 0.021947814 val_loss: 0.052084602\n",
      "Step = 69 train_loss: 0.021913959 val_loss: 0.046875697\n",
      "Step = 70 train_loss: 0.019812468 val_loss: 0.04575277\n",
      "Step = 71 train_loss: 0.021976475 val_loss: 0.04632871\n",
      "Step = 72 train_loss: 0.025613973 val_loss: 0.049660042\n",
      "Step = 73 train_loss: 0.022312293 val_loss: 0.05465177\n",
      "Step = 74 train_loss: 0.020023925 val_loss: 0.05921677\n",
      "Step = 75 train_loss: 0.01820802 val_loss: 0.060830906\n",
      "Step = 76 train_loss: 0.02334365 val_loss: 0.061457414\n",
      "Step = 77 train_loss: 0.01965199 val_loss: 0.06057598\n",
      "Step = 78 train_loss: 0.020970594 val_loss: 0.058282033\n",
      "Step = 79 train_loss: 0.018044505 val_loss: 0.0551806\n",
      "Step = 80 train_loss: 0.019213745 val_loss: 0.050765928\n",
      "Step = 81 train_loss: 0.020916015 val_loss: 0.0541695\n",
      "Step = 82 train_loss: 0.020568307 val_loss: 0.05851949\n",
      "Step = 83 train_loss: 0.020927785 val_loss: 0.0581752\n",
      "Step = 84 train_loss: 0.016046513 val_loss: 0.05749457\n",
      "Step = 85 train_loss: 0.020013146 val_loss: 0.056165673\n",
      "Step = 86 train_loss: 0.01866611 val_loss: 0.05413249\n",
      "Step = 87 train_loss: 0.018643783 val_loss: 0.051781904\n",
      "Step = 88 train_loss: 0.018575782 val_loss: 0.050033465\n",
      "Step = 89 train_loss: 0.02299546 val_loss: 0.049588464\n",
      "Step = 90 train_loss: 0.01890898 val_loss: 0.049409036\n",
      "Step = 91 train_loss: 0.01724024 val_loss: 0.046861682\n",
      "Step = 92 train_loss: 0.017331457 val_loss: 0.04315551\n",
      "Step = 93 train_loss: 0.017197436 val_loss: 0.041069947\n",
      "Step = 94 train_loss: 0.018089332 val_loss: 0.042232804\n",
      "Step = 95 train_loss: 0.015594123 val_loss: 0.044720765\n",
      "Step = 96 train_loss: 0.017473416 val_loss: 0.04625875\n",
      "Step = 97 train_loss: 0.019377725 val_loss: 0.04131282\n",
      "Step = 98 train_loss: 0.016173743 val_loss: 0.041208446\n",
      "Step = 99 train_loss: 0.019889887 val_loss: 0.043816146\n",
      "Step = 100 train_loss: 0.016275985 val_loss: 0.051763266\n",
      "Step = 101 train_loss: 0.012784267 val_loss: 0.052200485\n",
      "Step = 102 train_loss: 0.015581808 val_loss: 0.054204985\n",
      "Step = 103 train_loss: 0.01246878 val_loss: 0.054733057\n",
      "Step = 104 train_loss: 0.015107175 val_loss: 0.05242136\n",
      "Step = 105 train_loss: 0.012980429 val_loss: 0.047133036\n",
      "Step = 106 train_loss: 0.012909453 val_loss: 0.035605047\n",
      "Step = 107 train_loss: 0.014667166 val_loss: 0.03687112\n",
      "Step = 108 train_loss: 0.014914834 val_loss: 0.03545702\n",
      "Step = 109 train_loss: 0.013075171 val_loss: 0.043130126\n",
      "Step = 110 train_loss: 0.016244667 val_loss: 0.049622234\n",
      "Step = 111 train_loss: 0.013698498 val_loss: 0.03570123\n",
      "Step = 112 train_loss: 0.014636362 val_loss: 0.030686872\n",
      "Step = 113 train_loss: 0.014125242 val_loss: 0.045941014\n",
      "Step = 114 train_loss: 0.0152768 val_loss: 0.07448603\n",
      "Step = 115 train_loss: 0.01645312 val_loss: 0.07231237\n",
      "Step = 116 train_loss: 0.01646291 val_loss: 0.062230606\n",
      "Step = 117 train_loss: 0.017728679 val_loss: 0.02412702\n",
      "Step = 118 train_loss: 0.013566843 val_loss: 0.018658211\n",
      "Step = 119 train_loss: 0.015524342 val_loss: 0.0354584\n",
      "Step = 120 train_loss: 0.012606844 val_loss: 0.04330293\n",
      "Step = 121 train_loss: 0.016912447 val_loss: 0.033165198\n",
      "Step = 122 train_loss: 0.014491999 val_loss: 0.027554\n",
      "Step = 123 train_loss: 0.013196196 val_loss: 0.03276366\n",
      "Step = 124 train_loss: 0.01438794 val_loss: 0.03138445\n",
      "Step = 125 train_loss: 0.015136901 val_loss: 0.03369825\n",
      "Step = 126 train_loss: 0.0118955765 val_loss: 0.034186907\n",
      "Step = 127 train_loss: 0.014553999 val_loss: 0.060666073\n",
      "Step = 128 train_loss: 0.0131918155 val_loss: 0.061714638\n",
      "Step = 129 train_loss: 0.014989943 val_loss: 0.044726525\n",
      "Step = 130 train_loss: 0.012282879 val_loss: 0.030009927\n",
      "Step = 131 train_loss: 0.010520488 val_loss: 0.022962\n",
      "Step = 132 train_loss: 0.012046365 val_loss: 0.030949349\n",
      "Step = 133 train_loss: 0.010803671 val_loss: 0.02758152\n",
      "Step = 134 train_loss: 0.010837609 val_loss: 0.02896098\n",
      "Step = 135 train_loss: 0.010015888 val_loss: 0.018381622\n",
      "Step = 136 train_loss: 0.014257489 val_loss: 0.033179395\n",
      "Step = 137 train_loss: 0.013376142 val_loss: 0.056288637\n",
      "Step = 138 train_loss: 0.012754002 val_loss: 0.036781184\n",
      "Step = 139 train_loss: 0.01581439 val_loss: 0.017159319\n",
      "Step = 140 train_loss: 0.010705238 val_loss: 0.012653108\n",
      "Step = 141 train_loss: 0.011913041 val_loss: 0.012566535\n",
      "Step = 142 train_loss: 0.009852324 val_loss: 0.021475285\n",
      "Step = 143 train_loss: 0.011203389 val_loss: 0.0138794705\n",
      "Step = 144 train_loss: 0.012288294 val_loss: 0.011870614\n",
      "Step = 145 train_loss: 0.01175167 val_loss: 0.023722403\n",
      "Step = 146 train_loss: 0.013760337 val_loss: 0.03072396\n",
      "Step = 147 train_loss: 0.008619197 val_loss: 0.013137826\n",
      "Step = 148 train_loss: 0.009106193 val_loss: 0.0129529685\n",
      "Step = 149 train_loss: 0.011725037 val_loss: 0.07090281\n",
      "Step = 150 train_loss: 0.012197252 val_loss: 0.08915399\n",
      "Step = 151 train_loss: 0.011074503 val_loss: 0.03948529\n",
      "Step = 152 train_loss: 0.011200203 val_loss: 0.014918333\n",
      "Step = 153 train_loss: 0.015531481 val_loss: 0.014831105\n",
      "Step = 154 train_loss: 0.01123706 val_loss: 0.019303031\n",
      "Step = 155 train_loss: 0.013833361 val_loss: 0.10690681\n",
      "Step = 156 train_loss: 0.013552831 val_loss: 0.11339861\n",
      "Step = 157 train_loss: 0.012743049 val_loss: 0.1040956\n",
      "Step = 158 train_loss: 0.014071509 val_loss: 0.09612959\n",
      "Step = 159 train_loss: 0.014144926 val_loss: 0.08655593\n",
      "Step = 160 train_loss: 0.013792412 val_loss: 0.039371192\n",
      "Step = 161 train_loss: 0.013320656 val_loss: 0.014136477\n",
      "Step = 162 train_loss: 0.01666934 val_loss: 0.015063203\n",
      "Step = 163 train_loss: 0.013434268 val_loss: 0.012785088\n",
      "Step = 164 train_loss: 0.009987267 val_loss: 0.017381646\n",
      "Step = 165 train_loss: 0.012607881 val_loss: 0.02470073\n",
      "Step = 166 train_loss: 0.012354127 val_loss: 0.035115194\n",
      "Step = 167 train_loss: 0.009381561 val_loss: 0.02633768\n",
      "Step = 168 train_loss: 0.01052705 val_loss: 0.0124670165\n",
      "Step = 169 train_loss: 0.010976095 val_loss: 0.01646495\n",
      "Step = 170 train_loss: 0.009246831 val_loss: 0.012906496\n",
      "Step = 171 train_loss: 0.013258135 val_loss: 0.022517903\n",
      "Step = 172 train_loss: 0.014396114 val_loss: 0.06651061\n",
      "Step = 173 train_loss: 0.012367185 val_loss: 0.07260225\n",
      "Step = 174 train_loss: 0.011912419 val_loss: 0.035960935\n",
      "Step = 175 train_loss: 0.01453901 val_loss: 0.009979733\n",
      "Step = 176 train_loss: 0.014341743 val_loss: 0.016514601\n",
      "Step = 177 train_loss: 0.009147619 val_loss: 0.07585562\n",
      "Step = 178 train_loss: 0.009995743 val_loss: 0.093615815\n",
      "Step = 179 train_loss: 0.009966938 val_loss: 0.111558005\n",
      "Step = 180 train_loss: 0.008778906 val_loss: 0.07084789\n",
      "Step = 181 train_loss: 0.008602901 val_loss: 0.02419069\n",
      "Step = 182 train_loss: 0.008038763 val_loss: 0.040302377\n",
      "Step = 183 train_loss: 0.008988283 val_loss: 0.11330737\n",
      "Step = 184 train_loss: 0.006575838 val_loss: 0.13660531\n",
      "Step = 185 train_loss: 0.009972389 val_loss: 0.10550937\n",
      "Step = 186 train_loss: 0.009644011 val_loss: 0.0392055\n",
      "Step = 187 train_loss: 0.009915959 val_loss: 0.01446555\n",
      "Step = 188 train_loss: 0.0107510965 val_loss: 0.021839263\n",
      "Step = 189 train_loss: 0.007419752 val_loss: 0.060864728\n",
      "Step = 190 train_loss: 0.007626842 val_loss: 0.08737667\n",
      "Step = 191 train_loss: 0.008977733 val_loss: 0.10888452\n",
      "Step = 192 train_loss: 0.012039315 val_loss: 0.15186208\n",
      "Step = 193 train_loss: 0.012030073 val_loss: 0.18179306\n",
      "Step = 194 train_loss: 0.011260389 val_loss: 0.1798602\n",
      "Step = 195 train_loss: 0.010978667 val_loss: 0.17423049\n",
      "Step = 196 train_loss: 0.010481598 val_loss: 0.1372971\n",
      "Step = 197 train_loss: 0.007835986 val_loss: 0.0751345\n",
      "Step = 198 train_loss: 0.010572556 val_loss: 0.019054094\n",
      "Step = 199 train_loss: 0.007980279 val_loss: 0.009863074\n",
      "Step = 200 train_loss: 0.010044387 val_loss: 0.01623695\n",
      "Step = 201 train_loss: 0.007980594 val_loss: 0.022172311\n",
      "Step = 202 train_loss: 0.0094522415 val_loss: 0.018698437\n",
      "Step = 203 train_loss: 0.008691793 val_loss: 0.014498293\n",
      "Step = 204 train_loss: 0.008592343 val_loss: 0.014677664\n",
      "Step = 205 train_loss: 0.00862499 val_loss: 0.015333398\n",
      "Step = 206 train_loss: 0.009232605 val_loss: 0.014934501\n",
      "Step = 207 train_loss: 0.00713008 val_loss: 0.018673118\n",
      "Step = 208 train_loss: 0.009639463 val_loss: 0.050165355\n",
      "Step = 209 train_loss: 0.010400312 val_loss: 0.057271436\n",
      "Step = 210 train_loss: 0.009956628 val_loss: 0.04370534\n",
      "Step = 211 train_loss: 0.010143359 val_loss: 0.022387287\n",
      "Step = 212 train_loss: 0.008392628 val_loss: 0.016767263\n",
      "Step = 213 train_loss: 0.010026851 val_loss: 0.01619523\n",
      "Step = 214 train_loss: 0.008341236 val_loss: 0.015252009\n",
      "Step = 215 train_loss: 0.011331146 val_loss: 0.019129803\n",
      "Step = 216 train_loss: 0.007892574 val_loss: 0.024338339\n",
      "Step = 217 train_loss: 0.0064630355 val_loss: 0.028147953\n",
      "Step = 218 train_loss: 0.008928208 val_loss: 0.02969832\n",
      "Step = 219 train_loss: 0.00963119 val_loss: 0.032400507\n",
      "Step = 220 train_loss: 0.008853463 val_loss: 0.03268872\n",
      "Step = 221 train_loss: 0.01004749 val_loss: 0.06346042\n",
      "Step = 222 train_loss: 0.0097569665 val_loss: 0.106594585\n",
      "Step = 223 train_loss: 0.009180711 val_loss: 0.15646648\n",
      "Step = 224 train_loss: 0.011792278 val_loss: 0.21595685\n",
      "Step = 225 train_loss: 0.013287872 val_loss: 0.2633776\n",
      "Step = 226 train_loss: 0.0075914552 val_loss: 0.2953334\n",
      "Step = 227 train_loss: 0.010465799 val_loss: 0.29219064\n",
      "Step = 228 train_loss: 0.009665987 val_loss: 0.24887952\n",
      "Step = 229 train_loss: 0.010374173 val_loss: 0.15242583\n",
      "Step = 230 train_loss: 0.008259118 val_loss: 0.052141376\n",
      "Step = 231 train_loss: 0.011284668 val_loss: 0.041522242\n",
      "Step = 232 train_loss: 0.008497767 val_loss: 0.05325561\n",
      "Step = 233 train_loss: 0.009454738 val_loss: 0.052393574\n",
      "Step = 234 train_loss: 0.012682062 val_loss: 0.0421644\n",
      "Step = 235 train_loss: 0.011105115 val_loss: 0.02586925\n",
      "Step = 236 train_loss: 0.008878012 val_loss: 0.02074006\n",
      "Step = 237 train_loss: 0.011854506 val_loss: 0.020324962\n",
      "Step = 238 train_loss: 0.008472146 val_loss: 0.022667572\n",
      "Step = 239 train_loss: 0.007358994 val_loss: 0.03691059\n",
      "Step = 240 train_loss: 0.008082887 val_loss: 0.044449784\n",
      "Step = 241 train_loss: 0.0076521942 val_loss: 0.045493133\n",
      "Step = 242 train_loss: 0.01147858 val_loss: 0.03508474\n",
      "Step = 243 train_loss: 0.009988153 val_loss: 0.018455781\n",
      "Step = 244 train_loss: 0.008240737 val_loss: 0.012870267\n",
      "Step = 245 train_loss: 0.00702598 val_loss: 0.016942803\n",
      "Step = 246 train_loss: 0.007881238 val_loss: 0.017585315\n",
      "Step = 247 train_loss: 0.008545265 val_loss: 0.017194351\n",
      "Step = 248 train_loss: 0.0078996895 val_loss: 0.01581795\n",
      "Step = 249 train_loss: 0.011188632 val_loss: 0.013879241\n",
      "Step = 250 train_loss: 0.009525594 val_loss: 0.024607021\n",
      "Step = 251 train_loss: 0.010598986 val_loss: 0.0378594\n",
      "Step = 252 train_loss: 0.008045218 val_loss: 0.038993105\n",
      "Step = 253 train_loss: 0.009174674 val_loss: 0.02616482\n",
      "Step = 254 train_loss: 0.010581766 val_loss: 0.011903357\n",
      "Step = 255 train_loss: 0.00813842 val_loss: 0.01417428\n",
      "Step = 256 train_loss: 0.009798648 val_loss: 0.014529286\n",
      "Step = 257 train_loss: 0.008432599 val_loss: 0.012816325\n",
      "Step = 258 train_loss: 0.0074400376 val_loss: 0.02690456\n",
      "Step = 259 train_loss: 0.0070544314 val_loss: 0.042110465\n",
      "Step = 260 train_loss: 0.005966151 val_loss: 0.029531764\n",
      "Step = 261 train_loss: 0.008767437 val_loss: 0.013346382\n",
      "Step = 262 train_loss: 0.008286734 val_loss: 0.0141246915\n",
      "Step = 263 train_loss: 0.0073053073 val_loss: 0.013402859\n",
      "Step = 264 train_loss: 0.0068553397 val_loss: 0.012618927\n",
      "Step = 265 train_loss: 0.008317336 val_loss: 0.019282887\n",
      "Step = 266 train_loss: 0.007208706 val_loss: 0.0184591\n",
      "Step = 267 train_loss: 0.00606979 val_loss: 0.012442391\n",
      "Step = 268 train_loss: 0.0062243585 val_loss: 0.012323144\n",
      "Step = 269 train_loss: 0.0071214642 val_loss: 0.014383197\n",
      "Step = 270 train_loss: 0.010278502 val_loss: 0.019856941\n",
      "Step = 271 train_loss: 0.006526945 val_loss: 0.028136628\n",
      "Step = 272 train_loss: 0.0061941985 val_loss: 0.020085707\n",
      "Step = 273 train_loss: 0.00865369 val_loss: 0.012927958\n",
      "Step = 274 train_loss: 0.0071035367 val_loss: 0.014194326\n",
      "Step = 275 train_loss: 0.0046878983 val_loss: 0.020147316\n",
      "Step = 276 train_loss: 0.0060030394 val_loss: 0.022350036\n",
      "Step = 277 train_loss: 0.008523507 val_loss: 0.01789486\n",
      "Step = 278 train_loss: 0.006051244 val_loss: 0.018316481\n",
      "Step = 279 train_loss: 0.0053672595 val_loss: 0.019703794\n",
      "Step = 280 train_loss: 0.007914441 val_loss: 0.0210765\n",
      "Step = 281 train_loss: 0.006992044 val_loss: 0.019716242\n",
      "Step = 282 train_loss: 0.0065542315 val_loss: 0.020451725\n",
      "Step = 283 train_loss: 0.007177813 val_loss: 0.018563189\n",
      "Step = 284 train_loss: 0.0065756286 val_loss: 0.019821256\n",
      "Step = 285 train_loss: 0.0070339153 val_loss: 0.026889214\n",
      "Step = 286 train_loss: 0.0059141796 val_loss: 0.02312584\n",
      "Step = 287 train_loss: 0.008733222 val_loss: 0.023872094\n",
      "Step = 288 train_loss: 0.005472355 val_loss: 0.019578556\n",
      "Step = 289 train_loss: 0.008513423 val_loss: 0.018852077\n",
      "Step = 290 train_loss: 0.009672717 val_loss: 0.017939366\n",
      "Step = 291 train_loss: 0.008063748 val_loss: 0.017947542\n",
      "Step = 292 train_loss: 0.0054187956 val_loss: 0.017735776\n",
      "Step = 293 train_loss: 0.0063019074 val_loss: 0.015750179\n",
      "Step = 294 train_loss: 0.007840488 val_loss: 0.014964101\n",
      "Step = 295 train_loss: 0.0064033708 val_loss: 0.0123497015\n",
      "Step = 296 train_loss: 0.009124521 val_loss: 0.020824287\n",
      "Step = 297 train_loss: 0.006314138 val_loss: 0.107709326\n",
      "Step = 298 train_loss: 0.008750144 val_loss: 0.12175709\n",
      "Step = 299 train_loss: 0.0061099054 val_loss: 0.041694865\n",
      "Step = 300 train_loss: 0.006344251 val_loss: 0.018501338\n",
      "Step = 301 train_loss: 0.006253007 val_loss: 0.012022913\n",
      "Step = 302 train_loss: 0.007132738 val_loss: 0.028748283\n",
      "Step = 303 train_loss: 0.006300547 val_loss: 0.14150856\n",
      "Step = 304 train_loss: 0.006236668 val_loss: 0.15496059\n",
      "Step = 305 train_loss: 0.006083844 val_loss: 0.08970614\n",
      "Step = 306 train_loss: 0.0056153825 val_loss: 0.013109505\n",
      "Step = 307 train_loss: 0.007924692 val_loss: 0.021294875\n",
      "Step = 308 train_loss: 0.004743915 val_loss: 0.044533107\n",
      "Step = 309 train_loss: 0.0051258076 val_loss: 0.048441384\n",
      "Step = 310 train_loss: 0.006566053 val_loss: 0.034765646\n",
      "Step = 311 train_loss: 0.0043833107 val_loss: 0.018012142\n",
      "Step = 312 train_loss: 0.0056416076 val_loss: 0.013770251\n",
      "Step = 313 train_loss: 0.0083447825 val_loss: 0.06486269\n",
      "Step = 314 train_loss: 0.0066876085 val_loss: 0.10827037\n",
      "Step = 315 train_loss: 0.0065106335 val_loss: 0.05088383\n",
      "Step = 316 train_loss: 0.0063098366 val_loss: 0.043778453\n",
      "Step = 317 train_loss: 0.005123824 val_loss: 0.026620548\n",
      "Step = 318 train_loss: 0.006313813 val_loss: 0.0212213\n",
      "Step = 319 train_loss: 0.006057804 val_loss: 0.050934955\n",
      "Step = 320 train_loss: 0.0057025133 val_loss: 0.08393834\n",
      "Step = 321 train_loss: 0.008678367 val_loss: 0.105850905\n",
      "Step = 322 train_loss: 0.0057811607 val_loss: 0.037925534\n",
      "Step = 323 train_loss: 0.0072732237 val_loss: 0.009848471\n",
      "Step = 324 train_loss: 0.007714088 val_loss: 0.009498176\n",
      "Step = 325 train_loss: 0.005659841 val_loss: 0.041289963\n",
      "Step = 326 train_loss: 0.006587304 val_loss: 0.057087667\n",
      "Step = 327 train_loss: 0.0090203695 val_loss: 0.020823445\n",
      "Step = 328 train_loss: 0.008620632 val_loss: 0.012000826\n",
      "Step = 329 train_loss: 0.005564335 val_loss: 0.0133890435\n",
      "Step = 330 train_loss: 0.0083581135 val_loss: 0.030548265\n",
      "Step = 331 train_loss: 0.005774825 val_loss: 0.0850858\n",
      "Step = 332 train_loss: 0.0062331846 val_loss: 0.07767775\n",
      "Step = 333 train_loss: 0.006162147 val_loss: 0.029709531\n",
      "Step = 334 train_loss: 0.006842142 val_loss: 0.011167809\n",
      "Step = 335 train_loss: 0.00598383 val_loss: 0.0117988475\n",
      "Step = 336 train_loss: 0.006442102 val_loss: 0.029785706\n",
      "Step = 337 train_loss: 0.005255406 val_loss: 0.03495563\n",
      "Step = 338 train_loss: 0.008131739 val_loss: 0.0331757\n",
      "Step = 339 train_loss: 0.006026355 val_loss: 0.013586285\n",
      "Step = 340 train_loss: 0.0051060272 val_loss: 0.011694808\n",
      "Step = 341 train_loss: 0.0051881913 val_loss: 0.011973183\n",
      "Step = 342 train_loss: 0.0057892385 val_loss: 0.011436903\n",
      "Step = 343 train_loss: 0.0071824407 val_loss: 0.010996323\n",
      "Step = 344 train_loss: 0.0097513 val_loss: 0.017732054\n",
      "Step = 345 train_loss: 0.005322822 val_loss: 0.035391204\n",
      "Step = 346 train_loss: 0.006063978 val_loss: 0.02369491\n",
      "Step = 347 train_loss: 0.0065682596 val_loss: 0.011121372\n",
      "Step = 348 train_loss: 0.0043611373 val_loss: 0.010812155\n",
      "Step = 349 train_loss: 0.0051689516 val_loss: 0.009819006\n",
      "Step = 350 train_loss: 0.005104129 val_loss: 0.025307707\n",
      "Step = 351 train_loss: 0.0039376738 val_loss: 0.035140637\n",
      "Step = 352 train_loss: 0.007079967 val_loss: 0.013017695\n",
      "Step = 353 train_loss: 0.007499284 val_loss: 0.009086306\n",
      "Step = 354 train_loss: 0.0072045643 val_loss: 0.008766143\n",
      "Step = 355 train_loss: 0.0047180783 val_loss: 0.017150441\n",
      "Step = 356 train_loss: 0.005556629 val_loss: 0.029610971\n",
      "Step = 357 train_loss: 0.0043244283 val_loss: 0.03562403\n",
      "Step = 358 train_loss: 0.004480823 val_loss: 0.014211339\n",
      "Step = 359 train_loss: 0.00540508 val_loss: 0.0087421695\n",
      "Step = 360 train_loss: 0.006538355 val_loss: 0.008675784\n",
      "Step = 361 train_loss: 0.008165477 val_loss: 0.020631475\n",
      "Step = 362 train_loss: 0.0050622444 val_loss: 0.0646032\n",
      "Step = 363 train_loss: 0.005060446 val_loss: 0.032008074\n",
      "Step = 364 train_loss: 0.0044328608 val_loss: 0.008768498\n",
      "Step = 365 train_loss: 0.0067210016 val_loss: 0.0109797055\n",
      "Step = 366 train_loss: 0.0056890096 val_loss: 0.008898784\n",
      "Step = 367 train_loss: 0.004748568 val_loss: 0.010724189\n",
      "Step = 368 train_loss: 0.0055862637 val_loss: 0.024944672\n",
      "Step = 369 train_loss: 0.005916252 val_loss: 0.014539838\n",
      "Step = 370 train_loss: 0.0051740655 val_loss: 0.010625783\n",
      "Step = 371 train_loss: 0.0068804664 val_loss: 0.011692592\n",
      "Step = 372 train_loss: 0.0046108896 val_loss: 0.009313067\n",
      "Step = 373 train_loss: 0.0063159093 val_loss: 0.0153335435\n",
      "Step = 374 train_loss: 0.005535027 val_loss: 0.013922015\n",
      "Step = 375 train_loss: 0.0046494324 val_loss: 0.014610687\n",
      "Step = 376 train_loss: 0.0040183766 val_loss: 0.009796976\n",
      "Step = 377 train_loss: 0.006386431 val_loss: 0.01013307\n",
      "Step = 378 train_loss: 0.00680348 val_loss: 0.015270818\n",
      "Step = 379 train_loss: 0.0069490303 val_loss: 0.07755497\n",
      "Step = 380 train_loss: 0.0059547997 val_loss: 0.08017337\n",
      "Step = 381 train_loss: 0.0084682 val_loss: 0.015936397\n",
      "Step = 382 train_loss: 0.0050161225 val_loss: 0.009034382\n",
      "Step = 383 train_loss: 0.007913138 val_loss: 0.008540003\n",
      "Step = 384 train_loss: 0.0050548855 val_loss: 0.008881848\n",
      "Step = 385 train_loss: 0.005766299 val_loss: 0.010321506\n",
      "Step = 386 train_loss: 0.0040500695 val_loss: 0.009276647\n",
      "Step = 387 train_loss: 0.0054674777 val_loss: 0.012062475\n",
      "Step = 388 train_loss: 0.005257249 val_loss: 0.0090379855\n",
      "Step = 389 train_loss: 0.004331232 val_loss: 0.035591543\n",
      "Step = 390 train_loss: 0.005490921 val_loss: 0.10451947\n",
      "Step = 391 train_loss: 0.005705125 val_loss: 0.08388957\n",
      "Step = 392 train_loss: 0.007867316 val_loss: 0.028844554\n",
      "Step = 393 train_loss: 0.005529442 val_loss: 0.014443107\n",
      "Step = 394 train_loss: 0.0043178694 val_loss: 0.016965881\n",
      "Step = 395 train_loss: 0.008221293 val_loss: 0.012779876\n",
      "Step = 396 train_loss: 0.0071223127 val_loss: 0.037095066\n",
      "Step = 397 train_loss: 0.0058004186 val_loss: 0.06634065\n",
      "Step = 398 train_loss: 0.0052150018 val_loss: 0.042755887\n",
      "Step = 399 train_loss: 0.005967507 val_loss: 0.00986714\n",
      "Step = 400 train_loss: 0.0052746367 val_loss: 0.01696465\n",
      "Step = 401 train_loss: 0.0063824737 val_loss: 0.015277482\n",
      "Step = 402 train_loss: 0.008082664 val_loss: 0.016055612\n",
      "Step = 403 train_loss: 0.0045956364 val_loss: 0.054457154\n",
      "Step = 404 train_loss: 0.005574945 val_loss: 0.06514851\n",
      "Step = 405 train_loss: 0.005431347 val_loss: 0.03602169\n",
      "Step = 406 train_loss: 0.006966955 val_loss: 0.011942668\n",
      "Step = 407 train_loss: 0.0058306926 val_loss: 0.013655793\n",
      "Step = 408 train_loss: 0.0052024433 val_loss: 0.022761257\n",
      "Step = 409 train_loss: 0.0060710665 val_loss: 0.04078532\n",
      "Step = 410 train_loss: 0.005728734 val_loss: 0.008205687\n",
      "Step = 411 train_loss: 0.0068148356 val_loss: 0.008612213\n",
      "Step = 412 train_loss: 0.0050647566 val_loss: 0.011971518\n",
      "Step = 413 train_loss: 0.0051316093 val_loss: 0.009797941\n",
      "Step = 414 train_loss: 0.005086248 val_loss: 0.01066316\n",
      "Step = 415 train_loss: 0.0040904833 val_loss: 0.02986091\n",
      "Step = 416 train_loss: 0.006894837 val_loss: 0.024712116\n",
      "Step = 417 train_loss: 0.005182761 val_loss: 0.009451516\n",
      "Step = 418 train_loss: 0.006136327 val_loss: 0.008292501\n",
      "Step = 419 train_loss: 0.0058279233 val_loss: 0.011594931\n",
      "Step = 420 train_loss: 0.00538737 val_loss: 0.019365437\n",
      "Step = 421 train_loss: 0.0058414782 val_loss: 0.011651358\n",
      "Step = 422 train_loss: 0.007376642 val_loss: 0.008798519\n",
      "Step = 423 train_loss: 0.0072297934 val_loss: 0.008608582\n",
      "Step = 424 train_loss: 0.0051950123 val_loss: 0.009925165\n",
      "Step = 425 train_loss: 0.0050453963 val_loss: 0.009240678\n",
      "Step = 426 train_loss: 0.0062980456 val_loss: 0.01126772\n",
      "Step = 427 train_loss: 0.0050657643 val_loss: 0.009008455\n",
      "Step = 428 train_loss: 0.0045403596 val_loss: 0.011133395\n",
      "Step = 429 train_loss: 0.0061023203 val_loss: 0.008792859\n",
      "Step = 430 train_loss: 0.0031969862 val_loss: 0.048629064\n",
      "Step = 431 train_loss: 0.0045096604 val_loss: 0.05473506\n",
      "Step = 432 train_loss: 0.006541672 val_loss: 0.008687325\n",
      "Step = 433 train_loss: 0.00518766 val_loss: 0.015560402\n",
      "Step = 434 train_loss: 0.0056786817 val_loss: 0.012856025\n",
      "Step = 435 train_loss: 0.005219132 val_loss: 0.0101695005\n",
      "Step = 436 train_loss: 0.0043978165 val_loss: 0.018098246\n",
      "Step = 437 train_loss: 0.0061782678 val_loss: 0.0180564\n",
      "Step = 438 train_loss: 0.0053461585 val_loss: 0.00836555\n",
      "Step = 439 train_loss: 0.005346971 val_loss: 0.008962718\n",
      "Step = 440 train_loss: 0.00424201 val_loss: 0.011846192\n",
      "Step = 441 train_loss: 0.0055487854 val_loss: 0.028097397\n",
      "Step = 442 train_loss: 0.0052182814 val_loss: 0.018529106\n",
      "Step = 443 train_loss: 0.0070183775 val_loss: 0.009185578\n",
      "Step = 444 train_loss: 0.0051264935 val_loss: 0.015169174\n",
      "Step = 445 train_loss: 0.0061774594 val_loss: 0.014050704\n",
      "Step = 446 train_loss: 0.0075712153 val_loss: 0.01814362\n",
      "Step = 447 train_loss: 0.0059760488 val_loss: 0.06746063\n",
      "Step = 448 train_loss: 0.005287488 val_loss: 0.05011125\n",
      "Step = 449 train_loss: 0.005609585 val_loss: 0.009271815\n",
      "Step = 450 train_loss: 0.005056043 val_loss: 0.016545871\n",
      "Step = 451 train_loss: 0.005567926 val_loss: 0.012296783\n",
      "Step = 452 train_loss: 0.006541367 val_loss: 0.111107245\n",
      "Step = 453 train_loss: 0.0041659316 val_loss: 0.16243577\n",
      "Step = 454 train_loss: 0.005072302 val_loss: 0.17582107\n",
      "Step = 455 train_loss: 0.008205673 val_loss: 0.170436\n",
      "Step = 456 train_loss: 0.0060351994 val_loss: 0.1343765\n",
      "Step = 457 train_loss: 0.00772625 val_loss: 0.021283375\n",
      "Step = 458 train_loss: 0.0057406556 val_loss: 0.018919973\n",
      "Step = 459 train_loss: 0.006205784 val_loss: 0.019108938\n",
      "Step = 460 train_loss: 0.0072334735 val_loss: 0.01916385\n",
      "Step = 461 train_loss: 0.0066113826 val_loss: 0.016494265\n",
      "Step = 462 train_loss: 0.0035371021 val_loss: 0.013937088\n",
      "Step = 463 train_loss: 0.005282321 val_loss: 0.040248126\n",
      "Step = 464 train_loss: 0.007397826 val_loss: 0.009566819\n",
      "Step = 465 train_loss: 0.007495938 val_loss: 0.015870031\n",
      "Step = 466 train_loss: 0.0060570533 val_loss: 0.013818355\n",
      "Step = 467 train_loss: 0.006618133 val_loss: 0.23180921\n",
      "Step = 468 train_loss: 0.0046886364 val_loss: 0.4483033\n",
      "Step = 469 train_loss: 0.005830817 val_loss: 0.48786014\n",
      "Step = 470 train_loss: 0.0045758937 val_loss: 0.49240717\n",
      "Step = 471 train_loss: 0.0072972286 val_loss: 0.48143223\n",
      "Step = 472 train_loss: 0.0054038805 val_loss: 0.45883444\n",
      "Step = 473 train_loss: 0.0064215427 val_loss: 0.43832356\n",
      "Step = 474 train_loss: 0.0056267586 val_loss: 0.42534056\n",
      "Step = 475 train_loss: 0.006133604 val_loss: 0.41469228\n",
      "Step = 476 train_loss: 0.0053674923 val_loss: 0.40852988\n",
      "Step = 477 train_loss: 0.0041173343 val_loss: 0.38182655\n",
      "Step = 478 train_loss: 0.008248446 val_loss: 0.33007935\n",
      "Step = 479 train_loss: 0.0066276467 val_loss: 0.19203986\n",
      "Step = 480 train_loss: 0.0040961853 val_loss: 0.071160756\n",
      "Step = 481 train_loss: 0.0056430167 val_loss: 0.053072196\n",
      "Step = 482 train_loss: 0.004225297 val_loss: 0.06278516\n",
      "Step = 483 train_loss: 0.005609796 val_loss: 0.11131904\n",
      "Step = 484 train_loss: 0.0054629836 val_loss: 0.14835182\n",
      "Step = 485 train_loss: 0.0053888485 val_loss: 0.1359024\n",
      "Step = 486 train_loss: 0.0042868285 val_loss: 0.10779827\n",
      "Step = 487 train_loss: 0.003966095 val_loss: 0.08064467\n",
      "Step = 488 train_loss: 0.0048548155 val_loss: 0.040564764\n",
      "Step = 489 train_loss: 0.005383565 val_loss: 0.032192532\n",
      "Step = 490 train_loss: 0.0060372893 val_loss: 0.043322887\n",
      "Step = 491 train_loss: 0.004843886 val_loss: 0.043606237\n",
      "Step = 492 train_loss: 0.004139076 val_loss: 0.039772622\n",
      "Step = 493 train_loss: 0.0044786204 val_loss: 0.03035011\n",
      "Step = 494 train_loss: 0.006031521 val_loss: 0.021307392\n",
      "Step = 495 train_loss: 0.0059429226 val_loss: 0.026492497\n",
      "Step = 496 train_loss: 0.004454662 val_loss: 0.037016395\n",
      "Step = 497 train_loss: 0.0062006023 val_loss: 0.022305448\n",
      "Step = 498 train_loss: 0.006401744 val_loss: 0.021286352\n",
      "Step = 499 train_loss: 0.0045222356 val_loss: 0.024534183\n",
      "410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_21572\\607722968.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_21572\\607722968.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "# summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "val_losses=[]\n",
    "\n",
    "for t in range(500):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    '''save all the model in each epoch'''    \n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, 'Target_model/net_parameters'+str(t)+'.pkl')\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print ('Step = %d' % t, 'train_loss:',train_loss.data.numpy(),'val_loss:',val_loss.data.numpy())\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "   \n",
    "'''find the best model'''\n",
    "best_index=val_losses.index(np.min(val_losses))\n",
    "print(best_index)\n",
    "\n",
    "'''copy the best model from model file to the model_test file'''\n",
    "if 1:\n",
    "    shutil.copyfile('Target_model/net_parameters'+str(best_index)+'.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "'''Reload the best model''' \n",
    "model = Net()\n",
    "a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
    "optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.32235593\n",
      "mpe_val: 0.29555854\n",
      "mpe_a: 0.35575631379538586\n",
      "mpe_b: 0.1757375292729978\n",
      "rmse_train: 224.81284\n",
      "rmse_val: 202.30103\n",
      "rmse_a: 239.87259215074215\n",
      "rmse_b: 217.6015280277232\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.6515334844589233\n",
      "r2_val: 0.6068358421325684\n",
      "r2_a: 0.6229705199002005\n",
      "r2_b: 0.4175009753657175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
