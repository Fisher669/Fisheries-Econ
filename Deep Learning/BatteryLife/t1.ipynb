{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型效果\n",
    "\n",
    "r2_train: 0.9062628746032715\n",
    "r2_val: 0.8613404631614685\n",
    "r2_a: 0.9024388593844427\n",
    "r2_b: 0.5164209698966113\n",
    "\n",
    "对比（非lstm架构）\n",
    "r2_train: 0.9947811961174011\n",
    "r2_val: 0.9749422669410706\n",
    "r2_a: 0.9464100454957816\n",
    "r2_b: 0.6836000724611211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 修改模型需要改层数，回传部分，以及lstm部分\n",
    "'''Define the CNN model''' \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(8, 16, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(32, 64, kernel_size=(2, 2), stride=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(1, 2), stride=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "        )\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(64, 32, batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(7616, 100)\n",
    "        self.drop_layer6 = torch.nn.Dropout(p=0.2)\n",
    "        self.fc2 = torch.nn.Linear(100, 1)   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            x = self.layers(x) \n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.drop_layer6(x)                     \n",
    "            x = torch.sigmoid(self.fc2(x))        \n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during forward pass: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (12): Dropout(p=0.2, inplace=False)\n",
      "    (13): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (16): Dropout(p=0.2, inplace=False)\n",
      "    (17): Conv2d(64, 64, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (20): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(64, 32, batch_first=True)\n",
      "  (fc1): Linear(in_features=7616, out_features=100, bias=True)\n",
      "  (drop_layer6): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 11, 24]          20,008\n",
      "       BatchNorm2d-2            [-1, 8, 11, 24]              16\n",
      "         LeakyReLU-3            [-1, 8, 11, 24]               0\n",
      "         MaxPool2d-4            [-1, 8, 10, 23]               0\n",
      "           Dropout-5            [-1, 8, 10, 23]               0\n",
      "            Conv2d-6            [-1, 16, 9, 21]             784\n",
      "       BatchNorm2d-7            [-1, 16, 9, 21]              32\n",
      "         LeakyReLU-8            [-1, 16, 9, 21]               0\n",
      "           Dropout-9            [-1, 16, 9, 21]               0\n",
      "           Conv2d-10            [-1, 32, 8, 19]           3,104\n",
      "      BatchNorm2d-11            [-1, 32, 8, 19]              64\n",
      "        LeakyReLU-12            [-1, 32, 8, 19]               0\n",
      "          Dropout-13            [-1, 32, 8, 19]               0\n",
      "           Conv2d-14            [-1, 64, 7, 18]           8,256\n",
      "      BatchNorm2d-15            [-1, 64, 7, 18]             128\n",
      "        LeakyReLU-16            [-1, 64, 7, 18]               0\n",
      "          Dropout-17            [-1, 64, 7, 18]               0\n",
      "           Conv2d-18            [-1, 64, 7, 17]           8,256\n",
      "      BatchNorm2d-19            [-1, 64, 7, 17]             128\n",
      "        LeakyReLU-20            [-1, 64, 7, 17]               0\n",
      "          Dropout-21            [-1, 64, 7, 17]               0\n",
      "           Linear-22                  [-1, 100]         761,700\n",
      "          Dropout-23                  [-1, 100]               0\n",
      "           Linear-24                    [-1, 1]             101\n",
      "================================================================\n",
      "Total params: 802,577\n",
      "Trainable params: 802,577\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 0.80\n",
      "Params size (MB): 3.06\n",
      "Estimated Total Size (MB): 4.24\n",
      "----------------------------------------------------------------\n",
      "Step = 0 train_loss: 0.052072957 val_loss: 0.060188532\n",
      "Step = 1 train_loss: 0.047396008 val_loss: 0.059702404\n",
      "Step = 2 train_loss: 0.039703067 val_loss: 0.058724437\n",
      "Step = 3 train_loss: 0.039677575 val_loss: 0.05744201\n",
      "Step = 4 train_loss: 0.047464583 val_loss: 0.056076005\n",
      "Step = 5 train_loss: 0.040904187 val_loss: 0.054621987\n",
      "Step = 6 train_loss: 0.035548735 val_loss: 0.053116\n",
      "Step = 7 train_loss: 0.03328612 val_loss: 0.051628288\n",
      "Step = 8 train_loss: 0.038068093 val_loss: 0.050128926\n",
      "Step = 9 train_loss: 0.035160743 val_loss: 0.048572894\n",
      "Step = 10 train_loss: 0.033289496 val_loss: 0.047072317\n",
      "Step = 11 train_loss: 0.03299452 val_loss: 0.04561217\n",
      "Step = 12 train_loss: 0.031945508 val_loss: 0.04414718\n",
      "Step = 13 train_loss: 0.033004317 val_loss: 0.04279134\n",
      "Step = 14 train_loss: 0.031790726 val_loss: 0.04149771\n",
      "Step = 15 train_loss: 0.028701069 val_loss: 0.040271837\n",
      "Step = 16 train_loss: 0.031876065 val_loss: 0.039034598\n",
      "Step = 17 train_loss: 0.028389912 val_loss: 0.037930172\n",
      "Step = 18 train_loss: 0.030281454 val_loss: 0.036846463\n",
      "Step = 19 train_loss: 0.030336041 val_loss: 0.03581912\n",
      "Step = 20 train_loss: 0.024455396 val_loss: 0.034869585\n",
      "Step = 21 train_loss: 0.033220217 val_loss: 0.033972416\n",
      "Step = 22 train_loss: 0.029598556 val_loss: 0.033209793\n",
      "Step = 23 train_loss: 0.031270348 val_loss: 0.03249112\n",
      "Step = 24 train_loss: 0.03457263 val_loss: 0.031818975\n",
      "Step = 25 train_loss: 0.02968768 val_loss: 0.031207452\n",
      "Step = 26 train_loss: 0.027721431 val_loss: 0.03062554\n",
      "Step = 27 train_loss: 0.027572863 val_loss: 0.030153245\n",
      "Step = 28 train_loss: 0.03296389 val_loss: 0.029695561\n",
      "Step = 29 train_loss: 0.03012049 val_loss: 0.029388154\n",
      "Step = 30 train_loss: 0.030744754 val_loss: 0.029124172\n",
      "Step = 31 train_loss: 0.02921657 val_loss: 0.028997822\n",
      "Step = 32 train_loss: 0.02632827 val_loss: 0.028834876\n",
      "Step = 33 train_loss: 0.029917149 val_loss: 0.028692534\n",
      "Step = 34 train_loss: 0.033571146 val_loss: 0.028563788\n",
      "Step = 35 train_loss: 0.029136341 val_loss: 0.028402526\n",
      "Step = 36 train_loss: 0.033650216 val_loss: 0.028277751\n",
      "Step = 37 train_loss: 0.03030381 val_loss: 0.028206069\n",
      "Step = 38 train_loss: 0.029034238 val_loss: 0.028233072\n",
      "Step = 39 train_loss: 0.031039862 val_loss: 0.028251627\n",
      "Step = 40 train_loss: 0.029814482 val_loss: 0.02831739\n",
      "Step = 41 train_loss: 0.035719935 val_loss: 0.028367281\n",
      "Step = 42 train_loss: 0.02844377 val_loss: 0.028487226\n",
      "Step = 43 train_loss: 0.028650314 val_loss: 0.0286223\n",
      "Step = 44 train_loss: 0.027351119 val_loss: 0.028702425\n",
      "Step = 45 train_loss: 0.029105755 val_loss: 0.028801087\n",
      "Step = 46 train_loss: 0.030322976 val_loss: 0.02895487\n",
      "Step = 47 train_loss: 0.030149601 val_loss: 0.029023478\n",
      "Step = 48 train_loss: 0.029503267 val_loss: 0.02900515\n",
      "Step = 49 train_loss: 0.030579295 val_loss: 0.028958745\n",
      "Step = 50 train_loss: 0.031702843 val_loss: 0.028830366\n",
      "Step = 51 train_loss: 0.028258821 val_loss: 0.028582292\n",
      "Step = 52 train_loss: 0.034447566 val_loss: 0.028203841\n",
      "Step = 53 train_loss: 0.028376332 val_loss: 0.027761642\n",
      "Step = 54 train_loss: 0.027222987 val_loss: 0.027461037\n",
      "Step = 55 train_loss: 0.0283237 val_loss: 0.027459389\n",
      "Step = 56 train_loss: 0.03401571 val_loss: 0.027610058\n",
      "Step = 57 train_loss: 0.034526944 val_loss: 0.027506677\n",
      "Step = 58 train_loss: 0.02915436 val_loss: 0.0273715\n",
      "Step = 59 train_loss: 0.029446784 val_loss: 0.027257599\n",
      "Step = 60 train_loss: 0.027581591 val_loss: 0.027124269\n",
      "Step = 61 train_loss: 0.025991715 val_loss: 0.027062684\n",
      "Step = 62 train_loss: 0.028465059 val_loss: 0.02697341\n",
      "Step = 63 train_loss: 0.027571915 val_loss: 0.026850365\n",
      "Step = 64 train_loss: 0.036010362 val_loss: 0.026897632\n",
      "Step = 65 train_loss: 0.030634608 val_loss: 0.027297296\n",
      "Step = 66 train_loss: 0.029870905 val_loss: 0.027651168\n",
      "Step = 67 train_loss: 0.0316511 val_loss: 0.027804008\n",
      "Step = 68 train_loss: 0.033689514 val_loss: 0.027601402\n",
      "Step = 69 train_loss: 0.027611107 val_loss: 0.027430173\n",
      "Step = 70 train_loss: 0.032862976 val_loss: 0.027067754\n",
      "Step = 71 train_loss: 0.03088832 val_loss: 0.02673735\n",
      "Step = 72 train_loss: 0.027767789 val_loss: 0.026297439\n",
      "Step = 73 train_loss: 0.0283371 val_loss: 0.026097897\n",
      "Step = 74 train_loss: 0.026712269 val_loss: 0.025607618\n",
      "Step = 75 train_loss: 0.026954057 val_loss: 0.0247959\n",
      "Step = 76 train_loss: 0.030094948 val_loss: 0.024306431\n",
      "Step = 77 train_loss: 0.029113851 val_loss: 0.024023876\n",
      "Step = 78 train_loss: 0.023877548 val_loss: 0.023629153\n",
      "Step = 79 train_loss: 0.028676331 val_loss: 0.023476746\n",
      "Step = 80 train_loss: 0.026774654 val_loss: 0.023977973\n",
      "Step = 81 train_loss: 0.030938528 val_loss: 0.023719592\n",
      "Step = 82 train_loss: 0.030020965 val_loss: 0.024865756\n",
      "Step = 83 train_loss: 0.025747815 val_loss: 0.025411813\n",
      "Step = 84 train_loss: 0.02920396 val_loss: 0.025221264\n",
      "Step = 85 train_loss: 0.024016662 val_loss: 0.025870942\n",
      "Step = 86 train_loss: 0.027550949 val_loss: 0.026371462\n",
      "Step = 87 train_loss: 0.02907914 val_loss: 0.026553785\n",
      "Step = 88 train_loss: 0.028926251 val_loss: 0.027189415\n",
      "Step = 89 train_loss: 0.02899286 val_loss: 0.027659718\n",
      "Step = 90 train_loss: 0.02981054 val_loss: 0.028175427\n",
      "Step = 91 train_loss: 0.03173577 val_loss: 0.028095812\n",
      "Step = 92 train_loss: 0.02690158 val_loss: 0.028009616\n",
      "Step = 93 train_loss: 0.028828548 val_loss: 0.027613511\n",
      "Step = 94 train_loss: 0.026780682 val_loss: 0.027507363\n",
      "Step = 95 train_loss: 0.024166089 val_loss: 0.027384892\n",
      "Step = 96 train_loss: 0.028908141 val_loss: 0.026332466\n",
      "Step = 97 train_loss: 0.022663875 val_loss: 0.025124613\n",
      "Step = 98 train_loss: 0.026434628 val_loss: 0.025887428\n",
      "Step = 99 train_loss: 0.025375087 val_loss: 0.02617949\n",
      "Step = 100 train_loss: 0.029253652 val_loss: 0.026073596\n",
      "Step = 101 train_loss: 0.025874348 val_loss: 0.025868282\n",
      "Step = 102 train_loss: 0.025407711 val_loss: 0.025839288\n",
      "Step = 103 train_loss: 0.023909401 val_loss: 0.02589727\n",
      "Step = 104 train_loss: 0.026272409 val_loss: 0.026572363\n",
      "Step = 105 train_loss: 0.027211942 val_loss: 0.026698705\n",
      "Step = 106 train_loss: 0.025959102 val_loss: 0.02713886\n",
      "Step = 107 train_loss: 0.025887363 val_loss: 0.027117087\n",
      "Step = 108 train_loss: 0.025226118 val_loss: 0.026726592\n",
      "Step = 109 train_loss: 0.029791124 val_loss: 0.026288157\n",
      "Step = 110 train_loss: 0.027765922 val_loss: 0.02744771\n",
      "Step = 111 train_loss: 0.025819473 val_loss: 0.031073011\n",
      "Step = 112 train_loss: 0.030908803 val_loss: 0.032883253\n",
      "Step = 113 train_loss: 0.029263262 val_loss: 0.03517464\n",
      "Step = 114 train_loss: 0.028312203 val_loss: 0.037206724\n",
      "Step = 115 train_loss: 0.027363172 val_loss: 0.03768387\n",
      "Step = 116 train_loss: 0.028071046 val_loss: 0.036930844\n",
      "Step = 117 train_loss: 0.025834681 val_loss: 0.036011927\n",
      "Step = 118 train_loss: 0.025498278 val_loss: 0.035106115\n",
      "Step = 119 train_loss: 0.026041122 val_loss: 0.032726415\n",
      "Step = 120 train_loss: 0.024567233 val_loss: 0.027080884\n",
      "Step = 121 train_loss: 0.024826974 val_loss: 0.024926087\n",
      "Step = 122 train_loss: 0.02833843 val_loss: 0.020908326\n",
      "Step = 123 train_loss: 0.030622782 val_loss: 0.018257594\n",
      "Step = 124 train_loss: 0.025925111 val_loss: 0.018394453\n",
      "Step = 125 train_loss: 0.026525624 val_loss: 0.018690126\n",
      "Step = 126 train_loss: 0.027290342 val_loss: 0.018487874\n",
      "Step = 127 train_loss: 0.025526453 val_loss: 0.01856771\n",
      "Step = 128 train_loss: 0.027961763 val_loss: 0.01913553\n",
      "Step = 129 train_loss: 0.029342284 val_loss: 0.020164857\n",
      "Step = 130 train_loss: 0.02609247 val_loss: 0.021868244\n",
      "Step = 131 train_loss: 0.022850147 val_loss: 0.023531605\n",
      "Step = 132 train_loss: 0.028192546 val_loss: 0.024737222\n",
      "Step = 133 train_loss: 0.026857628 val_loss: 0.02296953\n",
      "Step = 134 train_loss: 0.027517147 val_loss: 0.019506304\n",
      "Step = 135 train_loss: 0.028071214 val_loss: 0.01894041\n",
      "Step = 136 train_loss: 0.025991969 val_loss: 0.019717498\n",
      "Step = 137 train_loss: 0.022609618 val_loss: 0.022270888\n",
      "Step = 138 train_loss: 0.026557058 val_loss: 0.025891261\n",
      "Step = 139 train_loss: 0.02600952 val_loss: 0.02494603\n",
      "Step = 140 train_loss: 0.02897247 val_loss: 0.026057431\n",
      "Step = 141 train_loss: 0.026987433 val_loss: 0.026311485\n",
      "Step = 142 train_loss: 0.026622977 val_loss: 0.027123898\n",
      "Step = 143 train_loss: 0.031106256 val_loss: 0.029824961\n",
      "Step = 144 train_loss: 0.03418722 val_loss: 0.033106506\n",
      "Step = 145 train_loss: 0.027795615 val_loss: 0.03611654\n",
      "Step = 146 train_loss: 0.025405856 val_loss: 0.036758073\n",
      "Step = 147 train_loss: 0.023427036 val_loss: 0.036625315\n",
      "Step = 148 train_loss: 0.024474112 val_loss: 0.036448434\n",
      "Step = 149 train_loss: 0.025501985 val_loss: 0.0365066\n",
      "Step = 150 train_loss: 0.02481066 val_loss: 0.036688477\n",
      "Step = 151 train_loss: 0.027024766 val_loss: 0.036825232\n",
      "Step = 152 train_loss: 0.025381211 val_loss: 0.036679063\n",
      "Step = 153 train_loss: 0.026278697 val_loss: 0.036298826\n",
      "Step = 154 train_loss: 0.023089798 val_loss: 0.035982564\n",
      "Step = 155 train_loss: 0.02861227 val_loss: 0.03572482\n",
      "Step = 156 train_loss: 0.024007805 val_loss: 0.035529982\n",
      "Step = 157 train_loss: 0.025070727 val_loss: 0.034777798\n",
      "Step = 158 train_loss: 0.025383232 val_loss: 0.03071077\n",
      "Step = 159 train_loss: 0.024548765 val_loss: 0.03267965\n",
      "Step = 160 train_loss: 0.022291478 val_loss: 0.03341494\n",
      "Step = 161 train_loss: 0.02139475 val_loss: 0.03992238\n",
      "Step = 162 train_loss: 0.020762088 val_loss: 0.04454358\n",
      "Step = 163 train_loss: 0.023444235 val_loss: 0.04591318\n",
      "Step = 164 train_loss: 0.022216104 val_loss: 0.046048608\n",
      "Step = 165 train_loss: 0.025242977 val_loss: 0.04568489\n",
      "Step = 166 train_loss: 0.024478901 val_loss: 0.0444956\n",
      "Step = 167 train_loss: 0.023703098 val_loss: 0.041985117\n",
      "Step = 168 train_loss: 0.024174185 val_loss: 0.030714035\n",
      "Step = 169 train_loss: 0.026162086 val_loss: 0.030886406\n",
      "Step = 170 train_loss: 0.023444192 val_loss: 0.031582683\n",
      "Step = 171 train_loss: 0.024680387 val_loss: 0.03911244\n",
      "Step = 172 train_loss: 0.02553108 val_loss: 0.04219942\n",
      "Step = 173 train_loss: 0.022911657 val_loss: 0.036258984\n",
      "Step = 174 train_loss: 0.023471626 val_loss: 0.027152166\n",
      "Step = 175 train_loss: 0.024645446 val_loss: 0.02009945\n",
      "Step = 176 train_loss: 0.021201363 val_loss: 0.023029583\n",
      "Step = 177 train_loss: 0.019498996 val_loss: 0.03732506\n",
      "Step = 178 train_loss: 0.02289763 val_loss: 0.03863826\n",
      "Step = 179 train_loss: 0.025871236 val_loss: 0.039168417\n",
      "Step = 180 train_loss: 0.022744037 val_loss: 0.03768915\n",
      "Step = 181 train_loss: 0.022674145 val_loss: 0.02833754\n",
      "Step = 182 train_loss: 0.020098481 val_loss: 0.019503059\n",
      "Step = 183 train_loss: 0.022639943 val_loss: 0.017633948\n",
      "Step = 184 train_loss: 0.022608148 val_loss: 0.017547961\n",
      "Step = 185 train_loss: 0.025990415 val_loss: 0.022173395\n",
      "Step = 186 train_loss: 0.02108841 val_loss: 0.036466584\n",
      "Step = 187 train_loss: 0.022724638 val_loss: 0.03961867\n",
      "Step = 188 train_loss: 0.02193279 val_loss: 0.03855159\n",
      "Step = 189 train_loss: 0.021793677 val_loss: 0.034528717\n",
      "Step = 190 train_loss: 0.023496468 val_loss: 0.017440489\n",
      "Step = 191 train_loss: 0.026428033 val_loss: 0.016894113\n",
      "Step = 192 train_loss: 0.02107206 val_loss: 0.017034\n",
      "Step = 193 train_loss: 0.020174412 val_loss: 0.030062074\n",
      "Step = 194 train_loss: 0.022132672 val_loss: 0.049025077\n",
      "Step = 195 train_loss: 0.025302289 val_loss: 0.051600084\n",
      "Step = 196 train_loss: 0.023682127 val_loss: 0.049385965\n",
      "Step = 197 train_loss: 0.022180738 val_loss: 0.046341587\n",
      "Step = 198 train_loss: 0.019516377 val_loss: 0.043301325\n",
      "Step = 199 train_loss: 0.020609988 val_loss: 0.0371481\n",
      "Step = 200 train_loss: 0.021067847 val_loss: 0.032595716\n",
      "Step = 201 train_loss: 0.020443926 val_loss: 0.028427232\n",
      "Step = 202 train_loss: 0.017532345 val_loss: 0.034274027\n",
      "Step = 203 train_loss: 0.020934733 val_loss: 0.042206068\n",
      "Step = 204 train_loss: 0.01761909 val_loss: 0.04863885\n",
      "Step = 205 train_loss: 0.022397308 val_loss: 0.044095993\n",
      "Step = 206 train_loss: 0.021161364 val_loss: 0.03523366\n",
      "Step = 207 train_loss: 0.02110308 val_loss: 0.023595402\n",
      "Step = 208 train_loss: 0.017431099 val_loss: 0.021999475\n",
      "Step = 209 train_loss: 0.02068644 val_loss: 0.029813088\n",
      "Step = 210 train_loss: 0.019937273 val_loss: 0.041322086\n",
      "Step = 211 train_loss: 0.0185422 val_loss: 0.0449922\n",
      "Step = 212 train_loss: 0.019288305 val_loss: 0.037180193\n",
      "Step = 213 train_loss: 0.019506713 val_loss: 0.018380882\n",
      "Step = 214 train_loss: 0.018967833 val_loss: 0.016695848\n",
      "Step = 215 train_loss: 0.017581593 val_loss: 0.02297405\n",
      "Step = 216 train_loss: 0.01684324 val_loss: 0.040797304\n",
      "Step = 217 train_loss: 0.020933406 val_loss: 0.04529278\n",
      "Step = 218 train_loss: 0.017786914 val_loss: 0.038542185\n",
      "Step = 219 train_loss: 0.016422125 val_loss: 0.029398115\n",
      "Step = 220 train_loss: 0.017382557 val_loss: 0.027485501\n",
      "Step = 221 train_loss: 0.018485483 val_loss: 0.020156413\n",
      "Step = 222 train_loss: 0.018323572 val_loss: 0.01640552\n",
      "Step = 223 train_loss: 0.019953279 val_loss: 0.015135376\n",
      "Step = 224 train_loss: 0.017184885 val_loss: 0.014472963\n",
      "Step = 225 train_loss: 0.016516047 val_loss: 0.012161121\n",
      "Step = 226 train_loss: 0.018581852 val_loss: 0.013274546\n",
      "Step = 227 train_loss: 0.021297803 val_loss: 0.012411216\n",
      "Step = 228 train_loss: 0.014236573 val_loss: 0.012809388\n",
      "Step = 229 train_loss: 0.016832517 val_loss: 0.012292403\n",
      "Step = 230 train_loss: 0.018167116 val_loss: 0.014200376\n",
      "Step = 231 train_loss: 0.019009456 val_loss: 0.028743409\n",
      "Step = 232 train_loss: 0.016795803 val_loss: 0.028337505\n",
      "Step = 233 train_loss: 0.016396146 val_loss: 0.012712567\n",
      "Step = 234 train_loss: 0.01631558 val_loss: 0.0115968445\n",
      "Step = 235 train_loss: 0.020282269 val_loss: 0.011688259\n",
      "Step = 236 train_loss: 0.017585048 val_loss: 0.020294245\n",
      "Step = 237 train_loss: 0.019526599 val_loss: 0.040202066\n",
      "Step = 238 train_loss: 0.020185053 val_loss: 0.04151145\n",
      "Step = 239 train_loss: 0.018683465 val_loss: 0.032156035\n",
      "Step = 240 train_loss: 0.017002176 val_loss: 0.01604564\n",
      "Step = 241 train_loss: 0.019832792 val_loss: 0.012285196\n",
      "Step = 242 train_loss: 0.016160492 val_loss: 0.01744308\n",
      "Step = 243 train_loss: 0.014293153 val_loss: 0.01652743\n",
      "Step = 244 train_loss: 0.017264366 val_loss: 0.0104789715\n",
      "Step = 245 train_loss: 0.018349862 val_loss: 0.029521108\n",
      "Step = 246 train_loss: 0.01559708 val_loss: 0.034036834\n",
      "Step = 247 train_loss: 0.015556061 val_loss: 0.029942445\n",
      "Step = 248 train_loss: 0.020083038 val_loss: 0.018649386\n",
      "Step = 249 train_loss: 0.015386154 val_loss: 0.01004652\n",
      "Step = 250 train_loss: 0.021006279 val_loss: 0.014777566\n",
      "Step = 251 train_loss: 0.015652098 val_loss: 0.01752456\n",
      "Step = 252 train_loss: 0.019425027 val_loss: 0.011588707\n",
      "Step = 253 train_loss: 0.016191492 val_loss: 0.01163507\n",
      "Step = 254 train_loss: 0.01522458 val_loss: 0.014284745\n",
      "Step = 255 train_loss: 0.013081152 val_loss: 0.0151816085\n",
      "Step = 256 train_loss: 0.01784645 val_loss: 0.0149091\n",
      "Step = 257 train_loss: 0.014253195 val_loss: 0.013211911\n",
      "Step = 258 train_loss: 0.014779917 val_loss: 0.016824936\n",
      "Step = 259 train_loss: 0.016282083 val_loss: 0.019743301\n",
      "Step = 260 train_loss: 0.014813172 val_loss: 0.02390397\n",
      "Step = 261 train_loss: 0.016635224 val_loss: 0.02718954\n",
      "Step = 262 train_loss: 0.013444895 val_loss: 0.023111386\n",
      "Step = 263 train_loss: 0.016576149 val_loss: 0.012880458\n",
      "Step = 264 train_loss: 0.01614097 val_loss: 0.015023123\n",
      "Step = 265 train_loss: 0.015182273 val_loss: 0.033966377\n",
      "Step = 266 train_loss: 0.012353886 val_loss: 0.021892225\n",
      "Step = 267 train_loss: 0.0151839275 val_loss: 0.009966109\n",
      "Step = 268 train_loss: 0.0136762215 val_loss: 0.019751193\n",
      "Step = 269 train_loss: 0.017203908 val_loss: 0.029263848\n",
      "Step = 270 train_loss: 0.014885171 val_loss: 0.023566231\n",
      "Step = 271 train_loss: 0.019093676 val_loss: 0.017277194\n",
      "Step = 272 train_loss: 0.014966055 val_loss: 0.0107500255\n",
      "Step = 273 train_loss: 0.01528945 val_loss: 0.011667671\n",
      "Step = 274 train_loss: 0.016645305 val_loss: 0.010296009\n",
      "Step = 275 train_loss: 0.013342337 val_loss: 0.013648823\n",
      "Step = 276 train_loss: 0.016049152 val_loss: 0.022020774\n",
      "Step = 277 train_loss: 0.01282478 val_loss: 0.019453809\n",
      "Step = 278 train_loss: 0.013022047 val_loss: 0.009166255\n",
      "Step = 279 train_loss: 0.012564211 val_loss: 0.0512289\n",
      "Step = 280 train_loss: 0.013601507 val_loss: 0.06871335\n",
      "Step = 281 train_loss: 0.016435202 val_loss: 0.06593667\n",
      "Step = 282 train_loss: 0.01732502 val_loss: 0.019354505\n",
      "Step = 283 train_loss: 0.014367236 val_loss: 0.014680371\n",
      "Step = 284 train_loss: 0.01458976 val_loss: 0.021648893\n",
      "Step = 285 train_loss: 0.01741745 val_loss: 0.017316949\n",
      "Step = 286 train_loss: 0.015205829 val_loss: 0.008247109\n",
      "Step = 287 train_loss: 0.015026582 val_loss: 0.030414628\n",
      "Step = 288 train_loss: 0.012933199 val_loss: 0.039582714\n",
      "Step = 289 train_loss: 0.013063966 val_loss: 0.03459816\n",
      "Step = 290 train_loss: 0.015370544 val_loss: 0.009424524\n",
      "Step = 291 train_loss: 0.014923966 val_loss: 0.016157957\n",
      "Step = 292 train_loss: 0.014071518 val_loss: 0.011218315\n",
      "Step = 293 train_loss: 0.01102797 val_loss: 0.014915946\n",
      "Step = 294 train_loss: 0.014508317 val_loss: 0.023338985\n",
      "Step = 295 train_loss: 0.013707892 val_loss: 0.017400311\n",
      "Step = 296 train_loss: 0.011629648 val_loss: 0.01223991\n",
      "Step = 297 train_loss: 0.014152891 val_loss: 0.010132063\n",
      "Step = 298 train_loss: 0.011160261 val_loss: 0.014841144\n",
      "Step = 299 train_loss: 0.014508196 val_loss: 0.016236274\n",
      "Step = 300 train_loss: 0.012100441 val_loss: 0.011221409\n",
      "Step = 301 train_loss: 0.009951526 val_loss: 0.015147973\n",
      "Step = 302 train_loss: 0.01123709 val_loss: 0.0075884187\n",
      "Step = 303 train_loss: 0.012143102 val_loss: 0.008373771\n",
      "Step = 304 train_loss: 0.01318955 val_loss: 0.008413071\n",
      "Step = 305 train_loss: 0.01574 val_loss: 0.010447475\n",
      "Step = 306 train_loss: 0.012826339 val_loss: 0.016838044\n",
      "Step = 307 train_loss: 0.010297297 val_loss: 0.015770935\n",
      "Step = 308 train_loss: 0.011700057 val_loss: 0.015313781\n",
      "Step = 309 train_loss: 0.01457954 val_loss: 0.012785304\n",
      "Step = 310 train_loss: 0.0133734895 val_loss: 0.012530815\n",
      "Step = 311 train_loss: 0.010889233 val_loss: 0.014474275\n",
      "Step = 312 train_loss: 0.009965379 val_loss: 0.021335667\n",
      "Step = 313 train_loss: 0.012447654 val_loss: 0.011637815\n",
      "Step = 314 train_loss: 0.0080376705 val_loss: 0.015157188\n",
      "Step = 315 train_loss: 0.012502357 val_loss: 0.0099045485\n",
      "Step = 316 train_loss: 0.012728533 val_loss: 0.026239166\n",
      "Step = 317 train_loss: 0.01167051 val_loss: 0.013023907\n",
      "Step = 318 train_loss: 0.015354147 val_loss: 0.0126900785\n",
      "Step = 319 train_loss: 0.011593877 val_loss: 0.011752226\n",
      "Step = 320 train_loss: 0.0124041345 val_loss: 0.013837874\n",
      "Step = 321 train_loss: 0.013711582 val_loss: 0.021134775\n",
      "Step = 322 train_loss: 0.013832941 val_loss: 0.019714803\n",
      "Step = 323 train_loss: 0.010934868 val_loss: 0.0076379143\n",
      "Step = 324 train_loss: 0.011549418 val_loss: 0.00661992\n",
      "Step = 325 train_loss: 0.013076811 val_loss: 0.008322954\n",
      "Step = 326 train_loss: 0.011797341 val_loss: 0.007661297\n",
      "Step = 327 train_loss: 0.01338405 val_loss: 0.006916795\n",
      "Step = 328 train_loss: 0.008027418 val_loss: 0.00573816\n",
      "Step = 329 train_loss: 0.011395577 val_loss: 0.0058103357\n",
      "Step = 330 train_loss: 0.01185166 val_loss: 0.005769959\n",
      "Step = 331 train_loss: 0.008340984 val_loss: 0.010380551\n",
      "Step = 332 train_loss: 0.012935538 val_loss: 0.007529422\n",
      "Step = 333 train_loss: 0.008895135 val_loss: 0.011072137\n",
      "Step = 334 train_loss: 0.009819586 val_loss: 0.0059422045\n",
      "Step = 335 train_loss: 0.009707089 val_loss: 0.0172236\n",
      "Step = 336 train_loss: 0.014807481 val_loss: 0.028011767\n",
      "Step = 337 train_loss: 0.01323595 val_loss: 0.0071741235\n",
      "Step = 338 train_loss: 0.010011021 val_loss: 0.025334474\n",
      "Step = 339 train_loss: 0.01222029 val_loss: 0.031202706\n",
      "Step = 340 train_loss: 0.012058689 val_loss: 0.030048236\n",
      "Step = 341 train_loss: 0.011218708 val_loss: 0.025559403\n",
      "Step = 342 train_loss: 0.013092914 val_loss: 0.02207618\n",
      "Step = 343 train_loss: 0.01418131 val_loss: 0.019468464\n",
      "Step = 344 train_loss: 0.014072278 val_loss: 0.022390481\n",
      "Step = 345 train_loss: 0.01276957 val_loss: 0.030322008\n",
      "Step = 346 train_loss: 0.0116565535 val_loss: 0.024477854\n",
      "Step = 347 train_loss: 0.010253238 val_loss: 0.012018911\n",
      "Step = 348 train_loss: 0.009575986 val_loss: 0.018864933\n",
      "Step = 349 train_loss: 0.009015269 val_loss: 0.020834917\n",
      "Step = 350 train_loss: 0.0107270125 val_loss: 0.011187001\n",
      "Step = 351 train_loss: 0.00912449 val_loss: 0.023744997\n",
      "Step = 352 train_loss: 0.011309759 val_loss: 0.025869437\n",
      "Step = 353 train_loss: 0.010487521 val_loss: 0.022445317\n",
      "Step = 354 train_loss: 0.012796547 val_loss: 0.009557953\n",
      "Step = 355 train_loss: 0.008529982 val_loss: 0.006933763\n",
      "Step = 356 train_loss: 0.01212571 val_loss: 0.0075005097\n",
      "Step = 357 train_loss: 0.010382305 val_loss: 0.013446609\n",
      "Step = 358 train_loss: 0.008807157 val_loss: 0.01881109\n",
      "Step = 359 train_loss: 0.010779226 val_loss: 0.008530949\n",
      "Step = 360 train_loss: 0.009876069 val_loss: 0.010579843\n",
      "Step = 361 train_loss: 0.008210061 val_loss: 0.015796741\n",
      "Step = 362 train_loss: 0.0095370235 val_loss: 0.020892702\n",
      "Step = 363 train_loss: 0.009616665 val_loss: 0.018374598\n",
      "Step = 364 train_loss: 0.008760217 val_loss: 0.015065973\n",
      "Step = 365 train_loss: 0.00988531 val_loss: 0.014490649\n",
      "Step = 366 train_loss: 0.011785166 val_loss: 0.015004694\n",
      "Step = 367 train_loss: 0.010145867 val_loss: 0.008310211\n",
      "Step = 368 train_loss: 0.009259512 val_loss: 0.01144079\n",
      "Step = 369 train_loss: 0.010615907 val_loss: 0.006634211\n",
      "Step = 370 train_loss: 0.006142852 val_loss: 0.0064644474\n",
      "Step = 371 train_loss: 0.00857579 val_loss: 0.0077259024\n",
      "Step = 372 train_loss: 0.010172937 val_loss: 0.013374422\n",
      "Step = 373 train_loss: 0.01102829 val_loss: 0.01889987\n",
      "Step = 374 train_loss: 0.01095828 val_loss: 0.018873006\n",
      "Step = 375 train_loss: 0.00682199 val_loss: 0.0121904\n",
      "Step = 376 train_loss: 0.009853418 val_loss: 0.0065244413\n",
      "Step = 377 train_loss: 0.008830765 val_loss: 0.035251293\n",
      "Step = 378 train_loss: 0.009564918 val_loss: 0.016682569\n",
      "Step = 379 train_loss: 0.007105278 val_loss: 0.0076788985\n",
      "Step = 380 train_loss: 0.01119844 val_loss: 0.0084872395\n",
      "Step = 381 train_loss: 0.010721044 val_loss: 0.006437703\n",
      "Step = 382 train_loss: 0.010826179 val_loss: 0.008185401\n",
      "Step = 383 train_loss: 0.007630334 val_loss: 0.0074535194\n",
      "Step = 384 train_loss: 0.006506267 val_loss: 0.0061870036\n",
      "Step = 385 train_loss: 0.009270093 val_loss: 0.0118042175\n",
      "Step = 386 train_loss: 0.00822496 val_loss: 0.009596386\n",
      "Step = 387 train_loss: 0.009179657 val_loss: 0.005596496\n",
      "Step = 388 train_loss: 0.009103625 val_loss: 0.010506884\n",
      "Step = 389 train_loss: 0.00979872 val_loss: 0.005086581\n",
      "Step = 390 train_loss: 0.0069595235 val_loss: 0.0145408\n",
      "Step = 391 train_loss: 0.009187267 val_loss: 0.017022775\n",
      "Step = 392 train_loss: 0.0078006363 val_loss: 0.014516649\n",
      "Step = 393 train_loss: 0.010174879 val_loss: 0.011378399\n",
      "Step = 394 train_loss: 0.007578622 val_loss: 0.01379498\n",
      "Step = 395 train_loss: 0.007723234 val_loss: 0.015787931\n",
      "Step = 396 train_loss: 0.008193129 val_loss: 0.007733138\n",
      "Step = 397 train_loss: 0.008620559 val_loss: 0.004889641\n",
      "Step = 398 train_loss: 0.007662226 val_loss: 0.014847242\n",
      "Step = 399 train_loss: 0.008019516 val_loss: 0.024806675\n",
      "Step = 400 train_loss: 0.011221693 val_loss: 0.021902869\n",
      "Step = 401 train_loss: 0.008624891 val_loss: 0.019431617\n",
      "Step = 402 train_loss: 0.0067299777 val_loss: 0.018154997\n",
      "Step = 403 train_loss: 0.005362526 val_loss: 0.013031219\n",
      "Step = 404 train_loss: 0.007361971 val_loss: 0.007427409\n",
      "Step = 405 train_loss: 0.010676816 val_loss: 0.036598623\n",
      "Step = 406 train_loss: 0.011794539 val_loss: 0.04071531\n",
      "Step = 407 train_loss: 0.012894701 val_loss: 0.06190244\n",
      "Step = 408 train_loss: 0.01203697 val_loss: 0.06353662\n",
      "Step = 409 train_loss: 0.013273025 val_loss: 0.018081069\n",
      "Step = 410 train_loss: 0.011012608 val_loss: 0.026035855\n",
      "Step = 411 train_loss: 0.0082424125 val_loss: 0.022371003\n",
      "Step = 412 train_loss: 0.013462187 val_loss: 0.02152731\n",
      "Step = 413 train_loss: 0.014099733 val_loss: 0.019914692\n",
      "Step = 414 train_loss: 0.008174575 val_loss: 0.03418518\n",
      "Step = 415 train_loss: 0.009760689 val_loss: 0.15370712\n",
      "Step = 416 train_loss: 0.01356546 val_loss: 0.13836554\n",
      "Step = 417 train_loss: 0.0110034095 val_loss: 0.12859231\n",
      "Step = 418 train_loss: 0.0121733 val_loss: 0.10400749\n",
      "Step = 419 train_loss: 0.014525569 val_loss: 0.09426938\n",
      "Step = 420 train_loss: 0.012019981 val_loss: 0.111832164\n",
      "Step = 421 train_loss: 0.011495336 val_loss: 0.030944727\n",
      "Step = 422 train_loss: 0.007378129 val_loss: 0.018118734\n",
      "Step = 423 train_loss: 0.0068747895 val_loss: 0.022880506\n",
      "Step = 424 train_loss: 0.010438373 val_loss: 0.023265665\n",
      "Step = 425 train_loss: 0.010213828 val_loss: 0.020239009\n",
      "Step = 426 train_loss: 0.00809871 val_loss: 0.01753108\n",
      "Step = 427 train_loss: 0.0071581677 val_loss: 0.033473548\n",
      "Step = 428 train_loss: 0.007692822 val_loss: 0.06734207\n",
      "Step = 429 train_loss: 0.008227491 val_loss: 0.05831762\n",
      "Step = 430 train_loss: 0.012286761 val_loss: 0.041172955\n",
      "Step = 431 train_loss: 0.01129689 val_loss: 0.044323754\n",
      "Step = 432 train_loss: 0.008187267 val_loss: 0.008092593\n",
      "Step = 433 train_loss: 0.007198641 val_loss: 0.019752597\n",
      "Step = 434 train_loss: 0.0072712726 val_loss: 0.02508593\n",
      "Step = 435 train_loss: 0.008752575 val_loss: 0.024550952\n",
      "Step = 436 train_loss: 0.008557401 val_loss: 0.02128725\n",
      "Step = 437 train_loss: 0.008689419 val_loss: 0.027085407\n",
      "Step = 438 train_loss: 0.008760828 val_loss: 0.019156076\n",
      "Step = 439 train_loss: 0.012685542 val_loss: 0.010979691\n",
      "Step = 440 train_loss: 0.0054434114 val_loss: 0.01592402\n",
      "Step = 441 train_loss: 0.008605866 val_loss: 0.017096885\n",
      "Step = 442 train_loss: 0.008721977 val_loss: 0.013640907\n",
      "Step = 443 train_loss: 0.0059158574 val_loss: 0.016319487\n",
      "Step = 444 train_loss: 0.005193829 val_loss: 0.017826078\n",
      "Step = 445 train_loss: 0.008124336 val_loss: 0.018072046\n",
      "Step = 446 train_loss: 0.0058180336 val_loss: 0.0096652405\n",
      "Step = 447 train_loss: 0.006163588 val_loss: 0.03272897\n",
      "Step = 448 train_loss: 0.008265981 val_loss: 0.060898166\n",
      "Step = 449 train_loss: 0.009276776 val_loss: 0.010162018\n",
      "Step = 450 train_loss: 0.0053621028 val_loss: 0.011420165\n",
      "Step = 451 train_loss: 0.009164871 val_loss: 0.010204409\n",
      "Step = 452 train_loss: 0.009441231 val_loss: 0.014854496\n",
      "Step = 453 train_loss: 0.009381109 val_loss: 0.103005074\n",
      "Step = 454 train_loss: 0.009851183 val_loss: 0.067071564\n",
      "Step = 455 train_loss: 0.009016943 val_loss: 0.010842598\n",
      "Step = 456 train_loss: 0.0073554358 val_loss: 0.019918319\n",
      "Step = 457 train_loss: 0.011575919 val_loss: 0.0055380296\n",
      "Step = 458 train_loss: 0.008033656 val_loss: 0.16542712\n",
      "Step = 459 train_loss: 0.008030379 val_loss: 0.1802689\n",
      "Step = 460 train_loss: 0.009805148 val_loss: 0.13603151\n",
      "Step = 461 train_loss: 0.00789941 val_loss: 0.007001445\n",
      "Step = 462 train_loss: 0.0058242423 val_loss: 0.022698035\n",
      "Step = 463 train_loss: 0.008830022 val_loss: 0.020806665\n",
      "Step = 464 train_loss: 0.0067470325 val_loss: 0.011351349\n",
      "Step = 465 train_loss: 0.00643043 val_loss: 0.012826155\n",
      "Step = 466 train_loss: 0.008962355 val_loss: 0.16205096\n",
      "Step = 467 train_loss: 0.006008343 val_loss: 0.18209486\n",
      "Step = 468 train_loss: 0.010123219 val_loss: 0.032589473\n",
      "Step = 469 train_loss: 0.0066044936 val_loss: 0.027056478\n",
      "Step = 470 train_loss: 0.0072612017 val_loss: 0.028947629\n",
      "Step = 471 train_loss: 0.010747213 val_loss: 0.029673845\n",
      "Step = 472 train_loss: 0.011801591 val_loss: 0.00621451\n",
      "Step = 473 train_loss: 0.007869089 val_loss: 0.22897157\n",
      "Step = 474 train_loss: 0.008260619 val_loss: 0.26256865\n",
      "Step = 475 train_loss: 0.013743206 val_loss: 0.23718667\n",
      "Step = 476 train_loss: 0.0107556665 val_loss: 0.005031752\n",
      "Step = 477 train_loss: 0.008642953 val_loss: 0.026981333\n",
      "Step = 478 train_loss: 0.013394534 val_loss: 0.016924242\n",
      "Step = 479 train_loss: 0.009959947 val_loss: 0.36206037\n",
      "Step = 480 train_loss: 0.008172811 val_loss: 0.49467972\n",
      "Step = 481 train_loss: 0.0068567237 val_loss: 0.50488293\n",
      "Step = 482 train_loss: 0.004644049 val_loss: 0.50423527\n",
      "Step = 483 train_loss: 0.009516713 val_loss: 0.4936992\n",
      "Step = 484 train_loss: 0.008665421 val_loss: 0.4729743\n",
      "Step = 485 train_loss: 0.007351108 val_loss: 0.44213626\n",
      "Step = 486 train_loss: 0.009325729 val_loss: 0.42071524\n",
      "Step = 487 train_loss: 0.007795631 val_loss: 0.43416464\n",
      "Step = 488 train_loss: 0.007549088 val_loss: 0.44506902\n",
      "Step = 489 train_loss: 0.011976977 val_loss: 0.4134097\n",
      "Step = 490 train_loss: 0.006341482 val_loss: 0.35273767\n",
      "Step = 491 train_loss: 0.0045929495 val_loss: 0.23690784\n",
      "Step = 492 train_loss: 0.010320195 val_loss: 0.12162088\n",
      "Step = 493 train_loss: 0.0076643373 val_loss: 0.07644956\n",
      "Step = 494 train_loss: 0.011711437 val_loss: 0.15503295\n",
      "Step = 495 train_loss: 0.008587252 val_loss: 0.22487909\n",
      "Step = 496 train_loss: 0.0074803135 val_loss: 0.2850368\n",
      "Step = 497 train_loss: 0.004952291 val_loss: 0.22682455\n",
      "Step = 498 train_loss: 0.008479727 val_loss: 0.0065835505\n",
      "Step = 499 train_loss: 0.0102259 val_loss: 0.020278817\n",
      "Step = 500 train_loss: 0.00792324 val_loss: 0.023786059\n",
      "Step = 501 train_loss: 0.0110625485 val_loss: 0.01169057\n",
      "Step = 502 train_loss: 0.008342187 val_loss: 0.012018642\n",
      "Step = 503 train_loss: 0.009109879 val_loss: 0.0833486\n",
      "Step = 504 train_loss: 0.00741554 val_loss: 0.17598385\n",
      "Step = 505 train_loss: 0.00830797 val_loss: 0.14703207\n",
      "Step = 506 train_loss: 0.008107515 val_loss: 0.038607255\n",
      "Step = 507 train_loss: 0.011266493 val_loss: 0.0064685717\n",
      "Step = 508 train_loss: 0.00841565 val_loss: 0.00511766\n",
      "Step = 509 train_loss: 0.011953994 val_loss: 0.020313838\n",
      "Step = 510 train_loss: 0.009369828 val_loss: 0.13785931\n",
      "Step = 511 train_loss: 0.008452023 val_loss: 0.12662557\n",
      "Step = 512 train_loss: 0.0051796595 val_loss: 0.012133687\n",
      "Step = 513 train_loss: 0.007625709 val_loss: 0.009666015\n",
      "Step = 514 train_loss: 0.009112419 val_loss: 0.0076281107\n",
      "Step = 515 train_loss: 0.008919864 val_loss: 0.048943937\n",
      "Step = 516 train_loss: 0.007783084 val_loss: 0.028851446\n",
      "Step = 517 train_loss: 0.0068360907 val_loss: 0.008778079\n",
      "Step = 518 train_loss: 0.007019658 val_loss: 0.020219887\n",
      "Step = 519 train_loss: 0.007150108 val_loss: 0.0088404855\n",
      "Step = 520 train_loss: 0.0044251895 val_loss: 0.007239828\n",
      "Step = 521 train_loss: 0.008876718 val_loss: 0.1064104\n",
      "Step = 522 train_loss: 0.007632022 val_loss: 0.08879207\n",
      "Step = 523 train_loss: 0.0071274075 val_loss: 0.00545532\n",
      "Step = 524 train_loss: 0.0050920565 val_loss: 0.031486426\n",
      "Step = 525 train_loss: 0.011812887 val_loss: 0.03873594\n",
      "Step = 526 train_loss: 0.0073440275 val_loss: 0.038432922\n",
      "Step = 527 train_loss: 0.011416858 val_loss: 0.0077712988\n",
      "Step = 528 train_loss: 0.0045414353 val_loss: 0.052321173\n",
      "Step = 529 train_loss: 0.0077844667 val_loss: 0.09727212\n",
      "Step = 530 train_loss: 0.006933756 val_loss: 0.04013227\n",
      "Step = 531 train_loss: 0.006231471 val_loss: 0.005986587\n",
      "Step = 532 train_loss: 0.0054539125 val_loss: 0.011599616\n",
      "Step = 533 train_loss: 0.006630537 val_loss: 0.006794173\n",
      "Step = 534 train_loss: 0.00844272 val_loss: 0.030850716\n",
      "Step = 535 train_loss: 0.00810083 val_loss: 0.1792687\n",
      "Step = 536 train_loss: 0.007367168 val_loss: 0.18423007\n",
      "Step = 537 train_loss: 0.011068075 val_loss: 0.065373376\n",
      "Step = 538 train_loss: 0.009852971 val_loss: 0.018655542\n",
      "Step = 539 train_loss: 0.00931176 val_loss: 0.03646939\n",
      "Step = 540 train_loss: 0.0065473127 val_loss: 0.037090622\n",
      "Step = 541 train_loss: 0.008994223 val_loss: 0.02066617\n",
      "Step = 542 train_loss: 0.006335971 val_loss: 0.03424877\n",
      "Step = 543 train_loss: 0.0059192358 val_loss: 0.11970934\n",
      "Step = 544 train_loss: 0.0076563414 val_loss: 0.060656317\n",
      "Step = 545 train_loss: 0.0055906633 val_loss: 0.008466279\n",
      "Step = 546 train_loss: 0.0087477425 val_loss: 0.029377306\n",
      "Step = 547 train_loss: 0.006740967 val_loss: 0.040200245\n",
      "Step = 548 train_loss: 0.0076987916 val_loss: 0.035943884\n",
      "Step = 549 train_loss: 0.012209118 val_loss: 0.014625969\n",
      "Step = 550 train_loss: 0.005631388 val_loss: 0.17000987\n",
      "Step = 551 train_loss: 0.006214768 val_loss: 0.17804922\n",
      "Step = 552 train_loss: 0.007158876 val_loss: 0.1192922\n",
      "Step = 553 train_loss: 0.00839764 val_loss: 0.031576924\n",
      "Step = 554 train_loss: 0.007753254 val_loss: 0.03355783\n",
      "Step = 555 train_loss: 0.0077264262 val_loss: 0.04362653\n",
      "Step = 556 train_loss: 0.007188102 val_loss: 0.043436494\n",
      "Step = 557 train_loss: 0.007230963 val_loss: 0.041228093\n",
      "Step = 558 train_loss: 0.006860023 val_loss: 0.008964406\n",
      "Step = 559 train_loss: 0.010116675 val_loss: 0.1533497\n",
      "Step = 560 train_loss: 0.006044836 val_loss: 0.1820556\n",
      "Step = 561 train_loss: 0.009440342 val_loss: 0.064212255\n",
      "Step = 562 train_loss: 0.006227119 val_loss: 0.01795091\n",
      "Step = 563 train_loss: 0.008709951 val_loss: 0.030953396\n",
      "Step = 564 train_loss: 0.009678412 val_loss: 0.02043429\n",
      "Step = 565 train_loss: 0.007851961 val_loss: 0.006582142\n",
      "Step = 566 train_loss: 0.0069369855 val_loss: 0.09084559\n",
      "Step = 567 train_loss: 0.0059250128 val_loss: 0.18695182\n",
      "Step = 568 train_loss: 0.004893484 val_loss: 0.13351184\n",
      "Step = 569 train_loss: 0.008282586 val_loss: 0.005944811\n",
      "Step = 570 train_loss: 0.0072567933 val_loss: 0.029995505\n",
      "Step = 571 train_loss: 0.007591911 val_loss: 0.035063274\n",
      "Step = 572 train_loss: 0.008454176 val_loss: 0.03235484\n",
      "Step = 573 train_loss: 0.006814978 val_loss: 0.02513232\n",
      "Step = 574 train_loss: 0.006540021 val_loss: 0.0067877015\n",
      "Step = 575 train_loss: 0.008260914 val_loss: 0.052422408\n",
      "Step = 576 train_loss: 0.008974189 val_loss: 0.10424208\n",
      "Step = 577 train_loss: 0.006973798 val_loss: 0.036928482\n",
      "Step = 578 train_loss: 0.005509437 val_loss: 0.006044605\n",
      "Step = 579 train_loss: 0.0049236543 val_loss: 0.012337464\n",
      "Step = 580 train_loss: 0.005557086 val_loss: 0.0062832637\n",
      "Step = 581 train_loss: 0.0058743907 val_loss: 0.020306394\n",
      "Step = 582 train_loss: 0.0056456504 val_loss: 0.06306837\n",
      "Step = 583 train_loss: 0.0062253694 val_loss: 0.01269855\n",
      "Step = 584 train_loss: 0.0062185777 val_loss: 0.0054378537\n",
      "Step = 585 train_loss: 0.004857474 val_loss: 0.0063851667\n",
      "Step = 586 train_loss: 0.007565705 val_loss: 0.00944578\n",
      "Step = 587 train_loss: 0.005934846 val_loss: 0.0055282293\n",
      "Step = 588 train_loss: 0.0053779264 val_loss: 0.0057275584\n",
      "Step = 589 train_loss: 0.00799688 val_loss: 0.010191239\n",
      "Step = 590 train_loss: 0.010518044 val_loss: 0.040696144\n",
      "Step = 591 train_loss: 0.005358767 val_loss: 0.01959581\n",
      "Step = 592 train_loss: 0.007175589 val_loss: 0.0054346775\n",
      "Step = 593 train_loss: 0.0050849686 val_loss: 0.0154674165\n",
      "Step = 594 train_loss: 0.007003924 val_loss: 0.008764841\n",
      "Step = 595 train_loss: 0.005906462 val_loss: 0.0098333545\n",
      "Step = 596 train_loss: 0.006042634 val_loss: 0.055211026\n",
      "Step = 597 train_loss: 0.00720973 val_loss: 0.008075253\n",
      "Step = 598 train_loss: 0.004090033 val_loss: 0.005515106\n",
      "Step = 599 train_loss: 0.008640557 val_loss: 0.007137975\n",
      "Step = 600 train_loss: 0.0075686756 val_loss: 0.02835215\n",
      "Step = 601 train_loss: 0.008486376 val_loss: 0.009755199\n",
      "Step = 602 train_loss: 0.009070991 val_loss: 0.007152301\n",
      "Step = 603 train_loss: 0.004913659 val_loss: 0.005356755\n",
      "Step = 604 train_loss: 0.0055240146 val_loss: 0.0054492787\n",
      "Step = 605 train_loss: 0.0072161206 val_loss: 0.009601889\n",
      "Step = 606 train_loss: 0.006422854 val_loss: 0.008646082\n",
      "Step = 607 train_loss: 0.00679965 val_loss: 0.0050281505\n",
      "Step = 608 train_loss: 0.00556849 val_loss: 0.009776554\n",
      "Step = 609 train_loss: 0.0070386725 val_loss: 0.013777997\n",
      "Step = 610 train_loss: 0.0040044636 val_loss: 0.011034224\n",
      "Step = 611 train_loss: 0.007780822 val_loss: 0.008978825\n",
      "Step = 612 train_loss: 0.006922968 val_loss: 0.073402286\n",
      "Step = 613 train_loss: 0.006668469 val_loss: 0.017489202\n",
      "Step = 614 train_loss: 0.00640803 val_loss: 0.019083556\n",
      "Step = 615 train_loss: 0.005312138 val_loss: 0.032553904\n",
      "Step = 616 train_loss: 0.0071605896 val_loss: 0.029809397\n",
      "Step = 617 train_loss: 0.0074368003 val_loss: 0.017579064\n",
      "Step = 618 train_loss: 0.0066547445 val_loss: 0.0070922514\n",
      "Step = 619 train_loss: 0.008222894 val_loss: 0.055934586\n",
      "Step = 620 train_loss: 0.0084000295 val_loss: 0.027689762\n",
      "Step = 621 train_loss: 0.007427305 val_loss: 0.011354154\n",
      "Step = 622 train_loss: 0.0042628082 val_loss: 0.03000432\n",
      "Step = 623 train_loss: 0.008019424 val_loss: 0.032472275\n",
      "Step = 624 train_loss: 0.009342225 val_loss: 0.032484382\n",
      "Step = 625 train_loss: 0.0076708775 val_loss: 0.030765934\n",
      "Step = 626 train_loss: 0.0066869995 val_loss: 0.018170843\n",
      "Step = 627 train_loss: 0.007292043 val_loss: 0.0058185626\n",
      "Step = 628 train_loss: 0.0070626806 val_loss: 0.01785719\n",
      "Step = 629 train_loss: 0.005210204 val_loss: 0.005826041\n",
      "Step = 630 train_loss: 0.009049216 val_loss: 0.018421646\n",
      "Step = 631 train_loss: 0.0058348733 val_loss: 0.024574239\n",
      "Step = 632 train_loss: 0.0032341145 val_loss: 0.02490915\n",
      "Step = 633 train_loss: 0.006786084 val_loss: 0.024621082\n",
      "Step = 634 train_loss: 0.005216716 val_loss: 0.024029601\n",
      "Step = 635 train_loss: 0.007905556 val_loss: 0.024236934\n",
      "Step = 636 train_loss: 0.005810753 val_loss: 0.018610427\n",
      "Step = 637 train_loss: 0.0032957024 val_loss: 0.015843445\n",
      "Step = 638 train_loss: 0.00524376 val_loss: 0.01564266\n",
      "Step = 639 train_loss: 0.004142928 val_loss: 0.015343627\n",
      "Step = 640 train_loss: 0.005597752 val_loss: 0.013621631\n",
      "Step = 641 train_loss: 0.0067348033 val_loss: 0.020238532\n",
      "Step = 642 train_loss: 0.00614051 val_loss: 0.023995025\n",
      "Step = 643 train_loss: 0.0047807647 val_loss: 0.024009349\n",
      "Step = 644 train_loss: 0.006421159 val_loss: 0.025331669\n",
      "Step = 645 train_loss: 0.006184812 val_loss: 0.030565629\n",
      "Step = 646 train_loss: 0.0058777407 val_loss: 0.030813063\n",
      "Step = 647 train_loss: 0.0067321328 val_loss: 0.02246677\n",
      "Step = 648 train_loss: 0.0034073347 val_loss: 0.010471917\n",
      "Step = 649 train_loss: 0.004890955 val_loss: 0.018712716\n",
      "Step = 650 train_loss: 0.004043092 val_loss: 0.021706274\n",
      "Step = 651 train_loss: 0.0065769632 val_loss: 0.020027325\n",
      "Step = 652 train_loss: 0.008398055 val_loss: 0.010914486\n",
      "Step = 653 train_loss: 0.0043639746 val_loss: 0.018345784\n",
      "Step = 654 train_loss: 0.0052163037 val_loss: 0.022677995\n",
      "Step = 655 train_loss: 0.0056994255 val_loss: 0.025585242\n",
      "Step = 656 train_loss: 0.004096299 val_loss: 0.022485532\n",
      "Step = 657 train_loss: 0.0057925405 val_loss: 0.015712418\n",
      "Step = 658 train_loss: 0.0059339534 val_loss: 0.013301017\n",
      "Step = 659 train_loss: 0.0065308483 val_loss: 0.027411558\n",
      "Step = 660 train_loss: 0.005294231 val_loss: 0.041717026\n",
      "Step = 661 train_loss: 0.0052068103 val_loss: 0.06762196\n",
      "Step = 662 train_loss: 0.004073244 val_loss: 0.11995804\n",
      "Step = 663 train_loss: 0.0053982236 val_loss: 0.08279428\n",
      "Step = 664 train_loss: 0.004348 val_loss: 0.047847506\n",
      "Step = 665 train_loss: 0.004332154 val_loss: 0.033016857\n",
      "Step = 666 train_loss: 0.0060738293 val_loss: 0.032666713\n",
      "Step = 667 train_loss: 0.0045208004 val_loss: 0.03779706\n",
      "Step = 668 train_loss: 0.0026714965 val_loss: 0.056173995\n",
      "Step = 669 train_loss: 0.007394994 val_loss: 0.061102528\n",
      "Step = 670 train_loss: 0.0068779686 val_loss: 0.01676638\n",
      "Step = 671 train_loss: 0.0032009436 val_loss: 0.009908202\n",
      "Step = 672 train_loss: 0.0064202314 val_loss: 0.008200594\n",
      "Step = 673 train_loss: 0.005354087 val_loss: 0.00941377\n",
      "Step = 674 train_loss: 0.0036407006 val_loss: 0.009138079\n",
      "Step = 675 train_loss: 0.0024142405 val_loss: 0.005727784\n",
      "Step = 676 train_loss: 0.0082042515 val_loss: 0.01246018\n",
      "Step = 677 train_loss: 0.0054059373 val_loss: 0.021366494\n",
      "Step = 678 train_loss: 0.0056806933 val_loss: 0.021068046\n",
      "Step = 679 train_loss: 0.0049739773 val_loss: 0.016549764\n",
      "Step = 680 train_loss: 0.0028625377 val_loss: 0.0068955123\n",
      "Step = 681 train_loss: 0.0051652435 val_loss: 0.0053968006\n",
      "Step = 682 train_loss: 0.0041531497 val_loss: 0.0076319263\n",
      "Step = 683 train_loss: 0.0046227206 val_loss: 0.01373982\n",
      "Step = 684 train_loss: 0.0027916373 val_loss: 0.006842923\n",
      "Step = 685 train_loss: 0.0038763944 val_loss: 0.0132381\n",
      "Step = 686 train_loss: 0.0039436496 val_loss: 0.024605913\n",
      "Step = 687 train_loss: 0.004208927 val_loss: 0.025995461\n",
      "Step = 688 train_loss: 0.0047640414 val_loss: 0.008131463\n",
      "Step = 689 train_loss: 0.0056091323 val_loss: 0.04024152\n",
      "Step = 690 train_loss: 0.004851625 val_loss: 0.14770548\n",
      "Step = 691 train_loss: 0.0027664204 val_loss: 0.17903852\n",
      "Step = 692 train_loss: 0.005308939 val_loss: 0.20314226\n",
      "Step = 693 train_loss: 0.0037290843 val_loss: 0.12962979\n",
      "Step = 694 train_loss: 0.004525118 val_loss: 0.0054940493\n",
      "Step = 695 train_loss: 0.0049698036 val_loss: 0.023143662\n",
      "Step = 696 train_loss: 0.005733737 val_loss: 0.025716217\n",
      "Step = 697 train_loss: 0.00408968 val_loss: 0.026020935\n",
      "Step = 698 train_loss: 0.005392099 val_loss: 0.026385238\n",
      "Step = 699 train_loss: 0.0068891174 val_loss: 0.019583272\n",
      "Step = 700 train_loss: 0.0047053886 val_loss: 0.0039117793\n",
      "Step = 701 train_loss: 0.0043684 val_loss: 0.061007343\n",
      "Step = 702 train_loss: 0.005600429 val_loss: 0.1653535\n",
      "Step = 703 train_loss: 0.005552342 val_loss: 0.2815792\n",
      "Step = 704 train_loss: 0.008085877 val_loss: 0.1950348\n",
      "Step = 705 train_loss: 0.0045550154 val_loss: 0.058608502\n",
      "Step = 706 train_loss: 0.004266905 val_loss: 0.0074849464\n",
      "Step = 707 train_loss: 0.00524453 val_loss: 0.026312673\n",
      "Step = 708 train_loss: 0.004496504 val_loss: 0.027415616\n",
      "Step = 709 train_loss: 0.004948129 val_loss: 0.02750179\n",
      "Step = 710 train_loss: 0.0041573197 val_loss: 0.028314125\n",
      "Step = 711 train_loss: 0.0060399342 val_loss: 0.023438577\n",
      "Step = 712 train_loss: 0.0037638466 val_loss: 0.009287731\n",
      "Step = 713 train_loss: 0.006035988 val_loss: 0.121293455\n",
      "Step = 714 train_loss: 0.0053099524 val_loss: 0.0808494\n",
      "Step = 715 train_loss: 0.0027063454 val_loss: 0.0093656145\n",
      "Step = 716 train_loss: 0.0076607424 val_loss: 0.009390157\n",
      "Step = 717 train_loss: 0.003138136 val_loss: 0.018551901\n",
      "Step = 718 train_loss: 0.003445552 val_loss: 0.021901391\n",
      "Step = 719 train_loss: 0.003649761 val_loss: 0.017301012\n",
      "Step = 720 train_loss: 0.0030930121 val_loss: 0.012418987\n",
      "Step = 721 train_loss: 0.0031853463 val_loss: 0.008455545\n",
      "Step = 722 train_loss: 0.0047059236 val_loss: 0.005143797\n",
      "Step = 723 train_loss: 0.0034129212 val_loss: 0.018731076\n",
      "Step = 724 train_loss: 0.0042926087 val_loss: 0.060038522\n",
      "Step = 725 train_loss: 0.0047602663 val_loss: 0.06659761\n",
      "Step = 726 train_loss: 0.005075497 val_loss: 0.012855916\n",
      "Step = 727 train_loss: 0.0064357645 val_loss: 0.008976241\n",
      "Step = 728 train_loss: 0.0021875596 val_loss: 0.007366118\n",
      "Step = 729 train_loss: 0.0038066257 val_loss: 0.0049225017\n",
      "Step = 730 train_loss: 0.0051158895 val_loss: 0.011081181\n",
      "Step = 731 train_loss: 0.00451663 val_loss: 0.019137995\n",
      "Step = 732 train_loss: 0.004798695 val_loss: 0.019079521\n",
      "Step = 733 train_loss: 0.0062074894 val_loss: 0.018335631\n",
      "Step = 734 train_loss: 0.004435748 val_loss: 0.0065253377\n",
      "Step = 735 train_loss: 0.0032382011 val_loss: 0.0045663076\n",
      "Step = 736 train_loss: 0.0038531409 val_loss: 0.004491258\n",
      "Step = 737 train_loss: 0.0036543708 val_loss: 0.006072782\n",
      "Step = 738 train_loss: 0.004319533 val_loss: 0.033850335\n",
      "Step = 739 train_loss: 0.006367213 val_loss: 0.005144502\n",
      "Step = 740 train_loss: 0.003891128 val_loss: 0.0114426445\n",
      "Step = 741 train_loss: 0.003122157 val_loss: 0.009162577\n",
      "Step = 742 train_loss: 0.0054237153 val_loss: 0.004526318\n",
      "Step = 743 train_loss: 0.0035520333 val_loss: 0.011172537\n",
      "Step = 744 train_loss: 0.004824846 val_loss: 0.023768956\n",
      "Step = 745 train_loss: 0.005488919 val_loss: 0.0052246833\n",
      "Step = 746 train_loss: 0.0034765773 val_loss: 0.0062947148\n",
      "Step = 747 train_loss: 0.004143357 val_loss: 0.004085827\n",
      "Step = 748 train_loss: 0.0039234688 val_loss: 0.02243624\n",
      "Step = 749 train_loss: 0.0043747597 val_loss: 0.02546173\n",
      "Step = 750 train_loss: 0.0030091167 val_loss: 0.0059561664\n",
      "Step = 751 train_loss: 0.0037838542 val_loss: 0.004246098\n",
      "Step = 752 train_loss: 0.0035426987 val_loss: 0.0076776734\n",
      "Step = 753 train_loss: 0.0048524593 val_loss: 0.008919325\n",
      "Step = 754 train_loss: 0.0036030407 val_loss: 0.005495691\n",
      "Step = 755 train_loss: 0.004222948 val_loss: 0.005519851\n",
      "Step = 756 train_loss: 0.006209868 val_loss: 0.004142218\n",
      "Step = 757 train_loss: 0.0053090896 val_loss: 0.0039939033\n",
      "Step = 758 train_loss: 0.004715004 val_loss: 0.009633605\n",
      "Step = 759 train_loss: 0.0066678417 val_loss: 0.009943177\n",
      "Step = 760 train_loss: 0.0055081565 val_loss: 0.011475353\n",
      "Step = 761 train_loss: 0.0076941247 val_loss: 0.0054622698\n",
      "Step = 762 train_loss: 0.0056158355 val_loss: 0.023595788\n",
      "Step = 763 train_loss: 0.0037904144 val_loss: 0.03954406\n",
      "Step = 764 train_loss: 0.0030597022 val_loss: 0.05961297\n",
      "Step = 765 train_loss: 0.005239171 val_loss: 0.0060315346\n",
      "Step = 766 train_loss: 0.0035906194 val_loss: 0.009363275\n",
      "Step = 767 train_loss: 0.0046573547 val_loss: 0.01529066\n",
      "Step = 768 train_loss: 0.004255195 val_loss: 0.015492629\n",
      "Step = 769 train_loss: 0.004538261 val_loss: 0.0060003954\n",
      "Step = 770 train_loss: 0.0042140624 val_loss: 0.011921747\n",
      "Step = 771 train_loss: 0.0050504482 val_loss: 0.007024173\n",
      "Step = 772 train_loss: 0.003476381 val_loss: 0.011827586\n",
      "Step = 773 train_loss: 0.004588425 val_loss: 0.013356866\n",
      "Step = 774 train_loss: 0.0034026892 val_loss: 0.016198533\n",
      "Step = 775 train_loss: 0.0044688266 val_loss: 0.019652309\n",
      "Step = 776 train_loss: 0.0046593314 val_loss: 0.016413735\n",
      "Step = 777 train_loss: 0.0023189725 val_loss: 0.012827966\n",
      "Step = 778 train_loss: 0.0032679695 val_loss: 0.012209071\n",
      "Step = 779 train_loss: 0.0030452663 val_loss: 0.013003279\n",
      "Step = 780 train_loss: 0.004590654 val_loss: 0.014535927\n",
      "Step = 781 train_loss: 0.0042335885 val_loss: 0.015710784\n",
      "Step = 782 train_loss: 0.0042575314 val_loss: 0.015166625\n",
      "Step = 783 train_loss: 0.005438012 val_loss: 0.012378364\n",
      "Step = 784 train_loss: 0.0058904076 val_loss: 0.012808644\n",
      "Step = 785 train_loss: 0.004944956 val_loss: 0.009568501\n",
      "Step = 786 train_loss: 0.0039958423 val_loss: 0.015392151\n",
      "Step = 787 train_loss: 0.0047597024 val_loss: 0.020635786\n",
      "Step = 788 train_loss: 0.004720015 val_loss: 0.02058187\n",
      "Step = 789 train_loss: 0.004790446 val_loss: 0.009548272\n",
      "Step = 790 train_loss: 0.0035036164 val_loss: 0.019381247\n",
      "Step = 791 train_loss: 0.00532643 val_loss: 0.03851545\n",
      "Step = 792 train_loss: 0.0047627245 val_loss: 0.010025468\n",
      "Step = 793 train_loss: 0.0056932075 val_loss: 0.013659842\n",
      "Step = 794 train_loss: 0.0041529443 val_loss: 0.01958585\n",
      "Step = 795 train_loss: 0.0049544577 val_loss: 0.006860681\n",
      "Step = 796 train_loss: 0.0038787785 val_loss: 0.049907748\n",
      "Step = 797 train_loss: 0.008718177 val_loss: 0.23689465\n",
      "Step = 798 train_loss: 0.0042623123 val_loss: 0.17593381\n",
      "Step = 799 train_loss: 0.0040304465 val_loss: 0.009169616\n",
      "Step = 800 train_loss: 0.003914091 val_loss: 0.012617875\n",
      "Step = 801 train_loss: 0.004119036 val_loss: 0.019949367\n",
      "Step = 802 train_loss: 0.005277894 val_loss: 0.015725851\n",
      "Step = 803 train_loss: 0.003825991 val_loss: 0.008137699\n",
      "Step = 804 train_loss: 0.0038910715 val_loss: 0.007842446\n",
      "Step = 805 train_loss: 0.0044979327 val_loss: 0.006869588\n",
      "Step = 806 train_loss: 0.0035656467 val_loss: 0.0040799063\n",
      "Step = 807 train_loss: 0.0056824535 val_loss: 0.007948027\n",
      "Step = 808 train_loss: 0.004646702 val_loss: 0.025549432\n",
      "Step = 809 train_loss: 0.002666523 val_loss: 0.035280485\n",
      "Step = 810 train_loss: 0.005416718 val_loss: 0.033953086\n",
      "Step = 811 train_loss: 0.004078082 val_loss: 0.04814436\n",
      "Step = 812 train_loss: 0.0035074728 val_loss: 0.02648683\n",
      "Step = 813 train_loss: 0.002716354 val_loss: 0.0058325506\n",
      "Step = 814 train_loss: 0.0023770821 val_loss: 0.008169787\n",
      "Step = 815 train_loss: 0.0050412607 val_loss: 0.005061248\n",
      "Step = 816 train_loss: 0.003657776 val_loss: 0.008452666\n",
      "Step = 817 train_loss: 0.0033697626 val_loss: 0.038368925\n",
      "Step = 818 train_loss: 0.0037260975 val_loss: 0.05194775\n",
      "Step = 819 train_loss: 0.0046158996 val_loss: 0.046652403\n",
      "Step = 820 train_loss: 0.0043795006 val_loss: 0.03771683\n",
      "Step = 821 train_loss: 0.0030660538 val_loss: 0.02102469\n",
      "Step = 822 train_loss: 0.00309163 val_loss: 0.008922634\n",
      "Step = 823 train_loss: 0.0032499444 val_loss: 0.0037045479\n",
      "Step = 824 train_loss: 0.003694206 val_loss: 0.004609165\n",
      "Step = 825 train_loss: 0.0034927889 val_loss: 0.013389269\n",
      "Step = 826 train_loss: 0.003664635 val_loss: 0.022324063\n",
      "Step = 827 train_loss: 0.003410753 val_loss: 0.026743961\n",
      "Step = 828 train_loss: 0.0036955248 val_loss: 0.016750349\n",
      "Step = 829 train_loss: 0.004409057 val_loss: 0.004390257\n",
      "Step = 830 train_loss: 0.004254939 val_loss: 0.008584399\n",
      "Step = 831 train_loss: 0.0039269086 val_loss: 0.011646739\n",
      "Step = 832 train_loss: 0.0028768533 val_loss: 0.011986976\n",
      "Step = 833 train_loss: 0.0025632195 val_loss: 0.010148029\n",
      "Step = 834 train_loss: 0.004034588 val_loss: 0.008517058\n",
      "Step = 835 train_loss: 0.0048680254 val_loss: 0.019171324\n",
      "Step = 836 train_loss: 0.004853504 val_loss: 0.013808348\n",
      "Step = 837 train_loss: 0.00411665 val_loss: 0.012894444\n",
      "Step = 838 train_loss: 0.002596239 val_loss: 0.012780053\n",
      "Step = 839 train_loss: 0.0029667546 val_loss: 0.012507023\n",
      "Step = 840 train_loss: 0.00406844 val_loss: 0.010797301\n",
      "Step = 841 train_loss: 0.00341174 val_loss: 0.010077779\n",
      "Step = 842 train_loss: 0.003232686 val_loss: 0.009840579\n",
      "Step = 843 train_loss: 0.0053927954 val_loss: 0.011482151\n",
      "Step = 844 train_loss: 0.005956307 val_loss: 0.020404063\n",
      "Step = 845 train_loss: 0.003946377 val_loss: 0.03493736\n",
      "Step = 846 train_loss: 0.0025501347 val_loss: 0.04827933\n",
      "Step = 847 train_loss: 0.002734164 val_loss: 0.024594376\n",
      "Step = 848 train_loss: 0.0031309773 val_loss: 0.013780996\n",
      "Step = 849 train_loss: 0.0033687302 val_loss: 0.01203434\n",
      "Step = 850 train_loss: 0.0037398466 val_loss: 0.012052601\n",
      "Step = 851 train_loss: 0.00429787 val_loss: 0.008292158\n",
      "Step = 852 train_loss: 0.0025957215 val_loss: 0.0071680704\n",
      "Step = 853 train_loss: 0.003861363 val_loss: 0.02223639\n",
      "Step = 854 train_loss: 0.0035007345 val_loss: 0.06705189\n",
      "Step = 855 train_loss: 0.0035147818 val_loss: 0.036597762\n",
      "Step = 856 train_loss: 0.0024761644 val_loss: 0.0029218746\n",
      "Step = 857 train_loss: 0.0023990518 val_loss: 0.018861784\n",
      "Step = 858 train_loss: 0.003234968 val_loss: 0.025085464\n",
      "Step = 859 train_loss: 0.0041175415 val_loss: 0.024633972\n",
      "Step = 860 train_loss: 0.0027941691 val_loss: 0.021946315\n",
      "Step = 861 train_loss: 0.0031878734 val_loss: 0.021008858\n",
      "Step = 862 train_loss: 0.0033304603 val_loss: 0.019957272\n",
      "Step = 863 train_loss: 0.003563237 val_loss: 0.004169196\n",
      "Step = 864 train_loss: 0.0042391685 val_loss: 0.07872886\n",
      "Step = 865 train_loss: 0.0036088217 val_loss: 0.28011686\n",
      "Step = 866 train_loss: 0.0035624057 val_loss: 0.385965\n",
      "Step = 867 train_loss: 0.005419197 val_loss: 0.16478027\n",
      "Step = 868 train_loss: 0.0047221524 val_loss: 0.02099952\n",
      "Step = 869 train_loss: 0.0032125434 val_loss: 0.015963797\n",
      "Step = 870 train_loss: 0.0031924557 val_loss: 0.022444125\n",
      "Step = 871 train_loss: 0.0029824367 val_loss: 0.020851297\n",
      "Step = 872 train_loss: 0.0038209807 val_loss: 0.01777381\n",
      "Step = 873 train_loss: 0.004784541 val_loss: 0.012619851\n",
      "Step = 874 train_loss: 0.0036825782 val_loss: 0.00454367\n",
      "Step = 875 train_loss: 0.0023541634 val_loss: 0.012827839\n",
      "Step = 876 train_loss: 0.0022502218 val_loss: 0.040080745\n",
      "Step = 877 train_loss: 0.0036225696 val_loss: 0.045672342\n",
      "Step = 878 train_loss: 0.004018484 val_loss: 0.007853521\n",
      "Step = 879 train_loss: 0.00323249 val_loss: 0.011919479\n",
      "Step = 880 train_loss: 0.0045019495 val_loss: 0.016621806\n",
      "Step = 881 train_loss: 0.004519061 val_loss: 0.017476747\n",
      "Step = 882 train_loss: 0.0034442118 val_loss: 0.017333081\n",
      "Step = 883 train_loss: 0.0031639936 val_loss: 0.011966194\n",
      "Step = 884 train_loss: 0.0035593729 val_loss: 0.0061539127\n",
      "Step = 885 train_loss: 0.005864313 val_loss: 0.0028925776\n",
      "Step = 886 train_loss: 0.005741479 val_loss: 0.015058347\n",
      "Step = 887 train_loss: 0.0034744842 val_loss: 0.02937382\n",
      "Step = 888 train_loss: 0.0050047114 val_loss: 0.026446452\n",
      "Step = 889 train_loss: 0.0032123844 val_loss: 0.009895101\n",
      "Step = 890 train_loss: 0.0026025726 val_loss: 0.0064759115\n",
      "Step = 891 train_loss: 0.004058769 val_loss: 0.015346594\n",
      "Step = 892 train_loss: 0.0027951077 val_loss: 0.0173744\n",
      "Step = 893 train_loss: 0.003917756 val_loss: 0.015973201\n",
      "Step = 894 train_loss: 0.0048942836 val_loss: 0.005461098\n",
      "Step = 895 train_loss: 0.0038755627 val_loss: 0.039831813\n",
      "Step = 896 train_loss: 0.0024257563 val_loss: 0.11681478\n",
      "Step = 897 train_loss: 0.0042023407 val_loss: 0.14209151\n",
      "Step = 898 train_loss: 0.00450585 val_loss: 0.12173777\n",
      "Step = 899 train_loss: 0.0029733342 val_loss: 0.02672694\n",
      "Step = 900 train_loss: 0.002170177 val_loss: 0.004531626\n",
      "Step = 901 train_loss: 0.0025700268 val_loss: 0.0077978554\n",
      "Step = 902 train_loss: 0.0026282049 val_loss: 0.015678369\n",
      "Step = 903 train_loss: 0.0033474457 val_loss: 0.018716477\n",
      "Step = 904 train_loss: 0.0025223084 val_loss: 0.018639382\n",
      "Step = 905 train_loss: 0.0039404966 val_loss: 0.013196168\n",
      "Step = 906 train_loss: 0.0031080444 val_loss: 0.007503892\n",
      "Step = 907 train_loss: 0.0036542648 val_loss: 0.098211676\n",
      "Step = 908 train_loss: 0.0043883603 val_loss: 0.23255704\n",
      "Step = 909 train_loss: 0.0024023617 val_loss: 0.26054645\n",
      "Step = 910 train_loss: 0.0036537203 val_loss: 0.13600798\n",
      "Step = 911 train_loss: 0.0037965658 val_loss: 0.017124051\n",
      "Step = 912 train_loss: 0.002702856 val_loss: 0.01580004\n",
      "Step = 913 train_loss: 0.0045607937 val_loss: 0.019700035\n",
      "Step = 914 train_loss: 0.0032864176 val_loss: 0.020825706\n",
      "Step = 915 train_loss: 0.0024450135 val_loss: 0.021024404\n",
      "Step = 916 train_loss: 0.004301707 val_loss: 0.020665983\n",
      "Step = 917 train_loss: 0.0028981955 val_loss: 0.019022256\n",
      "Step = 918 train_loss: 0.004780571 val_loss: 0.011929984\n",
      "Step = 919 train_loss: 0.0034300187 val_loss: 0.004006925\n",
      "Step = 920 train_loss: 0.0035903635 val_loss: 0.035577044\n",
      "Step = 921 train_loss: 0.0032217698 val_loss: 0.09242044\n",
      "Step = 922 train_loss: 0.0038245986 val_loss: 0.08085629\n",
      "Step = 923 train_loss: 0.0039805896 val_loss: 0.049114354\n",
      "Step = 924 train_loss: 0.0037481985 val_loss: 0.0053220685\n",
      "Step = 925 train_loss: 0.0031306755 val_loss: 0.006606624\n",
      "Step = 926 train_loss: 0.0022802236 val_loss: 0.014588335\n",
      "Step = 927 train_loss: 0.0030473717 val_loss: 0.014350484\n",
      "Step = 928 train_loss: 0.0037337495 val_loss: 0.01052629\n",
      "Step = 929 train_loss: 0.0021693883 val_loss: 0.007550004\n",
      "Step = 930 train_loss: 0.0027796403 val_loss: 0.005763084\n",
      "Step = 931 train_loss: 0.0031906255 val_loss: 0.004121787\n",
      "Step = 932 train_loss: 0.0046935226 val_loss: 0.0066762855\n",
      "Step = 933 train_loss: 0.005112753 val_loss: 0.010129863\n",
      "Step = 934 train_loss: 0.0022527464 val_loss: 0.016822811\n",
      "Step = 935 train_loss: 0.0035895233 val_loss: 0.01916321\n",
      "Step = 936 train_loss: 0.0026308936 val_loss: 0.007748095\n",
      "Step = 937 train_loss: 0.0036361495 val_loss: 0.007956834\n",
      "Step = 938 train_loss: 0.003109301 val_loss: 0.016178537\n",
      "Step = 939 train_loss: 0.0031083298 val_loss: 0.020345718\n",
      "Step = 940 train_loss: 0.0035842045 val_loss: 0.012009971\n",
      "Step = 941 train_loss: 0.003418626 val_loss: 0.011277402\n",
      "Step = 942 train_loss: 0.0034728432 val_loss: 0.012172062\n",
      "Step = 943 train_loss: 0.0027750933 val_loss: 0.011549545\n",
      "Step = 944 train_loss: 0.004202056 val_loss: 0.012250279\n",
      "Step = 945 train_loss: 0.004028486 val_loss: 0.012664313\n",
      "Step = 946 train_loss: 0.002821947 val_loss: 0.0126057165\n",
      "Step = 947 train_loss: 0.0022267657 val_loss: 0.012656008\n",
      "Step = 948 train_loss: 0.0034939966 val_loss: 0.012823426\n",
      "Step = 949 train_loss: 0.0030157962 val_loss: 0.012695084\n",
      "Step = 950 train_loss: 0.003378174 val_loss: 0.012071767\n",
      "Step = 951 train_loss: 0.0042753113 val_loss: 0.012202186\n",
      "Step = 952 train_loss: 0.0031203171 val_loss: 0.010598071\n",
      "Step = 953 train_loss: 0.004586431 val_loss: 0.009766636\n",
      "Step = 954 train_loss: 0.0026453235 val_loss: 0.008650058\n",
      "Step = 955 train_loss: 0.0025366086 val_loss: 0.009740371\n",
      "Step = 956 train_loss: 0.0036286344 val_loss: 0.011740349\n",
      "Step = 957 train_loss: 0.0024531633 val_loss: 0.016474128\n",
      "Step = 958 train_loss: 0.0033644584 val_loss: 0.01732247\n",
      "Step = 959 train_loss: 0.0052017984 val_loss: 0.009563372\n",
      "Step = 960 train_loss: 0.003073809 val_loss: 0.005172723\n",
      "Step = 961 train_loss: 0.0040886295 val_loss: 0.03327948\n",
      "Step = 962 train_loss: 0.004905495 val_loss: 0.04636158\n",
      "Step = 963 train_loss: 0.0045474814 val_loss: 0.010458454\n",
      "Step = 964 train_loss: 0.003347163 val_loss: 0.013429521\n",
      "Step = 965 train_loss: 0.0036261184 val_loss: 0.020948464\n",
      "Step = 966 train_loss: 0.0052925493 val_loss: 0.022707427\n",
      "Step = 967 train_loss: 0.0045166085 val_loss: 0.01972689\n",
      "Step = 968 train_loss: 0.0047133835 val_loss: 0.0336372\n",
      "Step = 969 train_loss: 0.003599366 val_loss: 0.2725473\n",
      "Step = 970 train_loss: 0.0029699644 val_loss: 0.3065046\n",
      "Step = 971 train_loss: 0.0032728987 val_loss: 0.12446155\n",
      "Step = 972 train_loss: 0.0025596865 val_loss: 0.0031707105\n",
      "Step = 973 train_loss: 0.0029861121 val_loss: 0.020519318\n",
      "Step = 974 train_loss: 0.0039050689 val_loss: 0.022342367\n",
      "Step = 975 train_loss: 0.0030886452 val_loss: 0.023208898\n",
      "Step = 976 train_loss: 0.0023890713 val_loss: 0.022062464\n",
      "Step = 977 train_loss: 0.003874574 val_loss: 0.01659472\n",
      "Step = 978 train_loss: 0.002577479 val_loss: 0.00930078\n",
      "Step = 979 train_loss: 0.0032338016 val_loss: 0.010181536\n",
      "Step = 980 train_loss: 0.0040422347 val_loss: 0.029839499\n",
      "Step = 981 train_loss: 0.0038600178 val_loss: 0.03458201\n",
      "Step = 982 train_loss: 0.0031669554 val_loss: 0.029554378\n",
      "Step = 983 train_loss: 0.0035977524 val_loss: 0.011545739\n",
      "Step = 984 train_loss: 0.0030256761 val_loss: 0.014283636\n",
      "Step = 985 train_loss: 0.0036317916 val_loss: 0.019207882\n",
      "Step = 986 train_loss: 0.0026599008 val_loss: 0.017889298\n",
      "Step = 987 train_loss: 0.0041318503 val_loss: 0.015252456\n",
      "Step = 988 train_loss: 0.0025809905 val_loss: 0.01177283\n",
      "Step = 989 train_loss: 0.002709622 val_loss: 0.010402083\n",
      "Step = 990 train_loss: 0.0019539339 val_loss: 0.025088156\n",
      "Step = 991 train_loss: 0.002747711 val_loss: 0.016584797\n",
      "Step = 992 train_loss: 0.0059585515 val_loss: 0.005867669\n",
      "Step = 993 train_loss: 0.0024624204 val_loss: 0.006796018\n",
      "Step = 994 train_loss: 0.0049892017 val_loss: 0.0059849266\n",
      "Step = 995 train_loss: 0.0019302315 val_loss: 0.0040901476\n",
      "Step = 996 train_loss: 0.0032315156 val_loss: 0.004975173\n",
      "Step = 997 train_loss: 0.002723885 val_loss: 0.009848909\n",
      "Step = 998 train_loss: 0.0029039495 val_loss: 0.0088167\n",
      "Step = 999 train_loss: 0.0031741061 val_loss: 0.00677789\n",
      "885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_29860\\2290839578.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_29860\\2290839578.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "val_losses=[]\n",
    "\n",
    "for t in range(1000):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    '''save all the model in each epoch'''    \n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, 'Target_model/net_parameters'+str(t)+'.pkl')\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print ('Step = %d' % t, 'train_loss:',train_loss.data.numpy(),'val_loss:',val_loss.data.numpy())\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "   \n",
    "'''find the best model'''\n",
    "best_index=val_losses.index(np.min(val_losses))\n",
    "print(best_index)\n",
    "\n",
    "'''copy the best model from model file to the model_test file'''\n",
    "if 1:\n",
    "    shutil.copyfile('Target_model/net_parameters'+str(best_index)+'.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "'''Reload the best model''' \n",
    "model = Net()\n",
    "a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
    "optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.15325734\n",
      "mpe_val: 0.15576191\n",
      "mpe_a: 0.1499941462482969\n",
      "mpe_b: 0.16038611912337858\n",
      "rmse_train: 116.59941\n",
      "rmse_val: 120.13955\n",
      "rmse_a: 122.02010890318114\n",
      "rmse_b: 198.2659199156527\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.9062628746032715\n",
      "r2_val: 0.8613404631614685\n",
      "r2_a: 0.9024388593844427\n",
      "r2_b: 0.5164209698966113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
