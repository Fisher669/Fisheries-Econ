{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n",
    "\n",
    "'''Define the CNN model''' \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1=torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d(1, 8, kernel_size=(50,50), stride=(5,40)),\n",
    "                        torch.nn.BatchNorm2d(8),\n",
    "                        torch.nn.LeakyReLU(inplace=True),\n",
    "                        torch.nn.MaxPool2d(kernel_size=2, stride=1))\n",
    "\n",
    "        self.layer2=torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d(8, 16, kernel_size=(2,3), stride=1),\n",
    "                        torch.nn.BatchNorm2d(16),\n",
    "                        torch.nn.LeakyReLU(inplace=True))\n",
    "        \n",
    "        self.layer3=torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d(16, 32, kernel_size=(2,3), stride=1),\n",
    "                        torch.nn.BatchNorm2d(32),\n",
    "                        torch.nn.LeakyReLU(inplace=True))\n",
    "        \n",
    "        self.layer4=torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d(32, 64, kernel_size=(2,2), stride=1),\n",
    "                        torch.nn.BatchNorm2d(64),\n",
    "                        torch.nn.LeakyReLU(inplace=True))\n",
    "        \n",
    "        self.layer5=torch.nn.Sequential(\n",
    "                        torch.nn.Conv2d(64, 64, kernel_size=(1,2), stride=1),\n",
    "                        torch.nn.BatchNorm2d(64),\n",
    "                        torch.nn.LeakyReLU(inplace=True))   \n",
    "        self.drop_layer1 = torch.nn.Dropout(p=0.2)\n",
    "        self.drop_layer3 = torch.nn.Dropout(p=0.4)\n",
    "        self.drop_layer4 = torch.nn.Dropout(p=0.4)\n",
    "        self.drop_layer5 = torch.nn.Dropout(p=0.2)\n",
    "        self.drop_layer6 = torch.nn.Dropout(p=0.2)\n",
    "        self.drop_layer7 = torch.nn.Dropout(p=0.4)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(4864,100)\n",
    "        self.fc2 = torch.nn.Linear(100,1)       \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)     \n",
    "        x = self.layer3(x)                   \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.drop_layer6(x)                     \n",
    "        x = torch.sigmoid(self.fc2(x))        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  )\n",
      "  (drop_layer1): Dropout(p=0.2, inplace=False)\n",
      "  (drop_layer3): Dropout(p=0.4, inplace=False)\n",
      "  (drop_layer4): Dropout(p=0.4, inplace=False)\n",
      "  (drop_layer5): Dropout(p=0.2, inplace=False)\n",
      "  (drop_layer6): Dropout(p=0.2, inplace=False)\n",
      "  (drop_layer7): Dropout(p=0.4, inplace=False)\n",
      "  (fc1): Linear(in_features=4864, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 11, 24]          20,008\n",
      "       BatchNorm2d-2            [-1, 8, 11, 24]              16\n",
      "         LeakyReLU-3            [-1, 8, 11, 24]               0\n",
      "         MaxPool2d-4            [-1, 8, 10, 23]               0\n",
      "            Conv2d-5            [-1, 16, 9, 21]             784\n",
      "       BatchNorm2d-6            [-1, 16, 9, 21]              32\n",
      "         LeakyReLU-7            [-1, 16, 9, 21]               0\n",
      "            Conv2d-8            [-1, 32, 8, 19]           3,104\n",
      "       BatchNorm2d-9            [-1, 32, 8, 19]              64\n",
      "        LeakyReLU-10            [-1, 32, 8, 19]               0\n",
      "           Linear-11                  [-1, 100]         486,500\n",
      "          Dropout-12                  [-1, 100]               0\n",
      "           Linear-13                    [-1, 1]             101\n",
      "================================================================\n",
      "Total params: 510,609\n",
      "Trainable params: 510,609\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 0.24\n",
      "Params size (MB): 1.95\n",
      "Estimated Total Size (MB): 2.57\n",
      "----------------------------------------------------------------\n",
      "Step = 0 train_loss: 0.08637022 val_loss: 0.052425697\n",
      "Step = 1 train_loss: 0.07974289 val_loss: 0.0529033\n",
      "Step = 2 train_loss: 0.06698538 val_loss: 0.05339349\n",
      "Step = 3 train_loss: 0.058654014 val_loss: 0.05389168\n",
      "Step = 4 train_loss: 0.052816704 val_loss: 0.054176893\n",
      "Step = 5 train_loss: 0.052187223 val_loss: 0.054103788\n",
      "Step = 6 train_loss: 0.048540648 val_loss: 0.05391057\n",
      "Step = 7 train_loss: 0.042186376 val_loss: 0.053575415\n",
      "Step = 8 train_loss: 0.038934205 val_loss: 0.053072564\n",
      "Step = 9 train_loss: 0.038687654 val_loss: 0.052474707\n",
      "Step = 10 train_loss: 0.038913943 val_loss: 0.051674772\n",
      "Step = 11 train_loss: 0.03410822 val_loss: 0.05070826\n",
      "Step = 12 train_loss: 0.031834334 val_loss: 0.049741548\n",
      "Step = 13 train_loss: 0.031048043 val_loss: 0.04872084\n",
      "Step = 14 train_loss: 0.0323293 val_loss: 0.047778863\n",
      "Step = 15 train_loss: 0.03232713 val_loss: 0.046891715\n",
      "Step = 16 train_loss: 0.028791364 val_loss: 0.045966465\n",
      "Step = 17 train_loss: 0.029130815 val_loss: 0.045142274\n",
      "Step = 18 train_loss: 0.028202029 val_loss: 0.044533484\n",
      "Step = 19 train_loss: 0.032157596 val_loss: 0.043870844\n",
      "Step = 20 train_loss: 0.029086692 val_loss: 0.04327935\n",
      "Step = 21 train_loss: 0.029664014 val_loss: 0.042655952\n",
      "Step = 22 train_loss: 0.028012343 val_loss: 0.0419502\n",
      "Step = 23 train_loss: 0.025332274 val_loss: 0.041248366\n",
      "Step = 24 train_loss: 0.031611856 val_loss: 0.040683858\n",
      "Step = 25 train_loss: 0.029989563 val_loss: 0.040208172\n",
      "Step = 26 train_loss: 0.027229628 val_loss: 0.039879255\n",
      "Step = 27 train_loss: 0.029659271 val_loss: 0.0396028\n",
      "Step = 28 train_loss: 0.02812648 val_loss: 0.039406106\n",
      "Step = 29 train_loss: 0.02976226 val_loss: 0.039193574\n",
      "Step = 30 train_loss: 0.028871039 val_loss: 0.039021716\n",
      "Step = 31 train_loss: 0.029816542 val_loss: 0.038764186\n",
      "Step = 32 train_loss: 0.023940746 val_loss: 0.038651034\n",
      "Step = 33 train_loss: 0.032099597 val_loss: 0.03759861\n",
      "Step = 34 train_loss: 0.0255296 val_loss: 0.03683929\n",
      "Step = 35 train_loss: 0.02285146 val_loss: 0.03635117\n",
      "Step = 36 train_loss: 0.024024757 val_loss: 0.035995416\n",
      "Step = 37 train_loss: 0.026782177 val_loss: 0.03571339\n",
      "Step = 38 train_loss: 0.0234717 val_loss: 0.035277817\n",
      "Step = 39 train_loss: 0.028149687 val_loss: 0.03502911\n",
      "Step = 40 train_loss: 0.02461239 val_loss: 0.03413845\n",
      "Step = 41 train_loss: 0.026658487 val_loss: 0.033176277\n",
      "Step = 42 train_loss: 0.026739389 val_loss: 0.031826846\n",
      "Step = 43 train_loss: 0.025627125 val_loss: 0.030816311\n",
      "Step = 44 train_loss: 0.02507155 val_loss: 0.030300207\n",
      "Step = 45 train_loss: 0.025891643 val_loss: 0.029665772\n",
      "Step = 46 train_loss: 0.026895795 val_loss: 0.029000241\n",
      "Step = 47 train_loss: 0.02959056 val_loss: 0.028451966\n",
      "Step = 48 train_loss: 0.024883658 val_loss: 0.027901448\n",
      "Step = 49 train_loss: 0.026453387 val_loss: 0.0273545\n",
      "Step = 50 train_loss: 0.022058977 val_loss: 0.026999477\n",
      "Step = 51 train_loss: 0.025832236 val_loss: 0.027138233\n",
      "Step = 52 train_loss: 0.02639654 val_loss: 0.027529748\n",
      "Step = 53 train_loss: 0.02443966 val_loss: 0.027689576\n",
      "Step = 54 train_loss: 0.024982683 val_loss: 0.027583038\n",
      "Step = 55 train_loss: 0.025156986 val_loss: 0.027748404\n",
      "Step = 56 train_loss: 0.023803515 val_loss: 0.02841416\n",
      "Step = 57 train_loss: 0.025294926 val_loss: 0.028940719\n",
      "Step = 58 train_loss: 0.022094978 val_loss: 0.028631724\n",
      "Step = 59 train_loss: 0.021557966 val_loss: 0.028704602\n",
      "Step = 60 train_loss: 0.020309862 val_loss: 0.030677682\n",
      "Step = 61 train_loss: 0.026632197 val_loss: 0.036979154\n",
      "Step = 62 train_loss: 0.023918392 val_loss: 0.048319172\n",
      "Step = 63 train_loss: 0.023371907 val_loss: 0.055116728\n",
      "Step = 64 train_loss: 0.022795737 val_loss: 0.055888515\n",
      "Step = 65 train_loss: 0.02167503 val_loss: 0.051025983\n",
      "Step = 66 train_loss: 0.02073765 val_loss: 0.040710542\n",
      "Step = 67 train_loss: 0.018948924 val_loss: 0.036260348\n",
      "Step = 68 train_loss: 0.018247675 val_loss: 0.03700999\n",
      "Step = 69 train_loss: 0.017899016 val_loss: 0.038119547\n",
      "Step = 70 train_loss: 0.025063755 val_loss: 0.039108645\n",
      "Step = 71 train_loss: 0.017737206 val_loss: 0.05019981\n",
      "Step = 72 train_loss: 0.014367672 val_loss: 0.060311086\n",
      "Step = 73 train_loss: 0.016184798 val_loss: 0.06401673\n",
      "Step = 74 train_loss: 0.017132608 val_loss: 0.06492266\n",
      "Step = 75 train_loss: 0.017237721 val_loss: 0.063888215\n",
      "Step = 76 train_loss: 0.01888489 val_loss: 0.064794905\n",
      "Step = 77 train_loss: 0.016620781 val_loss: 0.06652116\n",
      "Step = 78 train_loss: 0.016983481 val_loss: 0.06358697\n",
      "Step = 79 train_loss: 0.014448202 val_loss: 0.06083755\n",
      "Step = 80 train_loss: 0.013005363 val_loss: 0.05082662\n",
      "Step = 81 train_loss: 0.010713283 val_loss: 0.04418563\n",
      "Step = 82 train_loss: 0.017981319 val_loss: 0.041608382\n",
      "Step = 83 train_loss: 0.010063793 val_loss: 0.04074758\n",
      "Step = 84 train_loss: 0.010558669 val_loss: 0.03279865\n",
      "Step = 85 train_loss: 0.013530298 val_loss: 0.022531604\n",
      "Step = 86 train_loss: 0.012843405 val_loss: 0.023266874\n",
      "Step = 87 train_loss: 0.0145549225 val_loss: 0.04272231\n",
      "Step = 88 train_loss: 0.012281556 val_loss: 0.061050892\n",
      "Step = 89 train_loss: 0.011468828 val_loss: 0.089262314\n",
      "Step = 90 train_loss: 0.009883631 val_loss: 0.11042379\n",
      "Step = 91 train_loss: 0.009595751 val_loss: 0.11537484\n",
      "Step = 92 train_loss: 0.010880088 val_loss: 0.11762782\n",
      "Step = 93 train_loss: 0.00973929 val_loss: 0.11498321\n",
      "Step = 94 train_loss: 0.009705878 val_loss: 0.102079056\n",
      "Step = 95 train_loss: 0.009533284 val_loss: 0.057135507\n",
      "Step = 96 train_loss: 0.00916199 val_loss: 0.025814869\n",
      "Step = 97 train_loss: 0.010964825 val_loss: 0.01832353\n",
      "Step = 98 train_loss: 0.009921105 val_loss: 0.02371406\n",
      "Step = 99 train_loss: 0.010928748 val_loss: 0.041900788\n",
      "Step = 100 train_loss: 0.008853769 val_loss: 0.06834839\n",
      "Step = 101 train_loss: 0.01154923 val_loss: 0.09680617\n",
      "Step = 102 train_loss: 0.008595803 val_loss: 0.11338313\n",
      "Step = 103 train_loss: 0.007850395 val_loss: 0.11792471\n",
      "Step = 104 train_loss: 0.008622588 val_loss: 0.11450674\n",
      "Step = 105 train_loss: 0.011954694 val_loss: 0.10601361\n",
      "Step = 106 train_loss: 0.007846802 val_loss: 0.07919579\n",
      "Step = 107 train_loss: 0.00822171 val_loss: 0.05695827\n",
      "Step = 108 train_loss: 0.0075816237 val_loss: 0.03384775\n",
      "Step = 109 train_loss: 0.008806908 val_loss: 0.024776692\n",
      "Step = 110 train_loss: 0.0061437944 val_loss: 0.021746082\n",
      "Step = 111 train_loss: 0.006057374 val_loss: 0.016510446\n",
      "Step = 112 train_loss: 0.0073255342 val_loss: 0.021062955\n",
      "Step = 113 train_loss: 0.0069875997 val_loss: 0.020399295\n",
      "Step = 114 train_loss: 0.006400906 val_loss: 0.018082239\n",
      "Step = 115 train_loss: 0.0078341905 val_loss: 0.013742341\n",
      "Step = 116 train_loss: 0.007407929 val_loss: 0.018761732\n",
      "Step = 117 train_loss: 0.007212398 val_loss: 0.026907796\n",
      "Step = 118 train_loss: 0.0058140233 val_loss: 0.02611312\n",
      "Step = 119 train_loss: 0.006815296 val_loss: 0.015837347\n",
      "Step = 120 train_loss: 0.0064125406 val_loss: 0.005614778\n",
      "Step = 121 train_loss: 0.0071574906 val_loss: 0.009910873\n",
      "Step = 122 train_loss: 0.006340411 val_loss: 0.012159644\n",
      "Step = 123 train_loss: 0.005112027 val_loss: 0.013598569\n",
      "Step = 124 train_loss: 0.006646745 val_loss: 0.013916623\n",
      "Step = 125 train_loss: 0.0065402174 val_loss: 0.011969482\n",
      "Step = 126 train_loss: 0.00841502 val_loss: 0.012129387\n",
      "Step = 127 train_loss: 0.006150562 val_loss: 0.025049672\n",
      "Step = 128 train_loss: 0.0048441584 val_loss: 0.057031725\n",
      "Step = 129 train_loss: 0.0052640354 val_loss: 0.05143607\n",
      "Step = 130 train_loss: 0.00520283 val_loss: 0.018793503\n",
      "Step = 131 train_loss: 0.005369978 val_loss: 0.007271172\n",
      "Step = 132 train_loss: 0.006301426 val_loss: 0.02163188\n",
      "Step = 133 train_loss: 0.0056579537 val_loss: 0.020933036\n",
      "Step = 134 train_loss: 0.0049943696 val_loss: 0.024505109\n",
      "Step = 135 train_loss: 0.006502802 val_loss: 0.010701871\n",
      "Step = 136 train_loss: 0.00565592 val_loss: 0.015880391\n",
      "Step = 137 train_loss: 0.004807664 val_loss: 0.08113135\n",
      "Step = 138 train_loss: 0.005226428 val_loss: 0.114985466\n",
      "Step = 139 train_loss: 0.0071033044 val_loss: 0.07758248\n",
      "Step = 140 train_loss: 0.005950732 val_loss: 0.013025849\n",
      "Step = 141 train_loss: 0.004214324 val_loss: 0.009555398\n",
      "Step = 142 train_loss: 0.005940987 val_loss: 0.025605934\n",
      "Step = 143 train_loss: 0.0072835954 val_loss: 0.029893186\n",
      "Step = 144 train_loss: 0.006192203 val_loss: 0.02769112\n",
      "Step = 145 train_loss: 0.005128606 val_loss: 0.019283503\n",
      "Step = 146 train_loss: 0.005234031 val_loss: 0.004433895\n",
      "Step = 147 train_loss: 0.0037632585 val_loss: 0.025475392\n",
      "Step = 148 train_loss: 0.00528387 val_loss: 0.08956441\n",
      "Step = 149 train_loss: 0.004894072 val_loss: 0.09519323\n",
      "Step = 150 train_loss: 0.0049461355 val_loss: 0.03530172\n",
      "Step = 151 train_loss: 0.0045619616 val_loss: 0.0065030186\n",
      "Step = 152 train_loss: 0.004756083 val_loss: 0.008137823\n",
      "Step = 153 train_loss: 0.0043297512 val_loss: 0.009925951\n",
      "Step = 154 train_loss: 0.0049084965 val_loss: 0.012577526\n",
      "Step = 155 train_loss: 0.0055623236 val_loss: 0.0107143205\n",
      "Step = 156 train_loss: 0.0034489227 val_loss: 0.0077338917\n",
      "Step = 157 train_loss: 0.0043038647 val_loss: 0.004118521\n",
      "Step = 158 train_loss: 0.003921167 val_loss: 0.009550477\n",
      "Step = 159 train_loss: 0.0043142596 val_loss: 0.028189246\n",
      "Step = 160 train_loss: 0.0065152803 val_loss: 0.016192632\n",
      "Step = 161 train_loss: 0.0042006234 val_loss: 0.0037009814\n",
      "Step = 162 train_loss: 0.0033812919 val_loss: 0.013220673\n",
      "Step = 163 train_loss: 0.0033926654 val_loss: 0.022867614\n",
      "Step = 164 train_loss: 0.0039936705 val_loss: 0.026567154\n",
      "Step = 165 train_loss: 0.003611567 val_loss: 0.022357382\n",
      "Step = 166 train_loss: 0.0046796952 val_loss: 0.017538864\n",
      "Step = 167 train_loss: 0.004022234 val_loss: 0.007368133\n",
      "Step = 168 train_loss: 0.0036864774 val_loss: 0.0041116294\n",
      "Step = 169 train_loss: 0.004256516 val_loss: 0.008245209\n",
      "Step = 170 train_loss: 0.0038077687 val_loss: 0.004817671\n",
      "Step = 171 train_loss: 0.0029460075 val_loss: 0.004255842\n",
      "Step = 172 train_loss: 0.004249524 val_loss: 0.0033321918\n",
      "Step = 173 train_loss: 0.0037629397 val_loss: 0.006257599\n",
      "Step = 174 train_loss: 0.0034314112 val_loss: 0.0034117876\n",
      "Step = 175 train_loss: 0.0032596136 val_loss: 0.0073240916\n",
      "Step = 176 train_loss: 0.003829476 val_loss: 0.010376489\n",
      "Step = 177 train_loss: 0.0034431904 val_loss: 0.01121091\n",
      "Step = 178 train_loss: 0.0037628415 val_loss: 0.009201462\n",
      "Step = 179 train_loss: 0.0038379012 val_loss: 0.0032720752\n",
      "Step = 180 train_loss: 0.0030770795 val_loss: 0.015698986\n",
      "Step = 181 train_loss: 0.0034988872 val_loss: 0.06544804\n",
      "Step = 182 train_loss: 0.0032008712 val_loss: 0.08094965\n",
      "Step = 183 train_loss: 0.004583807 val_loss: 0.029267762\n",
      "Step = 184 train_loss: 0.002614407 val_loss: 0.010170212\n",
      "Step = 185 train_loss: 0.0026795517 val_loss: 0.0034372658\n",
      "Step = 186 train_loss: 0.0032995085 val_loss: 0.016016694\n",
      "Step = 187 train_loss: 0.0039947326 val_loss: 0.018614925\n",
      "Step = 188 train_loss: 0.0031985317 val_loss: 0.013344673\n",
      "Step = 189 train_loss: 0.0030582605 val_loss: 0.004116738\n",
      "Step = 190 train_loss: 0.0029144066 val_loss: 0.005232628\n",
      "Step = 191 train_loss: 0.003621764 val_loss: 0.024035795\n",
      "Step = 192 train_loss: 0.002991601 val_loss: 0.062330153\n",
      "Step = 193 train_loss: 0.002593941 val_loss: 0.08116901\n",
      "Step = 194 train_loss: 0.004194978 val_loss: 0.026872002\n",
      "Step = 195 train_loss: 0.0030473373 val_loss: 0.00505392\n",
      "Step = 196 train_loss: 0.003726157 val_loss: 0.008335333\n",
      "Step = 197 train_loss: 0.0028982882 val_loss: 0.02246287\n",
      "Step = 198 train_loss: 0.0042488673 val_loss: 0.023919294\n",
      "Step = 199 train_loss: 0.0035941734 val_loss: 0.017718332\n",
      "Step = 200 train_loss: 0.0028729315 val_loss: 0.008358233\n",
      "Step = 201 train_loss: 0.0035771532 val_loss: 0.0026934994\n",
      "Step = 202 train_loss: 0.0021140222 val_loss: 0.0044991532\n",
      "Step = 203 train_loss: 0.0027020397 val_loss: 0.018760385\n",
      "Step = 204 train_loss: 0.0027868156 val_loss: 0.028972402\n",
      "Step = 205 train_loss: 0.0029676547 val_loss: 0.024881052\n",
      "Step = 206 train_loss: 0.003328299 val_loss: 0.004105238\n",
      "Step = 207 train_loss: 0.0028542804 val_loss: 0.0040958524\n",
      "Step = 208 train_loss: 0.0032966307 val_loss: 0.0061440067\n",
      "Step = 209 train_loss: 0.0036773349 val_loss: 0.011901542\n",
      "Step = 210 train_loss: 0.0022038121 val_loss: 0.01523332\n",
      "Step = 211 train_loss: 0.0029354044 val_loss: 0.017776284\n",
      "Step = 212 train_loss: 0.0026626626 val_loss: 0.016484872\n",
      "Step = 213 train_loss: 0.003466247 val_loss: 0.008122751\n",
      "Step = 214 train_loss: 0.0023501497 val_loss: 0.005423905\n",
      "Step = 215 train_loss: 0.0034360888 val_loss: 0.0432698\n",
      "Step = 216 train_loss: 0.003802109 val_loss: 0.04660505\n",
      "Step = 217 train_loss: 0.0027408486 val_loss: 0.026257796\n",
      "Step = 218 train_loss: 0.0027744768 val_loss: 0.0066695744\n",
      "Step = 219 train_loss: 0.002857666 val_loss: 0.0031113247\n",
      "Step = 220 train_loss: 0.0034664816 val_loss: 0.004284487\n",
      "Step = 221 train_loss: 0.0033666377 val_loss: 0.0034742523\n",
      "Step = 222 train_loss: 0.0030389947 val_loss: 0.010374513\n",
      "Step = 223 train_loss: 0.002610389 val_loss: 0.019576957\n",
      "Step = 224 train_loss: 0.0027323451 val_loss: 0.0224983\n",
      "Step = 225 train_loss: 0.003520455 val_loss: 0.013716467\n",
      "Step = 226 train_loss: 0.0027209397 val_loss: 0.0023490281\n",
      "Step = 227 train_loss: 0.0032270646 val_loss: 0.003608608\n",
      "Step = 228 train_loss: 0.002093464 val_loss: 0.014898935\n",
      "Step = 229 train_loss: 0.0019906375 val_loss: 0.019651268\n",
      "Step = 230 train_loss: 0.0031596294 val_loss: 0.012855607\n",
      "Step = 231 train_loss: 0.0019550251 val_loss: 0.0028922553\n",
      "Step = 232 train_loss: 0.0026588168 val_loss: 0.003371345\n",
      "Step = 233 train_loss: 0.00205656 val_loss: 0.00893596\n",
      "Step = 234 train_loss: 0.0037984995 val_loss: 0.006011536\n",
      "Step = 235 train_loss: 0.0028471726 val_loss: 0.002353261\n",
      "Step = 236 train_loss: 0.0017634465 val_loss: 0.004851153\n",
      "Step = 237 train_loss: 0.002282899 val_loss: 0.013028888\n",
      "Step = 238 train_loss: 0.0022862838 val_loss: 0.010434504\n",
      "Step = 239 train_loss: 0.0029752902 val_loss: 0.0028061317\n",
      "Step = 240 train_loss: 0.0019688325 val_loss: 0.0024925678\n",
      "Step = 241 train_loss: 0.0020810093 val_loss: 0.005060743\n",
      "Step = 242 train_loss: 0.0022374098 val_loss: 0.006518197\n",
      "Step = 243 train_loss: 0.0030513501 val_loss: 0.0044238227\n",
      "Step = 244 train_loss: 0.0016998637 val_loss: 0.0032402102\n",
      "Step = 245 train_loss: 0.0029186667 val_loss: 0.0042915037\n",
      "Step = 246 train_loss: 0.0027380313 val_loss: 0.002106694\n",
      "Step = 247 train_loss: 0.0024648653 val_loss: 0.01305986\n",
      "Step = 248 train_loss: 0.0022413519 val_loss: 0.026436433\n",
      "Step = 249 train_loss: 0.002518639 val_loss: 0.023825377\n",
      "Step = 250 train_loss: 0.0022165636 val_loss: 0.014256506\n",
      "Step = 251 train_loss: 0.002210023 val_loss: 0.0039893114\n",
      "Step = 252 train_loss: 0.0034010885 val_loss: 0.0045769876\n",
      "Step = 253 train_loss: 0.0021813388 val_loss: 0.016050832\n",
      "Step = 254 train_loss: 0.0022045823 val_loss: 0.021539884\n",
      "Step = 255 train_loss: 0.0027464437 val_loss: 0.017930988\n",
      "Step = 256 train_loss: 0.0025373022 val_loss: 0.014274484\n",
      "Step = 257 train_loss: 0.0018386659 val_loss: 0.011942257\n",
      "Step = 258 train_loss: 0.0018591227 val_loss: 0.007629452\n",
      "Step = 259 train_loss: 0.0033279038 val_loss: 0.0023143445\n",
      "Step = 260 train_loss: 0.001995917 val_loss: 0.0072333985\n",
      "Step = 261 train_loss: 0.0020049699 val_loss: 0.023500824\n",
      "Step = 262 train_loss: 0.0031526792 val_loss: 0.017861787\n",
      "Step = 263 train_loss: 0.0022925541 val_loss: 0.009571907\n",
      "Step = 264 train_loss: 0.0016791543 val_loss: 0.0020536962\n",
      "Step = 265 train_loss: 0.002660974 val_loss: 0.0037894198\n",
      "Step = 266 train_loss: 0.0020751315 val_loss: 0.0027930918\n",
      "Step = 267 train_loss: 0.0013802422 val_loss: 0.002745543\n",
      "Step = 268 train_loss: 0.0018557776 val_loss: 0.0026556647\n",
      "Step = 269 train_loss: 0.0016306718 val_loss: 0.0024698034\n",
      "Step = 270 train_loss: 0.0028379124 val_loss: 0.0019623723\n",
      "Step = 271 train_loss: 0.0024185926 val_loss: 0.0018691625\n",
      "Step = 272 train_loss: 0.0016854738 val_loss: 0.0035800945\n",
      "Step = 273 train_loss: 0.0023432458 val_loss: 0.011512891\n",
      "Step = 274 train_loss: 0.0026406348 val_loss: 0.01390777\n",
      "Step = 275 train_loss: 0.0017870468 val_loss: 0.01163451\n",
      "Step = 276 train_loss: 0.0015610148 val_loss: 0.0036789957\n",
      "Step = 277 train_loss: 0.0033407633 val_loss: 0.002304196\n",
      "Step = 278 train_loss: 0.002009342 val_loss: 0.0045912373\n",
      "Step = 279 train_loss: 0.0016423326 val_loss: 0.0060437843\n",
      "Step = 280 train_loss: 0.0024246757 val_loss: 0.007112521\n",
      "Step = 281 train_loss: 0.0017394553 val_loss: 0.008518407\n",
      "Step = 282 train_loss: 0.0017206606 val_loss: 0.006142628\n",
      "Step = 283 train_loss: 0.0021490543 val_loss: 0.0021741379\n",
      "Step = 284 train_loss: 0.0024128514 val_loss: 0.0044661993\n",
      "Step = 285 train_loss: 0.0018449316 val_loss: 0.008095682\n",
      "Step = 286 train_loss: 0.001635095 val_loss: 0.0041376934\n",
      "Step = 287 train_loss: 0.0021472843 val_loss: 0.002154488\n",
      "Step = 288 train_loss: 0.0019975512 val_loss: 0.0024430999\n",
      "Step = 289 train_loss: 0.0013012626 val_loss: 0.0022532328\n",
      "Step = 290 train_loss: 0.0014498478 val_loss: 0.0022785526\n",
      "Step = 291 train_loss: 0.0017280455 val_loss: 0.0035293263\n",
      "Step = 292 train_loss: 0.0020973575 val_loss: 0.006906665\n",
      "Step = 293 train_loss: 0.0018787198 val_loss: 0.009002833\n",
      "Step = 294 train_loss: 0.00278192 val_loss: 0.004944173\n",
      "Step = 295 train_loss: 0.0020285451 val_loss: 0.0021729146\n",
      "Step = 296 train_loss: 0.0012173416 val_loss: 0.005063433\n",
      "Step = 297 train_loss: 0.0018862056 val_loss: 0.0017943305\n",
      "Step = 298 train_loss: 0.0016907315 val_loss: 0.0050583016\n",
      "Step = 299 train_loss: 0.0014780333 val_loss: 0.0071705463\n",
      "Step = 300 train_loss: 0.002000741 val_loss: 0.009871552\n",
      "Step = 301 train_loss: 0.0020951028 val_loss: 0.0072739646\n",
      "Step = 302 train_loss: 0.0014489158 val_loss: 0.0042417967\n",
      "Step = 303 train_loss: 0.00184017 val_loss: 0.004868253\n",
      "Step = 304 train_loss: 0.0016921773 val_loss: 0.0019249861\n",
      "Step = 305 train_loss: 0.0018064764 val_loss: 0.004126318\n",
      "Step = 306 train_loss: 0.0018303045 val_loss: 0.018941877\n",
      "Step = 307 train_loss: 0.0014063276 val_loss: 0.023073725\n",
      "Step = 308 train_loss: 0.0014916667 val_loss: 0.025042959\n",
      "Step = 309 train_loss: 0.002296583 val_loss: 0.004176856\n",
      "Step = 310 train_loss: 0.001559634 val_loss: 0.0050117224\n",
      "Step = 311 train_loss: 0.0014669368 val_loss: 0.016363682\n",
      "Step = 312 train_loss: 0.0019400704 val_loss: 0.016982822\n",
      "Step = 313 train_loss: 0.0021549377 val_loss: 0.013703929\n",
      "Step = 314 train_loss: 0.0017516749 val_loss: 0.0034688632\n",
      "Step = 315 train_loss: 0.0028659909 val_loss: 0.014514944\n",
      "Step = 316 train_loss: 0.0021047655 val_loss: 0.055484504\n",
      "Step = 317 train_loss: 0.0014092322 val_loss: 0.09124004\n",
      "Step = 318 train_loss: 0.0015571485 val_loss: 0.06589492\n",
      "Step = 319 train_loss: 0.0016903079 val_loss: 0.022947527\n",
      "Step = 320 train_loss: 0.0018430686 val_loss: 0.0054003056\n",
      "Step = 321 train_loss: 0.00179984 val_loss: 0.0020180827\n",
      "Step = 322 train_loss: 0.0020287216 val_loss: 0.012915989\n",
      "Step = 323 train_loss: 0.0023711047 val_loss: 0.015767647\n",
      "Step = 324 train_loss: 0.0019036353 val_loss: 0.010036675\n",
      "Step = 325 train_loss: 0.0022368678 val_loss: 0.0015377649\n",
      "Step = 326 train_loss: 0.0018474156 val_loss: 0.022786163\n",
      "Step = 327 train_loss: 0.0021845656 val_loss: 0.041700024\n",
      "Step = 328 train_loss: 0.0014096022 val_loss: 0.038932223\n",
      "Step = 329 train_loss: 0.0015838397 val_loss: 0.017762695\n",
      "Step = 330 train_loss: 0.001668873 val_loss: 0.004089589\n",
      "Step = 331 train_loss: 0.0016279902 val_loss: 0.0056028203\n",
      "Step = 332 train_loss: 0.0018121031 val_loss: 0.013915399\n",
      "Step = 333 train_loss: 0.0019506924 val_loss: 0.015881157\n",
      "Step = 334 train_loss: 0.0016835388 val_loss: 0.010803671\n",
      "Step = 335 train_loss: 0.0013390394 val_loss: 0.004711787\n",
      "Step = 336 train_loss: 0.001481319 val_loss: 0.0024209998\n",
      "Step = 337 train_loss: 0.0019171423 val_loss: 0.0017518023\n",
      "Step = 338 train_loss: 0.0012745081 val_loss: 0.001560263\n",
      "Step = 339 train_loss: 0.0011168774 val_loss: 0.0038742858\n",
      "Step = 340 train_loss: 0.0018397842 val_loss: 0.0014706004\n",
      "Step = 341 train_loss: 0.0021413146 val_loss: 0.0063764197\n",
      "Step = 342 train_loss: 0.0011642511 val_loss: 0.013053144\n",
      "Step = 343 train_loss: 0.0012763523 val_loss: 0.01785237\n",
      "Step = 344 train_loss: 0.0012445585 val_loss: 0.016512947\n",
      "Step = 345 train_loss: 0.00185033 val_loss: 0.011055429\n",
      "Step = 346 train_loss: 0.0018543842 val_loss: 0.0029356023\n",
      "Step = 347 train_loss: 0.0018967802 val_loss: 0.01778225\n",
      "Step = 348 train_loss: 0.0018284073 val_loss: 0.046559766\n",
      "Step = 349 train_loss: 0.0016858743 val_loss: 0.061657593\n",
      "Step = 350 train_loss: 0.0020205723 val_loss: 0.02394666\n",
      "Step = 351 train_loss: 0.0019354007 val_loss: 0.002197122\n",
      "Step = 352 train_loss: 0.0011490121 val_loss: 0.0036356838\n",
      "Step = 353 train_loss: 0.001913393 val_loss: 0.016389055\n",
      "Step = 354 train_loss: 0.00088908774 val_loss: 0.019682856\n",
      "Step = 355 train_loss: 0.0015441069 val_loss: 0.012483652\n",
      "Step = 356 train_loss: 0.0016274551 val_loss: 0.0024228133\n",
      "Step = 357 train_loss: 0.0016144181 val_loss: 0.006512292\n",
      "Step = 358 train_loss: 0.0014790494 val_loss: 0.030706251\n",
      "Step = 359 train_loss: 0.0015530866 val_loss: 0.047447894\n",
      "Step = 360 train_loss: 0.00140387 val_loss: 0.035134412\n",
      "Step = 361 train_loss: 0.0019617754 val_loss: 0.014876195\n",
      "Step = 362 train_loss: 0.0017486345 val_loss: 0.0015560035\n",
      "Step = 363 train_loss: 0.0017284335 val_loss: 0.0041961367\n",
      "Step = 364 train_loss: 0.0014082199 val_loss: 0.010646913\n",
      "Step = 365 train_loss: 0.0017341131 val_loss: 0.013518255\n",
      "Step = 366 train_loss: 0.0013014525 val_loss: 0.009731393\n",
      "Step = 367 train_loss: 0.0011449008 val_loss: 0.0033367893\n",
      "Step = 368 train_loss: 0.0013177434 val_loss: 0.001645766\n",
      "Step = 369 train_loss: 0.0012147232 val_loss: 0.0030569073\n",
      "Step = 370 train_loss: 0.0010499606 val_loss: 0.0039327643\n",
      "Step = 371 train_loss: 0.0020651133 val_loss: 0.006546804\n",
      "Step = 372 train_loss: 0.0011074252 val_loss: 0.007232453\n",
      "Step = 373 train_loss: 0.0014739165 val_loss: 0.0015091946\n",
      "Step = 374 train_loss: 0.0013889503 val_loss: 0.0027817495\n",
      "Step = 375 train_loss: 0.0014503902 val_loss: 0.011769487\n",
      "Step = 376 train_loss: 0.0013494026 val_loss: 0.017853336\n",
      "Step = 377 train_loss: 0.0014620502 val_loss: 0.016847128\n",
      "Step = 378 train_loss: 0.0014505556 val_loss: 0.009021338\n",
      "Step = 379 train_loss: 0.0011230803 val_loss: 0.0028410372\n",
      "Step = 380 train_loss: 0.0008851715 val_loss: 0.0014423662\n",
      "Step = 381 train_loss: 0.0012370135 val_loss: 0.0028389266\n",
      "Step = 382 train_loss: 0.0010874943 val_loss: 0.008973746\n",
      "Step = 383 train_loss: 0.001201008 val_loss: 0.0061764293\n",
      "Step = 384 train_loss: 0.0012375702 val_loss: 0.0017205166\n",
      "Step = 385 train_loss: 0.0014048293 val_loss: 0.0015064668\n",
      "Step = 386 train_loss: 0.0014954291 val_loss: 0.0034026296\n",
      "Step = 387 train_loss: 0.0012437808 val_loss: 0.0051981295\n",
      "Step = 388 train_loss: 0.001098135 val_loss: 0.0065592085\n",
      "Step = 389 train_loss: 0.0013711272 val_loss: 0.0016719755\n",
      "Step = 390 train_loss: 0.001712058 val_loss: 0.023298314\n",
      "Step = 391 train_loss: 0.0016539458 val_loss: 0.06720897\n",
      "Step = 392 train_loss: 0.0016760511 val_loss: 0.05417994\n",
      "Step = 393 train_loss: 0.0012704071 val_loss: 0.014324629\n",
      "Step = 394 train_loss: 0.0015894686 val_loss: 0.005628262\n",
      "Step = 395 train_loss: 0.001548919 val_loss: 0.02194202\n",
      "Step = 396 train_loss: 0.0013759111 val_loss: 0.026482023\n",
      "Step = 397 train_loss: 0.0015120278 val_loss: 0.024622347\n",
      "Step = 398 train_loss: 0.0012325444 val_loss: 0.010597804\n",
      "Step = 399 train_loss: 0.001968482 val_loss: 0.00853189\n",
      "Step = 400 train_loss: 0.0009897071 val_loss: 0.056235965\n",
      "Step = 401 train_loss: 0.0018695133 val_loss: 0.06233631\n",
      "Step = 402 train_loss: 0.0013033669 val_loss: 0.03885433\n",
      "Step = 403 train_loss: 0.0019908 val_loss: 0.01409867\n",
      "Step = 404 train_loss: 0.0010388569 val_loss: 0.0012022572\n",
      "Step = 405 train_loss: 0.001306887 val_loss: 0.0063973395\n",
      "Step = 406 train_loss: 0.0014759954 val_loss: 0.008103783\n",
      "Step = 407 train_loss: 0.001411997 val_loss: 0.00565007\n",
      "Step = 408 train_loss: 0.0009769945 val_loss: 0.0040579266\n",
      "Step = 409 train_loss: 0.0018180008 val_loss: 0.0019506381\n",
      "Step = 410 train_loss: 0.0013187735 val_loss: 0.0013828698\n",
      "Step = 411 train_loss: 0.001335984 val_loss: 0.0014573911\n",
      "Step = 412 train_loss: 0.0012198281 val_loss: 0.0019178105\n",
      "Step = 413 train_loss: 0.0015380887 val_loss: 0.0017713299\n",
      "Step = 414 train_loss: 0.0011253427 val_loss: 0.0023855127\n",
      "Step = 415 train_loss: 0.0013208378 val_loss: 0.0017278428\n",
      "Step = 416 train_loss: 0.001158346 val_loss: 0.002312158\n",
      "Step = 417 train_loss: 0.001105139 val_loss: 0.009282668\n",
      "Step = 418 train_loss: 0.0011075351 val_loss: 0.017734896\n",
      "Step = 419 train_loss: 0.0010038605 val_loss: 0.017647903\n",
      "Step = 420 train_loss: 0.00091937126 val_loss: 0.015963096\n",
      "Step = 421 train_loss: 0.0013774942 val_loss: 0.0054227947\n",
      "Step = 422 train_loss: 0.0010198394 val_loss: 0.00335839\n",
      "Step = 423 train_loss: 0.0007307014 val_loss: 0.018132372\n",
      "Step = 424 train_loss: 0.00085979444 val_loss: 0.026590569\n",
      "Step = 425 train_loss: 0.0010048018 val_loss: 0.01697079\n",
      "Step = 426 train_loss: 0.0014099288 val_loss: 0.0034366054\n",
      "Step = 427 train_loss: 0.0008895304 val_loss: 0.0023886224\n",
      "Step = 428 train_loss: 0.0012434311 val_loss: 0.00667313\n",
      "Step = 429 train_loss: 0.0010394083 val_loss: 0.0041026943\n",
      "Step = 430 train_loss: 0.001118487 val_loss: 0.0012821632\n",
      "Step = 431 train_loss: 0.0011453412 val_loss: 0.0016428131\n",
      "Step = 432 train_loss: 0.0008494519 val_loss: 0.0020933545\n",
      "Step = 433 train_loss: 0.0009186925 val_loss: 0.0029947688\n",
      "Step = 434 train_loss: 0.0011947445 val_loss: 0.0041239304\n",
      "Step = 435 train_loss: 0.0008010375 val_loss: 0.0019720895\n",
      "Step = 436 train_loss: 0.0010325663 val_loss: 0.0017789784\n",
      "Step = 437 train_loss: 0.00075586006 val_loss: 0.0111890985\n",
      "Step = 438 train_loss: 0.0010373452 val_loss: 0.025595956\n",
      "Step = 439 train_loss: 0.0011242141 val_loss: 0.012258351\n",
      "Step = 440 train_loss: 0.0008060128 val_loss: 0.0016497995\n",
      "Step = 441 train_loss: 0.0011552285 val_loss: 0.009793982\n",
      "Step = 442 train_loss: 0.001127489 val_loss: 0.020472554\n",
      "Step = 443 train_loss: 0.0007079906 val_loss: 0.02402394\n",
      "Step = 444 train_loss: 0.0010783768 val_loss: 0.016747497\n",
      "Step = 445 train_loss: 0.0011192398 val_loss: 0.001583146\n",
      "Step = 446 train_loss: 0.0011848789 val_loss: 0.0691861\n",
      "Step = 447 train_loss: 0.0012101508 val_loss: 0.2047157\n",
      "Step = 448 train_loss: 0.001525717 val_loss: 0.1767908\n",
      "Step = 449 train_loss: 0.0010758104 val_loss: 0.058493223\n",
      "Step = 450 train_loss: 0.0019643086 val_loss: 0.0058501316\n",
      "Step = 451 train_loss: 0.0010458708 val_loss: 0.0044505233\n",
      "Step = 452 train_loss: 0.0012320123 val_loss: 0.01337756\n",
      "Step = 453 train_loss: 0.0011800589 val_loss: 0.0052377763\n",
      "Step = 454 train_loss: 0.0012561666 val_loss: 0.0010806504\n",
      "Step = 455 train_loss: 0.0008410585 val_loss: 0.0012927915\n",
      "Step = 456 train_loss: 0.00067486067 val_loss: 0.002271009\n",
      "Step = 457 train_loss: 0.0012434765 val_loss: 0.004207409\n",
      "Step = 458 train_loss: 0.0015093274 val_loss: 0.004068811\n",
      "Step = 459 train_loss: 0.0015537429 val_loss: 0.0026427598\n",
      "Step = 460 train_loss: 0.0015075848 val_loss: 0.0027724789\n",
      "Step = 461 train_loss: 0.0011365508 val_loss: 0.0016961512\n",
      "Step = 462 train_loss: 0.0017002136 val_loss: 0.0035891181\n",
      "Step = 463 train_loss: 0.0015349253 val_loss: 0.0019148365\n",
      "Step = 464 train_loss: 0.0009397673 val_loss: 0.0041170144\n",
      "Step = 465 train_loss: 0.0010329126 val_loss: 0.014463894\n",
      "Step = 466 train_loss: 0.00084867957 val_loss: 0.019529236\n",
      "Step = 467 train_loss: 0.0013268398 val_loss: 0.012661545\n",
      "Step = 468 train_loss: 0.00083383557 val_loss: 0.006328072\n",
      "Step = 469 train_loss: 0.0012535836 val_loss: 0.0022906673\n",
      "Step = 470 train_loss: 0.0010618183 val_loss: 0.024227219\n",
      "Step = 471 train_loss: 0.0011578216 val_loss: 0.05917094\n",
      "Step = 472 train_loss: 0.0013586148 val_loss: 0.048576873\n",
      "Step = 473 train_loss: 0.0008800659 val_loss: 0.02546685\n",
      "Step = 474 train_loss: 0.0013791855 val_loss: 0.001612109\n",
      "Step = 475 train_loss: 0.0012757076 val_loss: 0.009254238\n",
      "Step = 476 train_loss: 0.0012006932 val_loss: 0.017007796\n",
      "Step = 477 train_loss: 0.0013755778 val_loss: 0.019761946\n",
      "Step = 478 train_loss: 0.0010101805 val_loss: 0.014069211\n",
      "Step = 479 train_loss: 0.0015057878 val_loss: 0.0009055397\n",
      "Step = 480 train_loss: 0.001246076 val_loss: 0.02301073\n",
      "Step = 481 train_loss: 0.0007333992 val_loss: 0.050089825\n",
      "Step = 482 train_loss: 0.00054480473 val_loss: 0.07961293\n",
      "Step = 483 train_loss: 0.0011518211 val_loss: 0.07610229\n",
      "Step = 484 train_loss: 0.0011554458 val_loss: 0.03816507\n",
      "Step = 485 train_loss: 0.0012702447 val_loss: 0.007294928\n",
      "Step = 486 train_loss: 0.0008526472 val_loss: 0.0031871155\n",
      "Step = 487 train_loss: 0.0006684325 val_loss: 0.01717826\n",
      "Step = 488 train_loss: 0.0006944831 val_loss: 0.024597805\n",
      "Step = 489 train_loss: 0.00084737607 val_loss: 0.023244975\n",
      "Step = 490 train_loss: 0.00077999604 val_loss: 0.016548706\n",
      "Step = 491 train_loss: 0.0012147769 val_loss: 0.001291756\n",
      "Step = 492 train_loss: 0.00090008444 val_loss: 0.059844963\n",
      "Step = 493 train_loss: 0.0007393212 val_loss: 0.11801294\n",
      "Step = 494 train_loss: 0.0008077842 val_loss: 0.11435024\n",
      "Step = 495 train_loss: 0.000992531 val_loss: 0.041546006\n",
      "Step = 496 train_loss: 0.001121348 val_loss: 0.0007654268\n",
      "Step = 497 train_loss: 0.0010093935 val_loss: 0.015541305\n",
      "Step = 498 train_loss: 0.0012463282 val_loss: 0.021564744\n",
      "Step = 499 train_loss: 0.0012815458 val_loss: 0.016341444\n",
      "Step = 500 train_loss: 0.00091629516 val_loss: 0.0057312194\n",
      "Step = 501 train_loss: 0.0013150375 val_loss: 0.0016748386\n",
      "Step = 502 train_loss: 0.00079815096 val_loss: 0.0045336206\n",
      "Step = 503 train_loss: 0.0008472211 val_loss: 0.013259892\n",
      "Step = 504 train_loss: 0.0008600597 val_loss: 0.0053271423\n",
      "Step = 505 train_loss: 0.000789413 val_loss: 0.0025460327\n",
      "Step = 506 train_loss: 0.0013804634 val_loss: 0.0106941685\n",
      "Step = 507 train_loss: 0.0009415119 val_loss: 0.016797096\n",
      "Step = 508 train_loss: 0.0008317753 val_loss: 0.016800353\n",
      "Step = 509 train_loss: 0.001461082 val_loss: 0.017806886\n",
      "Step = 510 train_loss: 0.0013010963 val_loss: 0.01216182\n",
      "Step = 511 train_loss: 0.000921602 val_loss: 0.0021469973\n",
      "Step = 512 train_loss: 0.0008454988 val_loss: 0.0072231996\n",
      "Step = 513 train_loss: 0.0011189252 val_loss: 0.017892342\n",
      "Step = 514 train_loss: 0.0014961482 val_loss: 0.0059029483\n",
      "Step = 515 train_loss: 0.0009092068 val_loss: 0.0018343588\n",
      "Step = 516 train_loss: 0.0008688955 val_loss: 0.0071176463\n",
      "Step = 517 train_loss: 0.0008971324 val_loss: 0.008416301\n",
      "Step = 518 train_loss: 0.0005881756 val_loss: 0.0066223782\n",
      "Step = 519 train_loss: 0.0008466725 val_loss: 0.0031394525\n",
      "Step = 520 train_loss: 0.0012328011 val_loss: 0.0014408001\n",
      "Step = 521 train_loss: 0.000747322 val_loss: 0.001094418\n",
      "Step = 522 train_loss: 0.000821018 val_loss: 0.0023544761\n",
      "Step = 523 train_loss: 0.0009153319 val_loss: 0.00090210285\n",
      "Step = 524 train_loss: 0.0013210222 val_loss: 0.0018924653\n",
      "Step = 525 train_loss: 0.001000723 val_loss: 0.0018469205\n",
      "Step = 526 train_loss: 0.00083879154 val_loss: 0.002699845\n",
      "Step = 527 train_loss: 0.0005109469 val_loss: 0.002087851\n",
      "Step = 528 train_loss: 0.0007325796 val_loss: 0.0013947549\n",
      "Step = 529 train_loss: 0.0011840614 val_loss: 0.0012664963\n",
      "Step = 530 train_loss: 0.0009845146 val_loss: 0.0022825594\n",
      "Step = 531 train_loss: 0.00051614305 val_loss: 0.0013617012\n",
      "Step = 532 train_loss: 0.00062735786 val_loss: 0.001995026\n",
      "Step = 533 train_loss: 0.0005847974 val_loss: 0.0065870676\n",
      "Step = 534 train_loss: 0.0010067031 val_loss: 0.01601519\n",
      "Step = 535 train_loss: 0.0010383497 val_loss: 0.015998129\n",
      "Step = 536 train_loss: 0.0014047829 val_loss: 0.0032046896\n",
      "Step = 537 train_loss: 0.0007944784 val_loss: 0.010771422\n",
      "Step = 538 train_loss: 0.0008908917 val_loss: 0.03017436\n",
      "Step = 539 train_loss: 0.0008674369 val_loss: 0.04297811\n",
      "Step = 540 train_loss: 0.0005371712 val_loss: 0.028918132\n",
      "Step = 541 train_loss: 0.0006465219 val_loss: 0.015262142\n",
      "Step = 542 train_loss: 0.0007706679 val_loss: 0.0014139226\n",
      "Step = 543 train_loss: 0.00055688486 val_loss: 0.0011272065\n",
      "Step = 544 train_loss: 0.001131297 val_loss: 0.006746319\n",
      "Step = 545 train_loss: 0.0007673686 val_loss: 0.01596464\n",
      "Step = 546 train_loss: 0.0006638819 val_loss: 0.020081446\n",
      "Step = 547 train_loss: 0.00060367986 val_loss: 0.01683035\n",
      "Step = 548 train_loss: 0.000896223 val_loss: 0.0033607148\n",
      "Step = 549 train_loss: 0.00091443065 val_loss: 0.008288323\n",
      "Step = 550 train_loss: 0.0006936102 val_loss: 0.08495509\n",
      "Step = 551 train_loss: 0.0012995305 val_loss: 0.14163607\n",
      "Step = 552 train_loss: 0.0009933212 val_loss: 0.13583861\n",
      "Step = 553 train_loss: 0.0010133868 val_loss: 0.034849826\n",
      "Step = 554 train_loss: 0.0011817917 val_loss: 0.0040235817\n",
      "Step = 555 train_loss: 0.0007924021 val_loss: 0.025265617\n",
      "Step = 556 train_loss: 0.0014104879 val_loss: 0.02580761\n",
      "Step = 557 train_loss: 0.0006178493 val_loss: 0.02053887\n",
      "Step = 558 train_loss: 0.0008568358 val_loss: 0.0029648505\n",
      "Step = 559 train_loss: 0.0013704372 val_loss: 0.022751182\n",
      "Step = 560 train_loss: 0.00075697654 val_loss: 0.0678937\n",
      "Step = 561 train_loss: 0.000617722 val_loss: 0.06386109\n",
      "Step = 562 train_loss: 0.0010714165 val_loss: 0.015635548\n",
      "Step = 563 train_loss: 0.00083353766 val_loss: 0.00072909985\n",
      "Step = 564 train_loss: 0.00067508215 val_loss: 0.009371638\n",
      "Step = 565 train_loss: 0.0013383125 val_loss: 0.009524625\n",
      "Step = 566 train_loss: 0.0009087272 val_loss: 0.0018102074\n",
      "Step = 567 train_loss: 0.00064404856 val_loss: 0.010519544\n",
      "Step = 568 train_loss: 0.0005067973 val_loss: 0.04347373\n",
      "Step = 569 train_loss: 0.0008111403 val_loss: 0.054547373\n",
      "Step = 570 train_loss: 0.000982157 val_loss: 0.02449512\n",
      "Step = 571 train_loss: 0.00095177663 val_loss: 0.002684873\n",
      "Step = 572 train_loss: 0.0010427763 val_loss: 0.015711933\n",
      "Step = 573 train_loss: 0.0007205467 val_loss: 0.025645273\n",
      "Step = 574 train_loss: 0.0011779926 val_loss: 0.023136433\n",
      "Step = 575 train_loss: 0.0006860059 val_loss: 0.011897861\n",
      "Step = 576 train_loss: 0.0005150357 val_loss: 0.0061746677\n",
      "Step = 577 train_loss: 0.0008955238 val_loss: 0.0046356907\n",
      "Step = 578 train_loss: 0.0005945539 val_loss: 0.0038423417\n",
      "Step = 579 train_loss: 0.00071154017 val_loss: 0.007350655\n",
      "Step = 580 train_loss: 0.00067096925 val_loss: 0.0077343937\n",
      "Step = 581 train_loss: 0.00072872837 val_loss: 0.00566173\n",
      "Step = 582 train_loss: 0.00052366534 val_loss: 0.0021687644\n",
      "Step = 583 train_loss: 0.00074731593 val_loss: 0.00639708\n",
      "Step = 584 train_loss: 0.0006761551 val_loss: 0.00859645\n",
      "Step = 585 train_loss: 0.00044364078 val_loss: 0.00661662\n",
      "Step = 586 train_loss: 0.00045582172 val_loss: 0.0017673315\n",
      "Step = 587 train_loss: 0.0009198135 val_loss: 0.014500364\n",
      "Step = 588 train_loss: 0.00074377947 val_loss: 0.02289806\n",
      "Step = 589 train_loss: 0.0006360054 val_loss: 0.018415004\n",
      "Step = 590 train_loss: 0.00061980507 val_loss: 0.0015335679\n",
      "Step = 591 train_loss: 0.00084860023 val_loss: 0.007546456\n",
      "Step = 592 train_loss: 0.00069149374 val_loss: 0.017859766\n",
      "Step = 593 train_loss: 0.0011331236 val_loss: 0.013911245\n",
      "Step = 594 train_loss: 0.00097634364 val_loss: 0.0015509453\n",
      "Step = 595 train_loss: 0.0008564035 val_loss: 0.009131967\n",
      "Step = 596 train_loss: 0.0009836389 val_loss: 0.011122468\n",
      "Step = 597 train_loss: 0.0008508381 val_loss: 0.0016849526\n",
      "Step = 598 train_loss: 0.0011570983 val_loss: 0.002083015\n",
      "Step = 599 train_loss: 0.0008288532 val_loss: 0.0048887446\n",
      "Step = 600 train_loss: 0.000674915 val_loss: 0.003950403\n",
      "Step = 601 train_loss: 0.00079942524 val_loss: 0.0068648313\n",
      "Step = 602 train_loss: 0.0006827626 val_loss: 0.053480685\n",
      "Step = 603 train_loss: 0.0005274161 val_loss: 0.063793585\n",
      "Step = 604 train_loss: 0.0007039717 val_loss: 0.030657772\n",
      "Step = 605 train_loss: 0.00092299626 val_loss: 0.004628593\n",
      "Step = 606 train_loss: 0.00043268173 val_loss: 0.006351208\n",
      "Step = 607 train_loss: 0.0008007217 val_loss: 0.012898665\n",
      "Step = 608 train_loss: 0.0011223261 val_loss: 0.008252743\n",
      "Step = 609 train_loss: 0.00093185465 val_loss: 0.0029198094\n",
      "Step = 610 train_loss: 0.00040677085 val_loss: 0.0007705042\n",
      "Step = 611 train_loss: 0.0009653755 val_loss: 0.0071013374\n",
      "Step = 612 train_loss: 0.0010909707 val_loss: 0.017136946\n",
      "Step = 613 train_loss: 0.00079784193 val_loss: 0.030255215\n",
      "Step = 614 train_loss: 0.0009674697 val_loss: 0.020297503\n",
      "Step = 615 train_loss: 0.0007406154 val_loss: 0.00925096\n",
      "Step = 616 train_loss: 0.0008506831 val_loss: 0.0012301388\n",
      "Step = 617 train_loss: 0.00057177537 val_loss: 0.009736349\n",
      "Step = 618 train_loss: 0.0009540795 val_loss: 0.009783293\n",
      "Step = 619 train_loss: 0.0011576372 val_loss: 0.007173484\n",
      "Step = 620 train_loss: 0.0007633165 val_loss: 0.0007924355\n",
      "Step = 621 train_loss: 0.00092493795 val_loss: 0.012322327\n",
      "Step = 622 train_loss: 0.000793213 val_loss: 0.019569153\n",
      "Step = 623 train_loss: 0.0009007548 val_loss: 0.0047954293\n",
      "Step = 624 train_loss: 0.0005050738 val_loss: 0.0011532932\n",
      "Step = 625 train_loss: 0.0007874367 val_loss: 0.0006076957\n",
      "Step = 626 train_loss: 0.00087779335 val_loss: 0.0070068226\n",
      "Step = 627 train_loss: 0.00069807883 val_loss: 0.014670189\n",
      "Step = 628 train_loss: 0.00062114024 val_loss: 0.011243332\n",
      "Step = 629 train_loss: 0.00073249836 val_loss: 0.0028676589\n",
      "Step = 630 train_loss: 0.00070574624 val_loss: 0.0034022725\n",
      "Step = 631 train_loss: 0.00073256186 val_loss: 0.014848295\n",
      "Step = 632 train_loss: 0.0006184544 val_loss: 0.0072993776\n",
      "Step = 633 train_loss: 0.0006299991 val_loss: 0.0014340326\n",
      "Step = 634 train_loss: 0.00076403003 val_loss: 0.003021307\n",
      "Step = 635 train_loss: 0.0006489077 val_loss: 0.015791038\n",
      "Step = 636 train_loss: 0.0007283701 val_loss: 0.020238217\n",
      "Step = 637 train_loss: 0.0009344261 val_loss: 0.015343783\n",
      "Step = 638 train_loss: 0.001018816 val_loss: 0.0064853556\n",
      "Step = 639 train_loss: 0.0012198006 val_loss: 0.0036075788\n",
      "Step = 640 train_loss: 0.0005729659 val_loss: 0.005244716\n",
      "Step = 641 train_loss: 0.00083424087 val_loss: 0.009146979\n",
      "Step = 642 train_loss: 0.00050400634 val_loss: 0.010485901\n",
      "Step = 643 train_loss: 0.0010244007 val_loss: 0.05623914\n",
      "Step = 644 train_loss: 0.00073546666 val_loss: 0.035322633\n",
      "Step = 645 train_loss: 0.0007902756 val_loss: 0.00490926\n",
      "Step = 646 train_loss: 0.0008918111 val_loss: 0.018594256\n",
      "Step = 647 train_loss: 0.00072420755 val_loss: 0.028827097\n",
      "Step = 648 train_loss: 0.0009209287 val_loss: 0.029652275\n",
      "Step = 649 train_loss: 0.0007669632 val_loss: 0.022128016\n",
      "Step = 650 train_loss: 0.000785231 val_loss: 0.008097318\n",
      "Step = 651 train_loss: 0.00077278935 val_loss: 0.0077957236\n",
      "Step = 652 train_loss: 0.0006723059 val_loss: 0.0131965205\n",
      "Step = 653 train_loss: 0.00078631495 val_loss: 0.0058115874\n",
      "Step = 654 train_loss: 0.00097586715 val_loss: 0.007188788\n",
      "Step = 655 train_loss: 0.0011027886 val_loss: 0.014849398\n",
      "Step = 656 train_loss: 0.0007218641 val_loss: 0.0056530056\n",
      "Step = 657 train_loss: 0.00076752616 val_loss: 0.015786076\n",
      "Step = 658 train_loss: 0.0007423647 val_loss: 0.026779218\n",
      "Step = 659 train_loss: 0.00064448244 val_loss: 0.025925549\n",
      "Step = 660 train_loss: 0.00076667994 val_loss: 0.015444735\n",
      "Step = 661 train_loss: 0.00039838094 val_loss: 0.007814047\n",
      "Step = 662 train_loss: 0.0002868952 val_loss: 0.0034697782\n",
      "Step = 663 train_loss: 0.00052382 val_loss: 0.010701523\n",
      "Step = 664 train_loss: 0.00061115116 val_loss: 0.007026356\n",
      "Step = 665 train_loss: 0.00067245995 val_loss: 0.0041570365\n",
      "Step = 666 train_loss: 0.0011451878 val_loss: 0.012221315\n",
      "Step = 667 train_loss: 0.00057338475 val_loss: 0.021799024\n",
      "Step = 668 train_loss: 0.00080197235 val_loss: 0.003970132\n",
      "Step = 669 train_loss: 0.00062587956 val_loss: 0.018440997\n",
      "Step = 670 train_loss: 0.00065994204 val_loss: 0.02538523\n",
      "Step = 671 train_loss: 0.00074159156 val_loss: 0.022753486\n",
      "Step = 672 train_loss: 0.00085730397 val_loss: 0.008167098\n",
      "Step = 673 train_loss: 0.0006464536 val_loss: 0.012220476\n",
      "Step = 674 train_loss: 0.0008508354 val_loss: 0.12261588\n",
      "Step = 675 train_loss: 0.00070602674 val_loss: 0.12761834\n",
      "Step = 676 train_loss: 0.0010780584 val_loss: 0.03712937\n",
      "Step = 677 train_loss: 0.000406432 val_loss: 0.001975314\n",
      "Step = 678 train_loss: 0.000530519 val_loss: 0.008097081\n",
      "Step = 679 train_loss: 0.00050437707 val_loss: 0.019761711\n",
      "Step = 680 train_loss: 0.00050143286 val_loss: 0.027837116\n",
      "Step = 681 train_loss: 0.0005784466 val_loss: 0.028320054\n",
      "Step = 682 train_loss: 0.00068134477 val_loss: 0.023485214\n",
      "Step = 683 train_loss: 0.0012298053 val_loss: 0.004366992\n",
      "Step = 684 train_loss: 0.0004410277 val_loss: 0.06619319\n",
      "Step = 685 train_loss: 0.0005700799 val_loss: 0.2284412\n",
      "Step = 686 train_loss: 0.0009079223 val_loss: 0.11688922\n",
      "Step = 687 train_loss: 0.00043215838 val_loss: 0.025128242\n",
      "Step = 688 train_loss: 0.0013014916 val_loss: 0.0037580642\n",
      "Step = 689 train_loss: 0.00062667014 val_loss: 0.006031044\n",
      "Step = 690 train_loss: 0.000669831 val_loss: 0.012055657\n",
      "Step = 691 train_loss: 0.0004926786 val_loss: 0.020339638\n",
      "Step = 692 train_loss: 0.0008092436 val_loss: 0.027848259\n",
      "Step = 693 train_loss: 0.00065612973 val_loss: 0.033332147\n",
      "Step = 694 train_loss: 0.0007229918 val_loss: 0.02941468\n",
      "Step = 695 train_loss: 0.00058841 val_loss: 0.01826498\n",
      "Step = 696 train_loss: 0.0007590891 val_loss: 0.00091063266\n",
      "Step = 697 train_loss: 0.000897267 val_loss: 0.10738062\n",
      "Step = 698 train_loss: 0.000772367 val_loss: 0.25045055\n",
      "Step = 699 train_loss: 0.0008102096 val_loss: 0.25179836\n",
      "Step = 700 train_loss: 0.0009924867 val_loss: 0.09583907\n",
      "Step = 701 train_loss: 0.0011388115 val_loss: 0.005461323\n",
      "Step = 702 train_loss: 0.0006135559 val_loss: 0.011901092\n",
      "Step = 703 train_loss: 0.00043582637 val_loss: 0.019432727\n",
      "Step = 704 train_loss: 0.00067861006 val_loss: 0.021127243\n",
      "Step = 705 train_loss: 0.001107281 val_loss: 0.0142089315\n",
      "Step = 706 train_loss: 0.00089429185 val_loss: 0.003256531\n",
      "Step = 707 train_loss: 0.0009367118 val_loss: 0.07472796\n",
      "Step = 708 train_loss: 0.00038533722 val_loss: 0.153182\n",
      "Step = 709 train_loss: 0.00039875187 val_loss: 0.1706953\n",
      "Step = 710 train_loss: 0.0009048728 val_loss: 0.051343963\n",
      "Step = 711 train_loss: 0.0008901742 val_loss: 0.008341353\n",
      "Step = 712 train_loss: 0.00042887699 val_loss: 0.030335192\n",
      "Step = 713 train_loss: 0.0011822582 val_loss: 0.0371187\n",
      "Step = 714 train_loss: 0.00058813684 val_loss: 0.0345799\n",
      "Step = 715 train_loss: 0.0006405068 val_loss: 0.026930315\n",
      "Step = 716 train_loss: 0.0003680556 val_loss: 0.023189416\n",
      "Step = 717 train_loss: 0.000685989 val_loss: 0.022812037\n",
      "Step = 718 train_loss: 0.0009663084 val_loss: 0.02030579\n",
      "Step = 719 train_loss: 0.00056054466 val_loss: 0.016021434\n",
      "Step = 720 train_loss: 0.0007441168 val_loss: 0.0026223613\n",
      "Step = 721 train_loss: 0.00059189484 val_loss: 0.019498508\n",
      "Step = 722 train_loss: 0.0005347282 val_loss: 0.034632903\n",
      "Step = 723 train_loss: 0.0007654715 val_loss: 0.01814517\n",
      "Step = 724 train_loss: 0.00064018904 val_loss: 0.009575329\n",
      "Step = 725 train_loss: 0.0005473674 val_loss: 0.0033292207\n",
      "Step = 726 train_loss: 0.0005466739 val_loss: 0.0018607351\n",
      "Step = 727 train_loss: 0.0006313576 val_loss: 0.0012882077\n",
      "Step = 728 train_loss: 0.00063317426 val_loss: 0.0033116897\n",
      "Step = 729 train_loss: 0.00040660214 val_loss: 0.0057592182\n",
      "Step = 730 train_loss: 0.000429943 val_loss: 0.006722067\n",
      "Step = 731 train_loss: 0.00074266834 val_loss: 0.0027445096\n",
      "Step = 732 train_loss: 0.00039272738 val_loss: 0.005789668\n",
      "Step = 733 train_loss: 0.00060330227 val_loss: 0.013894491\n",
      "Step = 734 train_loss: 0.00063003035 val_loss: 0.022945529\n",
      "Step = 735 train_loss: 0.0007118056 val_loss: 0.028827982\n",
      "Step = 736 train_loss: 0.0004984465 val_loss: 0.02057113\n",
      "Step = 737 train_loss: 0.0005176486 val_loss: 0.01170185\n",
      "Step = 738 train_loss: 0.0005625205 val_loss: 0.0074013183\n",
      "Step = 739 train_loss: 0.0012480089 val_loss: 0.009804168\n",
      "Step = 740 train_loss: 0.0005908411 val_loss: 0.028646639\n",
      "Step = 741 train_loss: 0.00038951743 val_loss: 0.059691068\n",
      "Step = 742 train_loss: 0.0006427206 val_loss: 0.040494516\n",
      "Step = 743 train_loss: 0.00047523517 val_loss: 0.018263625\n",
      "Step = 744 train_loss: 0.00028169394 val_loss: 0.013681708\n",
      "Step = 745 train_loss: 0.00078444876 val_loss: 0.012629305\n",
      "Step = 746 train_loss: 0.0005347041 val_loss: 0.013648167\n",
      "Step = 747 train_loss: 0.0010472963 val_loss: 0.0296046\n",
      "Step = 748 train_loss: 0.00046359858 val_loss: 0.06407566\n",
      "Step = 749 train_loss: 0.0006576482 val_loss: 0.077815905\n",
      "Step = 750 train_loss: 0.0006726552 val_loss: 0.0639596\n",
      "Step = 751 train_loss: 0.000783532 val_loss: 0.05695821\n",
      "Step = 752 train_loss: 0.0009868115 val_loss: 0.028123071\n",
      "Step = 753 train_loss: 0.0008896406 val_loss: 0.0009721505\n",
      "Step = 754 train_loss: 0.0003466515 val_loss: 0.011725258\n",
      "Step = 755 train_loss: 0.00052813353 val_loss: 0.02019525\n",
      "Step = 756 train_loss: 0.0009357815 val_loss: 0.014943063\n",
      "Step = 757 train_loss: 0.00074617076 val_loss: 0.010476104\n",
      "Step = 758 train_loss: 0.00053868524 val_loss: 0.03409659\n",
      "Step = 759 train_loss: 0.0008594018 val_loss: 0.02122944\n",
      "Step = 760 train_loss: 0.0008977404 val_loss: 0.013939562\n",
      "Step = 761 train_loss: 0.00065787346 val_loss: 0.034419645\n",
      "Step = 762 train_loss: 0.00073785544 val_loss: 0.045451075\n",
      "Step = 763 train_loss: 0.0008956933 val_loss: 0.0440567\n",
      "Step = 764 train_loss: 0.0009980238 val_loss: 0.029623369\n",
      "Step = 765 train_loss: 0.00046550157 val_loss: 0.010746264\n",
      "Step = 766 train_loss: 0.0004746579 val_loss: 0.032933798\n",
      "Step = 767 train_loss: 0.00069082493 val_loss: 0.029090768\n",
      "Step = 768 train_loss: 0.0005464912 val_loss: 0.010998697\n",
      "Step = 769 train_loss: 0.0007811911 val_loss: 0.025140477\n",
      "Step = 770 train_loss: 0.00048722545 val_loss: 0.03937494\n",
      "Step = 771 train_loss: 0.00069211476 val_loss: 0.04005278\n",
      "Step = 772 train_loss: 0.0005233186 val_loss: 0.031024445\n",
      "Step = 773 train_loss: 0.00042390655 val_loss: 0.028416565\n",
      "Step = 774 train_loss: 0.00065877516 val_loss: 0.03140379\n",
      "Step = 775 train_loss: 0.0004129932 val_loss: 0.028023833\n",
      "Step = 776 train_loss: 0.00086118997 val_loss: 0.012723192\n",
      "Step = 777 train_loss: 0.00058332644 val_loss: 0.004693782\n",
      "Step = 778 train_loss: 0.00045359432 val_loss: 0.025210971\n",
      "Step = 779 train_loss: 0.0006168102 val_loss: 0.03156167\n",
      "Step = 780 train_loss: 0.0003945422 val_loss: 0.037253127\n",
      "Step = 781 train_loss: 0.0005643113 val_loss: 0.0434066\n",
      "Step = 782 train_loss: 0.00047701705 val_loss: 0.027424246\n",
      "Step = 783 train_loss: 0.0007174037 val_loss: 0.020716103\n",
      "Step = 784 train_loss: 0.00063575536 val_loss: 0.01965669\n",
      "Step = 785 train_loss: 0.0006208389 val_loss: 0.020560294\n",
      "Step = 786 train_loss: 0.00068176514 val_loss: 0.021989772\n",
      "Step = 787 train_loss: 0.00054649595 val_loss: 0.021718599\n",
      "Step = 788 train_loss: 0.0007620809 val_loss: 0.032809522\n",
      "Step = 789 train_loss: 0.0005550921 val_loss: 0.062161054\n",
      "Step = 790 train_loss: 0.0003386961 val_loss: 0.09543481\n",
      "Step = 791 train_loss: 0.00047588567 val_loss: 0.11094707\n",
      "Step = 792 train_loss: 0.0007054212 val_loss: 0.042847313\n",
      "Step = 793 train_loss: 0.0006897467 val_loss: 0.013642407\n",
      "Step = 794 train_loss: 0.0007123149 val_loss: 0.01081172\n",
      "Step = 795 train_loss: 0.0005864309 val_loss: 0.010001351\n",
      "Step = 796 train_loss: 0.00051452394 val_loss: 0.01178334\n",
      "Step = 797 train_loss: 0.00042149978 val_loss: 0.0311971\n",
      "Step = 798 train_loss: 0.00027119892 val_loss: 0.075245924\n",
      "Step = 799 train_loss: 0.0007184352 val_loss: 0.107899584\n",
      "Step = 800 train_loss: 0.0006643459 val_loss: 0.090952\n",
      "Step = 801 train_loss: 0.0008700248 val_loss: 0.035275064\n",
      "Step = 802 train_loss: 0.0006270411 val_loss: 0.008168064\n",
      "Step = 803 train_loss: 0.0003471075 val_loss: 0.0037708923\n",
      "Step = 804 train_loss: 0.00061044417 val_loss: 0.0041092103\n",
      "Step = 805 train_loss: 0.00030942593 val_loss: 0.0034676986\n",
      "Step = 806 train_loss: 0.00068366836 val_loss: 0.004562511\n",
      "Step = 807 train_loss: 0.0005657397 val_loss: 0.0053081485\n",
      "Step = 808 train_loss: 0.00048102817 val_loss: 0.005540423\n",
      "Step = 809 train_loss: 0.00056911685 val_loss: 0.00588005\n",
      "Step = 810 train_loss: 0.00037069776 val_loss: 0.004263063\n",
      "Step = 811 train_loss: 0.000744513 val_loss: 0.006055942\n",
      "Step = 812 train_loss: 0.00067652564 val_loss: 0.0028801416\n",
      "Step = 813 train_loss: 0.00039731638 val_loss: 0.025173414\n",
      "Step = 814 train_loss: 0.00060205057 val_loss: 0.033061862\n",
      "Step = 815 train_loss: 0.0005285713 val_loss: 0.005524949\n",
      "Step = 816 train_loss: 0.0003962691 val_loss: 0.0073748375\n",
      "Step = 817 train_loss: 0.00049717276 val_loss: 0.01591014\n",
      "Step = 818 train_loss: 0.000504879 val_loss: 0.023992406\n",
      "Step = 819 train_loss: 0.0003795375 val_loss: 0.023524798\n",
      "Step = 820 train_loss: 0.00043900387 val_loss: 0.015824726\n",
      "Step = 821 train_loss: 0.0006099911 val_loss: 0.0009934246\n",
      "Step = 822 train_loss: 0.0005019018 val_loss: 0.007047454\n",
      "Step = 823 train_loss: 0.00059122057 val_loss: 0.020787966\n",
      "Step = 824 train_loss: 0.0008039714 val_loss: 0.016638504\n",
      "Step = 825 train_loss: 0.0005777757 val_loss: 0.023404222\n",
      "Step = 826 train_loss: 0.00076303136 val_loss: 0.029785253\n",
      "Step = 827 train_loss: 0.0006929784 val_loss: 0.017985987\n",
      "Step = 828 train_loss: 0.00040131694 val_loss: 0.0019614869\n",
      "Step = 829 train_loss: 0.0006863725 val_loss: 0.010224452\n",
      "Step = 830 train_loss: 0.0006110374 val_loss: 0.01872961\n",
      "Step = 831 train_loss: 0.00045220339 val_loss: 0.0215164\n",
      "Step = 832 train_loss: 0.0006156783 val_loss: 0.01269724\n",
      "Step = 833 train_loss: 0.000431969 val_loss: 0.008138888\n",
      "Step = 834 train_loss: 0.00028133986 val_loss: 0.007994512\n",
      "Step = 835 train_loss: 0.00038479667 val_loss: 0.008319704\n",
      "Step = 836 train_loss: 0.0003667464 val_loss: 0.009642418\n",
      "Step = 837 train_loss: 0.0005359915 val_loss: 0.018503975\n",
      "Step = 838 train_loss: 0.00032667338 val_loss: 0.029382186\n",
      "Step = 839 train_loss: 0.00043745217 val_loss: 0.035590008\n",
      "Step = 840 train_loss: 0.00057076814 val_loss: 0.039668843\n",
      "Step = 841 train_loss: 0.00078905316 val_loss: 0.03457563\n",
      "Step = 842 train_loss: 0.000869594 val_loss: 0.012374302\n",
      "Step = 843 train_loss: 0.00039587292 val_loss: 0.043133445\n",
      "Step = 844 train_loss: 0.0005831822 val_loss: 0.086718395\n",
      "Step = 845 train_loss: 0.00078607356 val_loss: 0.029988216\n",
      "Step = 846 train_loss: 0.0007415562 val_loss: 0.0034296077\n",
      "Step = 847 train_loss: 0.00056082703 val_loss: 0.005635675\n",
      "Step = 848 train_loss: 0.0007083512 val_loss: 0.009834081\n",
      "Step = 849 train_loss: 0.0006232607 val_loss: 0.009527068\n",
      "Step = 850 train_loss: 0.00046828447 val_loss: 0.009325052\n",
      "Step = 851 train_loss: 0.0004547704 val_loss: 0.0095635\n",
      "Step = 852 train_loss: 0.0006933861 val_loss: 0.008700979\n",
      "Step = 853 train_loss: 0.0005458058 val_loss: 0.009410234\n",
      "Step = 854 train_loss: 0.00087930716 val_loss: 0.010574929\n",
      "Step = 855 train_loss: 0.0004906781 val_loss: 0.014812861\n",
      "Step = 856 train_loss: 0.0005268211 val_loss: 0.018811557\n",
      "Step = 857 train_loss: 0.0007074069 val_loss: 0.013496758\n",
      "Step = 858 train_loss: 0.00025477188 val_loss: 0.009678467\n",
      "Step = 859 train_loss: 0.0006993032 val_loss: 0.03810648\n",
      "Step = 860 train_loss: 0.0004628637 val_loss: 0.03575542\n",
      "Step = 861 train_loss: 0.00078602385 val_loss: 0.011517656\n",
      "Step = 862 train_loss: 0.00066122314 val_loss: 0.016861893\n",
      "Step = 863 train_loss: 0.00031994487 val_loss: 0.027753046\n",
      "Step = 864 train_loss: 0.00046323068 val_loss: 0.037727434\n",
      "Step = 865 train_loss: 0.0006112605 val_loss: 0.03544695\n",
      "Step = 866 train_loss: 0.00076307426 val_loss: 0.017740011\n",
      "Step = 867 train_loss: 0.00035445185 val_loss: 0.012427964\n",
      "Step = 868 train_loss: 0.00062434 val_loss: 0.089196496\n",
      "Step = 869 train_loss: 0.00039807495 val_loss: 0.1799085\n",
      "Step = 870 train_loss: 0.00055073074 val_loss: 0.09608263\n",
      "Step = 871 train_loss: 0.00091969897 val_loss: 0.014766126\n",
      "Step = 872 train_loss: 0.0007647077 val_loss: 0.021720598\n",
      "Step = 873 train_loss: 0.0006569909 val_loss: 0.03267742\n",
      "Step = 874 train_loss: 0.000757746 val_loss: 0.037754934\n",
      "Step = 875 train_loss: 0.0004181185 val_loss: 0.034604475\n",
      "Step = 876 train_loss: 0.00040733008 val_loss: 0.028078722\n",
      "Step = 877 train_loss: 0.0008106452 val_loss: 0.013101614\n",
      "Step = 878 train_loss: 0.00032457343 val_loss: 0.012255067\n",
      "Step = 879 train_loss: 0.00063480495 val_loss: 0.017109308\n",
      "Step = 880 train_loss: 0.00049817027 val_loss: 0.024450628\n",
      "Step = 881 train_loss: 0.0004507479 val_loss: 0.035073537\n",
      "Step = 882 train_loss: 0.00036840406 val_loss: 0.03825851\n",
      "Step = 883 train_loss: 0.0004951012 val_loss: 0.036436822\n",
      "Step = 884 train_loss: 0.00045303156 val_loss: 0.025844231\n",
      "Step = 885 train_loss: 0.00055593526 val_loss: 0.021706851\n",
      "Step = 886 train_loss: 0.0004037061 val_loss: 0.021399394\n",
      "Step = 887 train_loss: 0.00048716037 val_loss: 0.01864768\n",
      "Step = 888 train_loss: 0.001276419 val_loss: 0.017190121\n",
      "Step = 889 train_loss: 0.0002809187 val_loss: 0.016740846\n",
      "Step = 890 train_loss: 0.0005918199 val_loss: 0.016878912\n",
      "Step = 891 train_loss: 0.0005029009 val_loss: 0.01719338\n",
      "Step = 892 train_loss: 0.0005447884 val_loss: 0.017697714\n",
      "Step = 893 train_loss: 0.00039663538 val_loss: 0.018303033\n",
      "Step = 894 train_loss: 0.00029040396 val_loss: 0.018974073\n",
      "Step = 895 train_loss: 0.000415685 val_loss: 0.030277018\n",
      "Step = 896 train_loss: 0.0004222132 val_loss: 0.056003563\n",
      "Step = 897 train_loss: 0.00079354446 val_loss: 0.05693406\n",
      "Step = 898 train_loss: 0.00034198785 val_loss: 0.037862018\n",
      "Step = 899 train_loss: 0.00045919683 val_loss: 0.021652391\n",
      "Step = 900 train_loss: 0.00041932584 val_loss: 0.0093827685\n",
      "Step = 901 train_loss: 0.00048437415 val_loss: 0.009895513\n",
      "Step = 902 train_loss: 0.0004302215 val_loss: 0.012536754\n",
      "Step = 903 train_loss: 0.00091179885 val_loss: 0.0049341135\n",
      "Step = 904 train_loss: 0.00049144286 val_loss: 0.015612975\n",
      "Step = 905 train_loss: 0.0005157384 val_loss: 0.10379018\n",
      "Step = 906 train_loss: 0.0005166041 val_loss: 0.18346784\n",
      "Step = 907 train_loss: 0.0004554461 val_loss: 0.15202755\n",
      "Step = 908 train_loss: 0.00075085566 val_loss: 0.08560179\n",
      "Step = 909 train_loss: 0.0006480105 val_loss: 0.046262592\n",
      "Step = 910 train_loss: 0.0005935372 val_loss: 0.006963902\n",
      "Step = 911 train_loss: 0.0006503406 val_loss: 0.003939113\n",
      "Step = 912 train_loss: 0.00065388106 val_loss: 0.0063367793\n",
      "Step = 913 train_loss: 0.0007905882 val_loss: 0.0042271786\n",
      "Step = 914 train_loss: 0.00082589785 val_loss: 0.0032232408\n",
      "Step = 915 train_loss: 0.00042222487 val_loss: 0.0054852134\n",
      "Step = 916 train_loss: 0.00065475027 val_loss: 0.004327903\n",
      "Step = 917 train_loss: 0.00035719058 val_loss: 0.003631212\n",
      "Step = 918 train_loss: 0.00045927663 val_loss: 0.005176533\n",
      "Step = 919 train_loss: 0.00039345276 val_loss: 0.004333814\n",
      "Step = 920 train_loss: 0.0005165319 val_loss: 0.020479662\n",
      "Step = 921 train_loss: 0.0003051045 val_loss: 0.011827477\n",
      "Step = 922 train_loss: 0.00054785545 val_loss: 0.006674507\n",
      "Step = 923 train_loss: 0.0005496299 val_loss: 0.019298295\n",
      "Step = 924 train_loss: 0.0004389098 val_loss: 0.013837511\n",
      "Step = 925 train_loss: 0.0005715306 val_loss: 0.0040385486\n",
      "Step = 926 train_loss: 0.0005642782 val_loss: 0.009967099\n",
      "Step = 927 train_loss: 0.00031296205 val_loss: 0.023948\n",
      "Step = 928 train_loss: 0.00025003456 val_loss: 0.045160137\n",
      "Step = 929 train_loss: 0.00041505272 val_loss: 0.059672378\n",
      "Step = 930 train_loss: 0.00048685807 val_loss: 0.035445627\n",
      "Step = 931 train_loss: 0.00023514521 val_loss: 0.016960735\n",
      "Step = 932 train_loss: 0.0002906142 val_loss: 0.0085235825\n",
      "Step = 933 train_loss: 0.0005440164 val_loss: 0.0046396065\n",
      "Step = 934 train_loss: 0.00070889475 val_loss: 0.007845234\n",
      "Step = 935 train_loss: 0.0002821169 val_loss: 0.008037674\n",
      "Step = 936 train_loss: 0.00042440483 val_loss: 0.0049863067\n",
      "Step = 937 train_loss: 0.00046851646 val_loss: 0.010765967\n",
      "Step = 938 train_loss: 0.00030389384 val_loss: 0.021437423\n",
      "Step = 939 train_loss: 0.00059514504 val_loss: 0.030429631\n",
      "Step = 940 train_loss: 0.0003529993 val_loss: 0.03260067\n",
      "Step = 941 train_loss: 0.00047072893 val_loss: 0.028749306\n",
      "Step = 942 train_loss: 0.00035845285 val_loss: 0.01567886\n",
      "Step = 943 train_loss: 0.00034221244 val_loss: 0.010786585\n",
      "Step = 944 train_loss: 0.0004303308 val_loss: 0.012781764\n",
      "Step = 945 train_loss: 0.00039195482 val_loss: 0.011565422\n",
      "Step = 946 train_loss: 0.00030482135 val_loss: 0.011832281\n",
      "Step = 947 train_loss: 0.00041766677 val_loss: 0.014623616\n",
      "Step = 948 train_loss: 0.0005007236 val_loss: 0.02123389\n",
      "Step = 949 train_loss: 0.00037639638 val_loss: 0.022477489\n",
      "Step = 950 train_loss: 0.0002186888 val_loss: 0.023614382\n",
      "Step = 951 train_loss: 0.00042831243 val_loss: 0.02023348\n",
      "Step = 952 train_loss: 0.0005440547 val_loss: 0.012660293\n",
      "Step = 953 train_loss: 0.0004738252 val_loss: 0.018649455\n",
      "Step = 954 train_loss: 0.0003031688 val_loss: 0.06671563\n",
      "Step = 955 train_loss: 0.00051453797 val_loss: 0.06350146\n",
      "Step = 956 train_loss: 0.00048951537 val_loss: 0.023083365\n",
      "Step = 957 train_loss: 0.00055912364 val_loss: 0.003944383\n",
      "Step = 958 train_loss: 0.00050410733 val_loss: 0.0036168739\n",
      "Step = 959 train_loss: 0.0002263092 val_loss: 0.009047556\n",
      "Step = 960 train_loss: 0.00033186766 val_loss: 0.011474144\n",
      "Step = 961 train_loss: 0.00034466956 val_loss: 0.017927404\n",
      "Step = 962 train_loss: 0.00041594976 val_loss: 0.015595\n",
      "Step = 963 train_loss: 0.00030204866 val_loss: 0.014995252\n",
      "Step = 964 train_loss: 0.00042112684 val_loss: 0.018007731\n",
      "Step = 965 train_loss: 0.0005328822 val_loss: 0.014515397\n",
      "Step = 966 train_loss: 0.00047028906 val_loss: 0.012686927\n",
      "Step = 967 train_loss: 0.0004299893 val_loss: 0.01230491\n",
      "Step = 968 train_loss: 0.0003481454 val_loss: 0.013987694\n",
      "Step = 969 train_loss: 0.0007490909 val_loss: 0.0093363235\n",
      "Step = 970 train_loss: 0.000684699 val_loss: 0.0045140362\n",
      "Step = 971 train_loss: 0.00031595037 val_loss: 0.00674224\n",
      "Step = 972 train_loss: 0.0005807864 val_loss: 0.0053033186\n",
      "Step = 973 train_loss: 0.00055746024 val_loss: 0.0073418473\n",
      "Step = 974 train_loss: 0.00030004221 val_loss: 0.014333095\n",
      "Step = 975 train_loss: 0.00040980987 val_loss: 0.02302572\n",
      "Step = 976 train_loss: 0.00051432435 val_loss: 0.027749581\n",
      "Step = 977 train_loss: 0.0006949958 val_loss: 0.024415331\n",
      "Step = 978 train_loss: 0.0007201751 val_loss: 0.0154431565\n",
      "Step = 979 train_loss: 0.0005022938 val_loss: 0.009113473\n",
      "Step = 980 train_loss: 0.00035899386 val_loss: 0.0071291816\n",
      "Step = 981 train_loss: 0.0003329764 val_loss: 0.007365206\n",
      "Step = 982 train_loss: 0.0005781835 val_loss: 0.003135965\n",
      "Step = 983 train_loss: 0.0005570518 val_loss: 0.0064355577\n",
      "Step = 984 train_loss: 0.00025882386 val_loss: 0.049729463\n",
      "Step = 985 train_loss: 0.0005432764 val_loss: 0.22447582\n",
      "Step = 986 train_loss: 0.0004888034 val_loss: 0.2643679\n",
      "Step = 987 train_loss: 0.0006521115 val_loss: 0.09061188\n",
      "Step = 988 train_loss: 0.0007832546 val_loss: 0.0006437015\n",
      "Step = 989 train_loss: 0.00049123215 val_loss: 0.023996856\n",
      "Step = 990 train_loss: 0.0006308205 val_loss: 0.03112056\n",
      "Step = 991 train_loss: 0.00072670786 val_loss: 0.03338837\n",
      "Step = 992 train_loss: 0.00067687134 val_loss: 0.027731469\n",
      "Step = 993 train_loss: 0.00061203004 val_loss: 0.0161124\n",
      "Step = 994 train_loss: 0.00034852105 val_loss: 0.01685403\n",
      "Step = 995 train_loss: 0.00040696654 val_loss: 0.026201954\n",
      "Step = 996 train_loss: 0.00045466243 val_loss: 0.028387805\n",
      "Step = 997 train_loss: 0.000574712 val_loss: 0.011796355\n",
      "Step = 998 train_loss: 0.00020601726 val_loss: 0.016990954\n",
      "Step = 999 train_loss: 0.00026045323 val_loss: 0.028827906\n",
      "625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_31812\\230823248.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_31812\\230823248.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.06297971\n",
      "mpe_val: 0.06487053\n",
      "mpe_a: 0.10443372808825281\n",
      "mpe_b: 0.11632744127639423\n",
      "rmse_train: 53.33674\n",
      "rmse_val: 55.007095\n",
      "rmse_a: 87.58928998987395\n",
      "rmse_b: 158.19394741898313\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "val_losses=[]\n",
    "\n",
    "for t in range(1000):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    '''save all the model in each epoch'''    \n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, 'Target_model/net_parameters'+str(t)+'.pkl')\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print ('Step = %d' % t, 'train_loss:',train_loss.data.numpy(),'val_loss:',val_loss.data.numpy())\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "   \n",
    "'''find the best model'''\n",
    "best_index=val_losses.index(np.min(val_losses))\n",
    "print(best_index)\n",
    "\n",
    "'''copy the best model from model file to the model_test file'''\n",
    "if 1:\n",
    "    shutil.copyfile('Target_model/net_parameters'+str(best_index)+'.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "'''Reload the best model''' \n",
    "model = Net()\n",
    "a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
    "optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
