Old project with huge speeding, deposited is the only destination.
# 电池寿命预测模型演进报告

## 1. 项目背景
基于电池充放电数据(Qd)预测电池循环寿命，解决传统方法耗时耗力的问题。

## 2. 开发者决策分析

### t1.ipynb (基础CNN)
**开发者思考：**
1. 选择CNN的原因：
   - 数据具有局部相关性（充放电曲线）
   - 希望自动提取特征，避免手工设计
2. 初始参数设置：
   - 保守的学习率(0.00001)防止震荡
   - 5层卷积逐步抽象特征

### t5.ipynb (Deep-Wide)
**实验意图：**
1. 尝试解决CNN的梯度消失问题
2. 通过并行结构增加网络容量
3. 失败原因分析：
   - 电池数据时序性强，需要更精细的特征提取

### t10.ipynb (GoogleNet改进)
**设计考量：**
1. 采用Inception模块的动机：
   - 多尺度特征融合
   - 减少参数量
2. 新增模型保存：
   - 方便回溯最佳模型
   - 避免训练中断损失

### t15.ipynb (CBAM注意力)
**创新点思考：**
1. 注意力机制引入：
   - 关注关键充放电阶段
   - 抑制噪声干扰
2. 学习率调整策略：
   - 发现模型收敛慢
   - 尝试动态调整提升效率

### t20.ipynb (模块化设计)
**工程优化：**
1. 模块化目的：
   - 提高代码复用性
   - 便于单独调试组件
2. SE Block选择：
   - 通道注意力更适合电池数据
   - 轻量化设计避免过拟合

### cnn-lstm.ipynb (最终方案)
**架构决策：**
1. 混合架构理由：
   - CNN处理局部特征
   - LSTM捕捉充放电循环模式
2. 参数优化：
   - 增大学习率加速收敛
   - Early Stopping防止过拟合

## 3. 关键决策转折点
1. 从CNN到CNN-LSTM：
   - 认识到纯空间特征提取的局限性
   - 时序建模显著提升测试集B表现

2. 注意力机制应用：
   - 发现某些充放电阶段更具预测性
   - 通过注意力加权重要时段

3. 工程实践改进：
   - 版本控制（模型保存）
   - 模块化开发
   - 自动化训练流程

## 4. 开发者经验总结
1. 有效策略：
   - 渐进式复杂度增加
   - 模块化验证组件效果
   - 严格的验证集监控

2. 失败教训：
   - 过宽的网络不利于时序建模
   - 单纯增加深度会导致梯度问题
   - 需要平衡模型复杂度和数据量

## 5. 完整决策路线图
| 版本 | 主要决策点 | 技术权衡 | 效果验证 |
|------|----------|---------|---------|
| t1   | 基础CNN架构 | 简单vs效果 | 验证CNN可行性 |
| t5   | 宽度优先 | 并行结构vs梯度流动 | 发现宽度限制 |
| t10  | 多尺度特征 | 计算量vs特征丰富度 | 提升A集表现 |
| t15  | 注意力机制 | 额外参数vs特征选择 | 显著降低方差 |
| t20  | 模块化设计 | 开发效率vs灵活性 | 加速迭代 |
| 最终版 | 时空混合 | 复杂度vs泛化性 | 最佳平衡 |
## 1. 项目背景
基于电池充放电数据(Qd)预测电池循环寿命，解决传统方法耗时耗力的问题。

## 2. 详细版本演进分析

### 2.1 t1.ipynb (基础CNN)
**修改内容：**
1. 网络结构：
```python
class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5,50))
        self.bn1 = nn.BatchNorm2d(8)
        # ...5层卷积结构...
```
2. 参数设置：
- 学习率: 0.00001 → 0.0001 (后续调整)
- 测试集划分比例: 0.36 → 0.2 (最终版本)

### 2.2 t5.ipynb (Deep-Wide)
**关键修改：**
1. 结构调整：
- 将单路CNN改为双路并行结构
- 增加网络宽度但减少深度
2. 新增模块：
```python
class WideBlock(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(16, 32, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 5, padding=2) 
```

### 2.3 t10.ipynb (GoogleNet改进)
**主要变更：**
1. 引入Inception模块：
```python 
class InceptionModule(nn.Module):
    def __init__(self):
        # 1x1, 3x3, 5x5卷积并行
        self.branch1 = nn.Conv2d(32, 16, 1)
        self.branch2 = nn.Sequential(
            nn.Conv2d(32, 16, 1),
            nn.Conv2d(16, 16, 3, padding=1)
        )
```
2. 新增功能：
- 模型保存机制(max_models_to_keep=10)

### 2.4 t15.ipynb (CBAM注意力)
**重要改进：**
1. 新增注意力模块：
```python
class CBAM(nn.Module):
    def __init__(self):
        self.channel_attention = ChannelAttention()
        self.spatial_attention = SpatialAttention()
```
2. 训练优化：
- 学习率调度器：
```python
scheduler = torch.optim.lr_scheduler.LambdaLR(
    optimizer, 
    lr_lambda=lambda epoch: 0.8 ** (epoch//40)
)
```

### 2.5 t20.ipynb (模块化设计)
**架构升级：**
1. 模块化重构：
```python
class SEBlock(nn.Module):
    def __init__(self):
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(64, 4),
            nn.ReLU(),
            nn.Linear(4, 64),
            nn.Sigmoid()
        )
```
2. 工程优化：
- 使用einops库简化维度操作

### 2.6 cnn-lstm.ipynb (最终方案)
**核心调整：**
1. 混合架构：
```python
class CNNLSTMModel(nn.Module):
    def __init__(self):
        self.cnn = CNNBackbone()  # 5层CNN
        self.lstm = nn.LSTM(64, 64, 2)  # 2层LSTM
```
2. 关键参数：
- 学习率: 0.001 (提高10倍)
- Early Stopping: patience=3
- Batch Size: 32 → 64

## 3. 完整演进路线图
| 版本 | 主要改进 | 代码变更行数 | 性能提升 |
|------|---------|------------|---------|
| t1   | 基础CNN | 200+       | 基准    |
| t5   | 加宽网络 | +150       | ↓15%    |
| t10  | Inception | +300     | ↑5% (A集) |
| t15  | 注意力机制 | +400    | ↑8%     |
| t20  | 模块化   | +500       | ↑12%    |
| 最终版 | CNN-LSTM | +600      | ↑30%    |

## 4. 关键代码变更示例
1. LSTM集成代码：
```python
# 旧版 (纯CNN)
x = self.cnn(x)
return x

# 新版 (CNN-LSTM)
x = self.cnn(x)  # [bs, c, h, w]
x = x.flatten(2) # [bs, c, seq_len]
x = x.permute(2, 0, 1) # [seq_len, bs, c]
_, (h_n, _) = self.lstm(x)  # 加入LSTM处理
```

2. 注意力机制实现：
```python
# CBAM模块前向传播
def forward(self, x):
    ca = self.channel_attention(x)
    sa = self.spatial_attention(x)
    return ca * sa * x  # 双重注意力
```
## 1. 项目背景
基于电池充放电数据(Qd)预测电池循环寿命，解决传统方法耗时耗力的问题。

## 2. 模型演进路径

### 2.1 初始阶段 (t1.ipynb)
- 架构：基础CNN网络
- 特点：
  - 学习率0.00001
  - 测试集划分比例0.36
- 性能：
  - 训练集R2: 0.906
  - 测试集B R2: 0.516

### 2.2 架构探索 (t5.ipynb)
- 尝试Deep-Wide结构
- 结果：性能下降(R2训练集0.651)
- 结论：复杂CNN结构不适用

### 2.3 经典网络改进 (t10.ipynb)
- 基于GoogleNet修改
- 新增模型保存机制
- 性能基准：
  - 训练集R2: 0.994(存在过拟合)

### 2.4 注意力机制引入 (t15.ipynb)
- 关键改进：
  - 加入CBAM注意力模块
  - 学习率调整到0.0001
  - 引入动态学习率衰减

### 2.5 模块化设计 (t20.ipynb)
- 主要特点：
  - Inception模块
  - SE注意力机制
  - 高度模块化设计

### 2.6 最终方案 (cnn-lstm.ipynb)
- 混合架构：
  - CNN提取空间特征
  - LSTM处理时序依赖
- 参数：
  - 学习率0.001
  - Early Stopping(patience=3)
- 优势：
  - 测试集B R2提升30%
  - 更好的泛化能力

## 3. 关键结论
1. 纯CNN结构容易过拟合
2. 注意力机制能提升特征提取能力
3. 时序建模(LSTM)显著改善预测效果
4. 学习率0.001配合Early Stopping最优

## 4. 后续优化方向
1. 尝试Transformer架构
2. 引入更多工程特征
3. 优化数据增强策略
# 电池寿命预测测试文档

## 优化想法

增加模型复杂度
对于a，b数据集，数据集到底有什么差别？能不能分别建立模型？
lstm并不是一个很好的选择
通道注意力加在lstm之前，空间注意力在第两次卷积之后

## 实验建议

### a， b分开建立模型，a在t7里面从0.946提升到了0.948

### Coding improvement

1. Hard-disk saving method
2. CSA method
3. 

## Outline

1. 老模型
2. googlenet
3. resnet
4. 

## 原来的模型

train_loss: 0.0007874367 val_loss: 0.0006076957

### 老的结构

1. 五层卷积层，卷积层内部BN层，池化层，leakrelu激活函数
2. lstm层
3. 用全连接层，2层，第一层relu激活函数，第二层sigmoid激活函数
4. 返回x

### 卷积层具体参数

1. 1,8,50,50,5,40
2. 8,16,2,3,1
3. 16,32,2,3,1
4. 32,64,2,2,1
5. 64,64,1,2,1


### 结果

对比（非lstm架构）
r2_train: 0.9947811961174011
r2_val: 0.9749422669410706
r2_a: 0.9464100454957816
r2_b: 0.6836000724611211

![alt text](image.png)

## 原来基础上面的改进

### t1.在末端加入lstm模型

#### 结果

r2_train: 0.9062628746032715
r2_val: 0.8613404631614685
r2_a: 0.9024388593844427
r2_b: 0.5164209698966113

![alt text](image-1.png)

### t2.减少卷积层数量

深度不够，不足以捕捉到全局特征，因此减少卷积层数量是错误的

#### 结果

r2_train: 0.780303955078125
r2_val: 0.757173478603363
r2_a: 0.7901904334717177
r2_b: 0.38925433189972136

![alt text](image-2.png)

### t3.增加卷积层数量

增加卷积层数量，提高模型的复杂度，增加模型的非线性
拟合程度严重下降，作者第二数据集他妈的是不是造假的
#### 结果

r2_train: 0.9049155116081238
r2_val: 0.8364689946174622
r2_a: 0.9160007727042746
r2_b: -0.204700199031272

![alt text](image-3.png)

## deep-wide模型

### 基础结构

卷积层和lstm层并列，以捕捉各个时间段的特征

### t4.改用deep-wide模型

#### 结构

1. 五层卷积层，卷积层内部BN层，池化层，leakrelu激活函数
2. 单层lstm层
3. 用全连接层，2层，第一层relu激活函数，第二层sigmoid激活函数
4. 返回x

#### 结果

r2_train: 0.6286368370056152
r2_val: 0.6448044180870056
r2_a: 0.6082580908824828
r2_b: 0.09643709934666189

![alt text](image-4.png)

### t5.修改deep-wide模型

#### 结构

三层卷积再分架构，总分总

#### 结果

r2_train: 0.6515334844589233
r2_val: 0.6068358421325684
r2_a: 0.6229705199002005
r2_b: 0.4175009753657175

![alt text](image-5.png)

### t6.增加连接层

增加连接层，提高模型的非线性

#### 结果

r2_train: 0.6515334844589233
r2_val: 0.6068358421325684
r2_a: 0.6229705199002005
r2_b: 0.4175009753657175

全连接层数过多，三层全连接，拟合严重下降
r2_train: -0.042836546897888184
r2_val: -0.22020554542541504
r2_a: -0.012773520355835455
r2_b: -0.03456034262445051
![alt text](image-6.png)

------------------------------------------------

## Google Net

### googlenet

![alt text](image-7.png)

首先是三层卷积，然后是inception模块，inception模块由四个分支组成，每个分支都有不同卷积核，然后再concat到一起，然后再做BN和relu，最后再做平均池化，再做全连接层。

**lstm放在哪里？**

### 基础模型，t7

不加入lstm结构,并且在t7模型里面修改了保存模型前十的代码，免得炸硬盘

#### 结果

r2_train: 0.9642801284790039
r2_val: 0.937027096748352
r2_a: 0.9487450590108414
r2_b: -0.3664594028772752

#### 解释

对于b数据集都是很逆天的数值啊！对于b数据集区间确实这个并不是一个好的模型，但是对于a数据集，这个模型的表现还是很不错的。
可以加一下lstm层试试，看看能不能提升一下中间层数据的遗忘问题
![alt text](image-8.png)

### t8.直接在后面的cnn层里面加入lstm层

每一个后面都加lstm，注意在cnn输出之后是4d，但是lstm只接受3d
每一层加不了，只能在最后一层加lstm，模型不能降低维度，lstm里面两层隐藏层就去掉一层dense层

#### 结果

r2_train: 0.37430518865585327
r2_val: 0.328583300113678
r2_a: 0.3304816952119657
r2_b: 0.1727084187753034

![alt text](image-9.png)

#### 解释

模型在底层估值偏高，可以跟其他偏低的模型合并一下
或者直接加一层深度，lstm貌似降低了模型的复杂度，但是并没有提升模型的准确度

### t9.把inception模块前面的公用卷积层加深一层

#### 结果

r2_train: 0.37430518865585327
r2_val: 0.328583300113678
r2_a: 0.3304816952119657
r2_b: 0.1727084187753034
![alt text](image-10.png)
#### 解释

目前模型的深度，不能升高了，升高会遗忘数据，而且模型的准确度也会下降，因此不建议这样做
并且跟t8结果一致
换一下其他东西来

### t10.增加lstm层

#### 结果

r2_train: 0.6651217341423035
r2_val: 0.6585894227027893
r2_a: 0.5952010329940542
r2_b: 0.2590036137604501

![alt text](image-11.png)

### t.11 增加注意力机制,EAC layer

#### 结果

r2_train: -1.334923505783081
r2_val: -2.1382460594177246
r2_a: -1.247219262209173
r2_b: -0.19222578991129535

### t.12 直接在原基础上加注意力,添加退火算法,anchor

余弦退火算法不太好啊
![alt text](image-15.png)

#### 结果

r2_train: 0.6538496017456055
r2_val: 0.6645767688751221
r2_a: 0.6417636985700611
r2_b: 0.3985379421089278

![alt text](image-13.png)

### t.15 CBAM

#### 非lstm结构

r2_train: 0.5624848008155823
r2_val: 0.5904593467712402
r2_a: 0.5453239678910258
r2_b: 0.3088849502555602
![alt text](image-14.png)

#### lstm结构

r2_train: 0.0057558417320251465
r2_val: -0.04023587703704834
r2_a: 0.010478782863896807
r2_b: -1.003607048239521


r2_train: 0.7859207391738892
r2_val: 0.8685193061828613
r2_a: 0.877216659949271
r2_b: -0.19765214711434442
![alt text](image-16.png)
---

### t.16 新的基准模型

r2_train: 0.8834208250045776
r2_val: 0.817699670791626
r2_a: 0.8904541430809176
r2_b: 0.3611642297971548

### t.19 融合板块，效果并不好，单独使用

### t.20 在googlenet的基础上，增加CBAM模块

Best model val_loss: 0.0029008272103965282
r2_train: 0.916574239730835
r2_val: 0.8610522747039795
r2_a: 0.9197794929374496
r2_b: 0.30184489455758945
![alt text](image-17.png)

### t.21

### t.22

Best model val_loss: 0.0021987545769661665
r2_train: 0.9278562068939209
r2_val: 0.894588828086853
r2_a: 0.9204749834864762
r2_b: 0.3626542944451242
![alt text](image-18.png)

### t.23

r2_train: 0.9754045605659485
r2_val: 0.967309296131134
r2_a: 0.9399577096441425
r2_b: 0.6718936079079099

### t.24

### t.25

## resnet architecture

### resnet-32

#### brief explaination, based on t7 to develop

![alt text](image-12.png)

Working Process

1. Design of Residual Unit
2. Conv 3 layers
3. input and output

