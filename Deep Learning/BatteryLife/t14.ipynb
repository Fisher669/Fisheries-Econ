{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "class EcaLayer(nn.Module):\n",
    "    \"\"\"Constructs a ECA module.\"\"\"\n",
    "    \n",
    "    def __init__(self, channel, k_size=3):\n",
    "        super(EcaLayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)  # 使用平均池化\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        return x * self.sigmoid(y).expand_as(x)  # 计算ECA模块输出\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(8, 16, kernel_size=(2, 3)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(16, 32, kernel_size=(2, 3)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.eca1 = EcaLayer(32)\n",
    "        self.layer2 =nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(2, 2)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=(1, 2)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(128,256,kernel_size=(1,2)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(256, 32, batch_first=True)\n",
    "        self.fc1 = nn.Linear(32, 32)  # 修正为与LSTM输出相匹配\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(p=0.2)  # 添加Dropout层\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            x = self.layers1(x)\n",
    "            x = self.eca1(x)\n",
    "            x = self.layer2(x)\n",
    "            # print(x.shape)\n",
    "            x = to_3d(x)  # 转换为3D\n",
    "            # print(x.shape)\n",
    "            x, _ = self.lstm(x)  # LSTM处理\n",
    "            # print(x.shape)\n",
    "            x = x[:, -1, :].unsqueeze(2).unsqueeze(3)  # 取最后一个时序输出\n",
    "            # print(x.shape)\n",
    "            # x = x[:,-1,:]\n",
    "            x = x.view(x.size(0), -1)  # 压缩为2d\n",
    "            # print(x.shape)\n",
    "            x = torch.relu(self.fc1(x))  # 压缩多余维度\n",
    "            x = self.dropout(x)  # 使用定义的Dropout层\n",
    "            return torch.sigmoid(self.fc2(x))\n",
    "        except RuntimeError as e:\n",
    "            print(f\"运行时错误：{e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"发生意外错误：{e}\")\n",
    "            raise  # 重新抛出异常，以便进一步处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (12): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (eca1): EcaLayer(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (conv): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(256, 32, batch_first=True)\n",
      "  (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Step = 0, train_loss: 0.06595000624656677, val_loss: 0.06753279268741608\n",
      "Step = 1, train_loss: 0.07055023312568665, val_loss: 0.06792967021465302\n",
      "Step = 2, train_loss: 0.0680525004863739, val_loss: 0.0681668221950531\n",
      "Step = 3, train_loss: 0.06774706393480301, val_loss: 0.06824491918087006\n",
      "Step = 4, train_loss: 0.06955386698246002, val_loss: 0.06842631101608276\n",
      "Step = 5, train_loss: 0.0671163871884346, val_loss: 0.06860597431659698\n",
      "Step = 6, train_loss: 0.06540640443563461, val_loss: 0.068796806037426\n",
      "Step = 7, train_loss: 0.06571848690509796, val_loss: 0.0689690038561821\n",
      "Step = 8, train_loss: 0.06442193686962128, val_loss: 0.0691448524594307\n",
      "Step = 9, train_loss: 0.06226426735520363, val_loss: 0.06934954226016998\n",
      "Step = 10, train_loss: 0.06339254230260849, val_loss: 0.06956686824560165\n",
      "Step = 11, train_loss: 0.06097913160920143, val_loss: 0.06986025720834732\n",
      "Step = 12, train_loss: 0.0647730678319931, val_loss: 0.07014376670122147\n",
      "Step = 13, train_loss: 0.05826626718044281, val_loss: 0.07039045542478561\n",
      "Step = 14, train_loss: 0.061877403408288956, val_loss: 0.0706552267074585\n",
      "Step = 15, train_loss: 0.05893329903483391, val_loss: 0.0709294006228447\n",
      "Step = 16, train_loss: 0.06153152510523796, val_loss: 0.07114621251821518\n",
      "Step = 17, train_loss: 0.05613931268453598, val_loss: 0.07137911766767502\n",
      "Step = 18, train_loss: 0.05883562192320824, val_loss: 0.07160593569278717\n",
      "Step = 19, train_loss: 0.05225386098027229, val_loss: 0.07183048129081726\n",
      "Step = 20, train_loss: 0.0563388466835022, val_loss: 0.07207538187503815\n",
      "Step = 21, train_loss: 0.05434674397110939, val_loss: 0.0723390206694603\n",
      "Step = 22, train_loss: 0.05263054743409157, val_loss: 0.07253829389810562\n",
      "Step = 23, train_loss: 0.05600150674581528, val_loss: 0.07253604382276535\n",
      "Step = 24, train_loss: 0.05353488773107529, val_loss: 0.07236627489328384\n",
      "Step = 25, train_loss: 0.0519384890794754, val_loss: 0.07208877056837082\n",
      "Step = 26, train_loss: 0.05062428489327431, val_loss: 0.07154770940542221\n",
      "Step = 27, train_loss: 0.05069519951939583, val_loss: 0.07080375403165817\n",
      "Step = 28, train_loss: 0.05049195513129234, val_loss: 0.06993623822927475\n",
      "Step = 29, train_loss: 0.04808643087744713, val_loss: 0.0690564215183258\n",
      "Step = 30, train_loss: 0.051112864166498184, val_loss: 0.06808061897754669\n",
      "Step = 31, train_loss: 0.04596618190407753, val_loss: 0.06711766868829727\n",
      "Step = 32, train_loss: 0.05078596621751785, val_loss: 0.06614543497562408\n",
      "Step = 33, train_loss: 0.04603759944438934, val_loss: 0.06528442353010178\n",
      "Step = 34, train_loss: 0.045621611177921295, val_loss: 0.06439753621816635\n",
      "Step = 35, train_loss: 0.04667607322335243, val_loss: 0.06354858726263046\n",
      "Step = 36, train_loss: 0.04564760625362396, val_loss: 0.06264263391494751\n",
      "Step = 37, train_loss: 0.04360697418451309, val_loss: 0.06168900057673454\n",
      "Step = 38, train_loss: 0.045952823013067245, val_loss: 0.06073466315865517\n",
      "Step = 39, train_loss: 0.045506563037633896, val_loss: 0.05992887169122696\n",
      "Step = 40, train_loss: 0.04532979428768158, val_loss: 0.059175096452236176\n",
      "Step = 41, train_loss: 0.047177162021398544, val_loss: 0.058349497616291046\n",
      "Step = 42, train_loss: 0.04564603790640831, val_loss: 0.057457879185676575\n",
      "Step = 43, train_loss: 0.03959602117538452, val_loss: 0.056714918464422226\n",
      "Step = 44, train_loss: 0.046466726809740067, val_loss: 0.05589970201253891\n",
      "Step = 45, train_loss: 0.042501501739025116, val_loss: 0.05530888959765434\n",
      "Step = 46, train_loss: 0.04444480314850807, val_loss: 0.054708585143089294\n",
      "Step = 47, train_loss: 0.04341365396976471, val_loss: 0.05397370830178261\n",
      "Step = 48, train_loss: 0.03998812288045883, val_loss: 0.05328426882624626\n",
      "Step = 49, train_loss: 0.04055551439523697, val_loss: 0.052521444857120514\n",
      "Step = 50, train_loss: 0.039433803409338, val_loss: 0.05170725658535957\n",
      "Step = 51, train_loss: 0.03897014632821083, val_loss: 0.050899215042591095\n",
      "Step = 52, train_loss: 0.03643463924527168, val_loss: 0.050185784697532654\n",
      "Step = 53, train_loss: 0.0415194071829319, val_loss: 0.049441345036029816\n",
      "Step = 54, train_loss: 0.04003928229212761, val_loss: 0.04891473427414894\n",
      "Step = 55, train_loss: 0.0388251468539238, val_loss: 0.04842715710401535\n",
      "Step = 56, train_loss: 0.03943021595478058, val_loss: 0.047933049499988556\n",
      "Step = 57, train_loss: 0.0381980761885643, val_loss: 0.047417208552360535\n",
      "Step = 58, train_loss: 0.03820696845650673, val_loss: 0.04709789529442787\n",
      "Step = 59, train_loss: 0.04536912590265274, val_loss: 0.04661179706454277\n",
      "Step = 60, train_loss: 0.03695642575621605, val_loss: 0.0461716465651989\n",
      "Step = 61, train_loss: 0.04088974744081497, val_loss: 0.04570955038070679\n",
      "Step = 62, train_loss: 0.038276318460702896, val_loss: 0.045154064893722534\n",
      "Step = 63, train_loss: 0.036332856863737106, val_loss: 0.04468819126486778\n",
      "Step = 64, train_loss: 0.035334400832653046, val_loss: 0.0443771667778492\n",
      "Step = 65, train_loss: 0.03622918948531151, val_loss: 0.043984297662973404\n",
      "Step = 66, train_loss: 0.038178760558366776, val_loss: 0.04355457425117493\n",
      "Step = 67, train_loss: 0.038025155663490295, val_loss: 0.0431416891515255\n",
      "Step = 68, train_loss: 0.0356457382440567, val_loss: 0.0426008366048336\n",
      "Step = 69, train_loss: 0.036097198724746704, val_loss: 0.04214950278401375\n",
      "Step = 70, train_loss: 0.03970625624060631, val_loss: 0.04164363071322441\n",
      "Step = 71, train_loss: 0.03542611375451088, val_loss: 0.04115649312734604\n",
      "Step = 72, train_loss: 0.032539449632167816, val_loss: 0.04071653261780739\n",
      "Step = 73, train_loss: 0.030869605019688606, val_loss: 0.04023071005940437\n",
      "Step = 74, train_loss: 0.03829209506511688, val_loss: 0.039909858256578445\n",
      "Step = 75, train_loss: 0.030797667801380157, val_loss: 0.039650578051805496\n",
      "Step = 76, train_loss: 0.03476885333657265, val_loss: 0.03950340673327446\n",
      "Step = 77, train_loss: 0.03332192450761795, val_loss: 0.03919527679681778\n",
      "Step = 78, train_loss: 0.03348131850361824, val_loss: 0.03876934200525284\n",
      "Step = 79, train_loss: 0.03762877732515335, val_loss: 0.03847134858369827\n",
      "Step = 80, train_loss: 0.03402823954820633, val_loss: 0.03814760968089104\n",
      "Step = 81, train_loss: 0.03576449304819107, val_loss: 0.0380234532058239\n",
      "Step = 82, train_loss: 0.029576104134321213, val_loss: 0.038096122443675995\n",
      "Step = 83, train_loss: 0.03760102018713951, val_loss: 0.03782721981406212\n",
      "Step = 84, train_loss: 0.03705175966024399, val_loss: 0.03740112483501434\n",
      "Step = 85, train_loss: 0.03246469795703888, val_loss: 0.03704364597797394\n",
      "Step = 86, train_loss: 0.030300119891762733, val_loss: 0.036750227212905884\n",
      "Step = 87, train_loss: 0.031739018857479095, val_loss: 0.03617241233587265\n",
      "Step = 88, train_loss: 0.03408660739660263, val_loss: 0.03634005784988403\n",
      "Step = 89, train_loss: 0.033393725752830505, val_loss: 0.03665954992175102\n",
      "Step = 90, train_loss: 0.030444761738181114, val_loss: 0.03690887242555618\n",
      "Step = 91, train_loss: 0.030529988929629326, val_loss: 0.03720356151461601\n",
      "Step = 92, train_loss: 0.0357779897749424, val_loss: 0.03713364526629448\n",
      "Step = 93, train_loss: 0.03284508362412453, val_loss: 0.037093810737133026\n",
      "Step = 94, train_loss: 0.039029479026794434, val_loss: 0.0371035672724247\n",
      "Step = 95, train_loss: 0.034444283694028854, val_loss: 0.03697438910603523\n",
      "Step = 96, train_loss: 0.030287934467196465, val_loss: 0.03674595057964325\n",
      "Step = 97, train_loss: 0.027431320399045944, val_loss: 0.03664640709757805\n",
      "Step = 98, train_loss: 0.03128761425614357, val_loss: 0.0367019847035408\n",
      "Step = 99, train_loss: 0.030233873054385185, val_loss: 0.036661725491285324\n",
      "Step = 100, train_loss: 0.03278733044862747, val_loss: 0.03641517460346222\n",
      "Step = 101, train_loss: 0.030919862911105156, val_loss: 0.036116428673267365\n",
      "Step = 102, train_loss: 0.032546911388635635, val_loss: 0.03616338595747948\n",
      "Step = 103, train_loss: 0.026413682848215103, val_loss: 0.03598008677363396\n",
      "Step = 104, train_loss: 0.03411507606506348, val_loss: 0.035795457661151886\n",
      "Step = 105, train_loss: 0.03391365706920624, val_loss: 0.03558584302663803\n",
      "Step = 106, train_loss: 0.02996709570288658, val_loss: 0.035451147705316544\n",
      "Step = 107, train_loss: 0.03252408653497696, val_loss: 0.035289399325847626\n",
      "Step = 108, train_loss: 0.037526171654462814, val_loss: 0.03531728312373161\n",
      "Step = 109, train_loss: 0.03225884214043617, val_loss: 0.03518645465373993\n",
      "Step = 110, train_loss: 0.03301999717950821, val_loss: 0.03501763194799423\n",
      "Step = 111, train_loss: 0.03069065697491169, val_loss: 0.03487682715058327\n",
      "Step = 112, train_loss: 0.03000570274889469, val_loss: 0.03463293984532356\n",
      "Step = 113, train_loss: 0.030333783477544785, val_loss: 0.03433123603463173\n",
      "Step = 114, train_loss: 0.03578566759824753, val_loss: 0.03403332829475403\n",
      "Step = 115, train_loss: 0.03184565156698227, val_loss: 0.033820297569036484\n",
      "Step = 116, train_loss: 0.03148152679204941, val_loss: 0.033727049827575684\n",
      "Step = 117, train_loss: 0.029940901324152946, val_loss: 0.033521443605422974\n",
      "Step = 118, train_loss: 0.029094994068145752, val_loss: 0.03328031301498413\n",
      "Step = 119, train_loss: 0.028587017208337784, val_loss: 0.0330888107419014\n",
      "Step = 120, train_loss: 0.0283239483833313, val_loss: 0.03293575718998909\n",
      "Step = 121, train_loss: 0.034540437161922455, val_loss: 0.03275274112820625\n",
      "Step = 122, train_loss: 0.030146777629852295, val_loss: 0.03270506486296654\n",
      "Step = 123, train_loss: 0.034841541200876236, val_loss: 0.0326535589993\n",
      "Step = 124, train_loss: 0.033218059688806534, val_loss: 0.032541681081056595\n",
      "Step = 125, train_loss: 0.034935470670461655, val_loss: 0.03242507576942444\n",
      "Step = 126, train_loss: 0.033648405224084854, val_loss: 0.03249673917889595\n",
      "Step = 127, train_loss: 0.030410174280405045, val_loss: 0.03223655745387077\n",
      "Step = 128, train_loss: 0.030456729233264923, val_loss: 0.032200876623392105\n",
      "Step = 129, train_loss: 0.030497726052999496, val_loss: 0.03218802064657211\n",
      "Step = 130, train_loss: 0.03558335080742836, val_loss: 0.03215892240405083\n",
      "Step = 131, train_loss: 0.030218062922358513, val_loss: 0.03209197148680687\n",
      "Step = 132, train_loss: 0.0313180610537529, val_loss: 0.03206297382712364\n",
      "Step = 133, train_loss: 0.026872392743825912, val_loss: 0.03199537470936775\n",
      "Step = 134, train_loss: 0.029787806794047356, val_loss: 0.03176344186067581\n",
      "Step = 135, train_loss: 0.028211984783411026, val_loss: 0.03154771402478218\n",
      "Step = 136, train_loss: 0.03239182010293007, val_loss: 0.03152768686413765\n",
      "Step = 137, train_loss: 0.03156990557909012, val_loss: 0.03141231834888458\n",
      "Step = 138, train_loss: 0.029543818905949593, val_loss: 0.031055796891450882\n",
      "Step = 139, train_loss: 0.032734740525484085, val_loss: 0.031020257622003555\n",
      "Step = 140, train_loss: 0.030439920723438263, val_loss: 0.030870039016008377\n",
      "Step = 141, train_loss: 0.03373245522379875, val_loss: 0.030673250555992126\n",
      "Step = 142, train_loss: 0.03216934576630592, val_loss: 0.03064054623246193\n",
      "Step = 143, train_loss: 0.02784101851284504, val_loss: 0.03048996813595295\n",
      "Step = 144, train_loss: 0.02570423111319542, val_loss: 0.030299006029963493\n",
      "Step = 145, train_loss: 0.02587928995490074, val_loss: 0.030280781909823418\n",
      "Step = 146, train_loss: 0.02948632650077343, val_loss: 0.030040491372346878\n",
      "Step = 147, train_loss: 0.024086782708764076, val_loss: 0.03005075454711914\n",
      "Step = 148, train_loss: 0.029979944229125977, val_loss: 0.030302803963422775\n",
      "Step = 149, train_loss: 0.033583562821149826, val_loss: 0.030377443879842758\n",
      "Step = 150, train_loss: 0.03302176296710968, val_loss: 0.030390027910470963\n",
      "Step = 151, train_loss: 0.029261182993650436, val_loss: 0.030302712693810463\n",
      "Step = 152, train_loss: 0.028429878875613213, val_loss: 0.030314529314637184\n",
      "Step = 153, train_loss: 0.029664577916264534, val_loss: 0.03045765683054924\n",
      "Step = 154, train_loss: 0.02736086957156658, val_loss: 0.030650537461042404\n",
      "Step = 155, train_loss: 0.0316283255815506, val_loss: 0.03041251376271248\n",
      "Step = 156, train_loss: 0.027800273150205612, val_loss: 0.030222974717617035\n",
      "Step = 157, train_loss: 0.03416995704174042, val_loss: 0.03021285869181156\n",
      "Step = 158, train_loss: 0.02757071517407894, val_loss: 0.030065784230828285\n",
      "Step = 159, train_loss: 0.02998490445315838, val_loss: 0.030323676764965057\n",
      "Step = 160, train_loss: 0.027754098176956177, val_loss: 0.0306342002004385\n",
      "Step = 161, train_loss: 0.028832167387008667, val_loss: 0.030912846326828003\n",
      "Step = 162, train_loss: 0.027584876865148544, val_loss: 0.030918529257178307\n",
      "Step = 163, train_loss: 0.02573239989578724, val_loss: 0.03092995285987854\n",
      "Step = 164, train_loss: 0.03214970976114273, val_loss: 0.03081541508436203\n",
      "Step = 165, train_loss: 0.026936452835798264, val_loss: 0.030878016725182533\n",
      "Step = 166, train_loss: 0.030467577278614044, val_loss: 0.030925873667001724\n",
      "Step = 167, train_loss: 0.031112369149923325, val_loss: 0.030991196632385254\n",
      "Step = 168, train_loss: 0.02878638356924057, val_loss: 0.030995581299066544\n",
      "Step = 169, train_loss: 0.029200216755270958, val_loss: 0.030888469889760017\n",
      "Step = 170, train_loss: 0.03433189168572426, val_loss: 0.030779920518398285\n",
      "Step = 171, train_loss: 0.025685055181384087, val_loss: 0.030621476471424103\n",
      "Step = 172, train_loss: 0.02397562749683857, val_loss: 0.03042309172451496\n",
      "Step = 173, train_loss: 0.03087913617491722, val_loss: 0.030407030135393143\n",
      "Step = 174, train_loss: 0.02863876335322857, val_loss: 0.030296942219138145\n",
      "Step = 175, train_loss: 0.030721250921487808, val_loss: 0.030029302462935448\n",
      "Step = 176, train_loss: 0.028864046558737755, val_loss: 0.0299488864839077\n",
      "Step = 177, train_loss: 0.02666298858821392, val_loss: 0.029806319624185562\n",
      "Step = 178, train_loss: 0.032776590436697006, val_loss: 0.02961556799709797\n",
      "Step = 179, train_loss: 0.0354265496134758, val_loss: 0.029572725296020508\n",
      "Step = 180, train_loss: 0.033491816371679306, val_loss: 0.029317190870642662\n",
      "Step = 181, train_loss: 0.02980942651629448, val_loss: 0.029425978660583496\n",
      "Step = 182, train_loss: 0.030821897089481354, val_loss: 0.029338562861084938\n",
      "Step = 183, train_loss: 0.025135599076747894, val_loss: 0.029316186904907227\n",
      "Step = 184, train_loss: 0.03089028224349022, val_loss: 0.029356470331549644\n",
      "Step = 185, train_loss: 0.029584448784589767, val_loss: 0.029198266565799713\n",
      "Step = 186, train_loss: 0.034593261778354645, val_loss: 0.029381699860095978\n",
      "Step = 187, train_loss: 0.029793355613946915, val_loss: 0.029364539310336113\n",
      "Step = 188, train_loss: 0.027211274951696396, val_loss: 0.029399795457720757\n",
      "Step = 189, train_loss: 0.026537351310253143, val_loss: 0.02933681383728981\n",
      "Step = 190, train_loss: 0.026470951735973358, val_loss: 0.02938835509121418\n",
      "Step = 191, train_loss: 0.026013117283582687, val_loss: 0.029365431517362595\n",
      "Step = 192, train_loss: 0.027274956926703453, val_loss: 0.02909688465297222\n",
      "Step = 193, train_loss: 0.028833486139774323, val_loss: 0.028949545696377754\n",
      "Step = 194, train_loss: 0.03178481385111809, val_loss: 0.02881382405757904\n",
      "Step = 195, train_loss: 0.029834289103746414, val_loss: 0.02869139052927494\n",
      "Step = 196, train_loss: 0.030682172626256943, val_loss: 0.028648702427744865\n",
      "Step = 197, train_loss: 0.030359594151377678, val_loss: 0.028510166332125664\n",
      "Step = 198, train_loss: 0.03694229573011398, val_loss: 0.028466811403632164\n",
      "Step = 199, train_loss: 0.02866971120238304, val_loss: 0.02855178713798523\n",
      "Step = 200, train_loss: 0.0317537784576416, val_loss: 0.0286458358168602\n",
      "Step = 201, train_loss: 0.031338274478912354, val_loss: 0.028770655393600464\n",
      "Step = 202, train_loss: 0.032026518136262894, val_loss: 0.028860220685601234\n",
      "Step = 203, train_loss: 0.02373591810464859, val_loss: 0.02884681709110737\n",
      "Step = 204, train_loss: 0.030126452445983887, val_loss: 0.028756752610206604\n",
      "Step = 205, train_loss: 0.029056796804070473, val_loss: 0.028777463361620903\n",
      "Step = 206, train_loss: 0.027037033811211586, val_loss: 0.028941191732883453\n",
      "Step = 207, train_loss: 0.029441090300679207, val_loss: 0.02888077311217785\n",
      "Step = 208, train_loss: 0.028085904195904732, val_loss: 0.02866457775235176\n",
      "Step = 209, train_loss: 0.028078611940145493, val_loss: 0.028440769761800766\n",
      "Step = 210, train_loss: 0.03104851394891739, val_loss: 0.028470827266573906\n",
      "Step = 211, train_loss: 0.03157319128513336, val_loss: 0.028487879782915115\n",
      "Step = 212, train_loss: 0.031062543392181396, val_loss: 0.028198055922985077\n",
      "Step = 213, train_loss: 0.030261052772402763, val_loss: 0.02823098935186863\n",
      "Step = 214, train_loss: 0.03319714963436127, val_loss: 0.028442734852433205\n",
      "Step = 215, train_loss: 0.0319245383143425, val_loss: 0.03129667043685913\n",
      "Step = 216, train_loss: 0.03696243092417717, val_loss: 0.03320610150694847\n",
      "Step = 217, train_loss: 0.03309829533100128, val_loss: 0.034176409244537354\n",
      "Step = 218, train_loss: 0.033042073249816895, val_loss: 0.03436421602964401\n",
      "Step = 219, train_loss: 0.029194194823503494, val_loss: 0.034295082092285156\n",
      "Step = 220, train_loss: 0.029575960710644722, val_loss: 0.03431132808327675\n",
      "Step = 221, train_loss: 0.028657184913754463, val_loss: 0.03426552191376686\n",
      "Step = 222, train_loss: 0.03542942926287651, val_loss: 0.03422708436846733\n",
      "Step = 223, train_loss: 0.030713535845279694, val_loss: 0.03426242992281914\n",
      "Step = 224, train_loss: 0.028013277798891068, val_loss: 0.034099772572517395\n",
      "Step = 225, train_loss: 0.02901296503841877, val_loss: 0.033783331513404846\n",
      "Step = 226, train_loss: 0.03245370090007782, val_loss: 0.033720336854457855\n",
      "Step = 227, train_loss: 0.028600087389349937, val_loss: 0.03340460732579231\n",
      "Step = 228, train_loss: 0.023168455809354782, val_loss: 0.03313343971967697\n",
      "Step = 229, train_loss: 0.026250727474689484, val_loss: 0.0329192690551281\n",
      "Step = 230, train_loss: 0.028967108577489853, val_loss: 0.03273304924368858\n",
      "Step = 231, train_loss: 0.03253602981567383, val_loss: 0.03263511136174202\n",
      "Step = 232, train_loss: 0.029360856860876083, val_loss: 0.03245607018470764\n",
      "Step = 233, train_loss: 0.0343928299844265, val_loss: 0.03216660022735596\n",
      "Step = 234, train_loss: 0.028604885563254356, val_loss: 0.03184710070490837\n",
      "Step = 235, train_loss: 0.028697337955236435, val_loss: 0.031698886305093765\n",
      "Step = 236, train_loss: 0.02615078166127205, val_loss: 0.03132547810673714\n",
      "Step = 237, train_loss: 0.03313785418868065, val_loss: 0.031142661347985268\n",
      "Step = 238, train_loss: 0.03304535523056984, val_loss: 0.03100746124982834\n",
      "Step = 239, train_loss: 0.02992519550025463, val_loss: 0.030812039971351624\n",
      "Step = 240, train_loss: 0.03084392473101616, val_loss: 0.03056594729423523\n",
      "Step = 241, train_loss: 0.03147603198885918, val_loss: 0.030367720872163773\n",
      "Step = 242, train_loss: 0.02807527966797352, val_loss: 0.030294764786958694\n",
      "Step = 243, train_loss: 0.030004611238837242, val_loss: 0.03014538437128067\n",
      "Step = 244, train_loss: 0.03223379701375961, val_loss: 0.030383752658963203\n",
      "Step = 245, train_loss: 0.025603037327528, val_loss: 0.030567843466997147\n",
      "Step = 246, train_loss: 0.02898736484348774, val_loss: 0.030845215544104576\n",
      "Step = 247, train_loss: 0.03048490174114704, val_loss: 0.03076728619635105\n",
      "Step = 248, train_loss: 0.030755748972296715, val_loss: 0.030807064846158028\n",
      "Step = 249, train_loss: 0.0314907468855381, val_loss: 0.030756883323192596\n",
      "Step = 250, train_loss: 0.027026792988181114, val_loss: 0.03090335614979267\n",
      "Step = 251, train_loss: 0.022110216319561005, val_loss: 0.030851194635033607\n",
      "Step = 252, train_loss: 0.028765829280018806, val_loss: 0.030887262895703316\n",
      "Step = 253, train_loss: 0.029818095266819, val_loss: 0.030777795240283012\n",
      "Step = 254, train_loss: 0.025725355371832848, val_loss: 0.03082534484565258\n",
      "Step = 255, train_loss: 0.029283933341503143, val_loss: 0.03069285862147808\n",
      "Step = 256, train_loss: 0.030479086562991142, val_loss: 0.03054933063685894\n",
      "Step = 257, train_loss: 0.029430149123072624, val_loss: 0.030520683154463768\n",
      "Step = 258, train_loss: 0.026874883100390434, val_loss: 0.030515562742948532\n",
      "Step = 259, train_loss: 0.030155343934893608, val_loss: 0.03033095970749855\n",
      "Step = 260, train_loss: 0.02516489289700985, val_loss: 0.03008292429149151\n",
      "Step = 261, train_loss: 0.03377272188663483, val_loss: 0.029971003532409668\n",
      "Step = 262, train_loss: 0.0314551405608654, val_loss: 0.029807589948177338\n",
      "Step = 263, train_loss: 0.029107529670000076, val_loss: 0.029435334727168083\n",
      "Step = 264, train_loss: 0.028998922556638718, val_loss: 0.029306359589099884\n",
      "Step = 265, train_loss: 0.028780238702893257, val_loss: 0.029010046273469925\n",
      "Step = 266, train_loss: 0.031259920448064804, val_loss: 0.028822006657719612\n",
      "Step = 267, train_loss: 0.027528513222932816, val_loss: 0.028779033571481705\n",
      "Step = 268, train_loss: 0.028032861649990082, val_loss: 0.028689544647932053\n",
      "Step = 269, train_loss: 0.027340980246663094, val_loss: 0.028429020196199417\n",
      "Step = 270, train_loss: 0.03300372511148453, val_loss: 0.028265127912163734\n",
      "Step = 271, train_loss: 0.031093696132302284, val_loss: 0.02831982634961605\n",
      "Step = 272, train_loss: 0.033384066075086594, val_loss: 0.028188176453113556\n",
      "Step = 273, train_loss: 0.0361197367310524, val_loss: 0.028201602399349213\n",
      "Step = 274, train_loss: 0.02894461527466774, val_loss: 0.028205011039972305\n",
      "Step = 275, train_loss: 0.028441794216632843, val_loss: 0.02814806066453457\n",
      "Step = 276, train_loss: 0.029462583363056183, val_loss: 0.028089631348848343\n",
      "Step = 277, train_loss: 0.03092472068965435, val_loss: 0.02783145196735859\n",
      "Step = 278, train_loss: 0.0326046496629715, val_loss: 0.027920665219426155\n",
      "Step = 279, train_loss: 0.02380763366818428, val_loss: 0.027874216437339783\n",
      "Step = 280, train_loss: 0.030871357768774033, val_loss: 0.027748355641961098\n",
      "Step = 281, train_loss: 0.03154456242918968, val_loss: 0.027480116114020348\n",
      "Step = 282, train_loss: 0.03045530803501606, val_loss: 0.02741185389459133\n",
      "Step = 283, train_loss: 0.031532254070043564, val_loss: 0.0273054838180542\n",
      "Step = 284, train_loss: 0.030711166560649872, val_loss: 0.027288705110549927\n",
      "Step = 285, train_loss: 0.02721075899899006, val_loss: 0.027044549584388733\n",
      "Step = 286, train_loss: 0.0234090406447649, val_loss: 0.02697618119418621\n",
      "Step = 287, train_loss: 0.03231902793049812, val_loss: 0.026976224035024643\n",
      "Step = 288, train_loss: 0.0329989418387413, val_loss: 0.027181506156921387\n",
      "Step = 289, train_loss: 0.032213471829891205, val_loss: 0.02731332927942276\n",
      "Step = 290, train_loss: 0.03543148562312126, val_loss: 0.027305681258440018\n",
      "Step = 291, train_loss: 0.033903416246175766, val_loss: 0.027331186458468437\n",
      "Step = 292, train_loss: 0.03284291923046112, val_loss: 0.027335230261087418\n",
      "Step = 293, train_loss: 0.028838736936450005, val_loss: 0.027538860216736794\n",
      "Step = 294, train_loss: 0.02862223982810974, val_loss: 0.02795560099184513\n",
      "Step = 295, train_loss: 0.031411219388246536, val_loss: 0.028751680627465248\n",
      "Step = 296, train_loss: 0.0294567309319973, val_loss: 0.02913491055369377\n",
      "Step = 297, train_loss: 0.031152669340372086, val_loss: 0.029195107519626617\n",
      "Step = 298, train_loss: 0.030517738312482834, val_loss: 0.028949379920959473\n",
      "Step = 299, train_loss: 0.024527814239263535, val_loss: 0.02875332161784172\n",
      "Step = 300, train_loss: 0.02916167862713337, val_loss: 0.028702937066555023\n",
      "Step = 301, train_loss: 0.03455876186490059, val_loss: 0.02877108007669449\n",
      "Step = 302, train_loss: 0.029037462547421455, val_loss: 0.028591301292181015\n",
      "Step = 303, train_loss: 0.03309467062354088, val_loss: 0.028500579297542572\n",
      "Step = 304, train_loss: 0.030553342774510384, val_loss: 0.028440438210964203\n",
      "Step = 305, train_loss: 0.031361259520053864, val_loss: 0.028534764423966408\n",
      "Step = 306, train_loss: 0.03271566331386566, val_loss: 0.028727583587169647\n",
      "Step = 307, train_loss: 0.029495270922780037, val_loss: 0.029482737183570862\n",
      "Step = 308, train_loss: 0.027585098519921303, val_loss: 0.02973754145205021\n",
      "Step = 309, train_loss: 0.031514544039964676, val_loss: 0.029199251905083656\n",
      "Step = 310, train_loss: 0.02711290866136551, val_loss: 0.028636083006858826\n",
      "Step = 311, train_loss: 0.02891988679766655, val_loss: 0.027970919385552406\n",
      "Step = 312, train_loss: 0.028895564377307892, val_loss: 0.02598496526479721\n",
      "Step = 313, train_loss: 0.0336124449968338, val_loss: 0.023813528940081596\n",
      "Step = 314, train_loss: 0.02856329455971718, val_loss: 0.02278698794543743\n",
      "Step = 315, train_loss: 0.027508990839123726, val_loss: 0.02480176091194153\n",
      "Step = 316, train_loss: 0.03198493272066116, val_loss: 0.028150709345936775\n",
      "Step = 317, train_loss: 0.026662223041057587, val_loss: 0.03159042075276375\n",
      "Step = 318, train_loss: 0.026928309351205826, val_loss: 0.03283311426639557\n",
      "Step = 319, train_loss: 0.02470414899289608, val_loss: 0.033453747630119324\n",
      "Step = 320, train_loss: 0.024915160611271858, val_loss: 0.031216545030474663\n",
      "Step = 321, train_loss: 0.03445306420326233, val_loss: 0.028618615120649338\n",
      "Step = 322, train_loss: 0.028128625825047493, val_loss: 0.027509817853569984\n",
      "Step = 323, train_loss: 0.03154747933149338, val_loss: 0.026224235072731972\n",
      "Step = 324, train_loss: 0.028426555916666985, val_loss: 0.025431275367736816\n",
      "Step = 325, train_loss: 0.030121980234980583, val_loss: 0.025004465132951736\n",
      "Step = 326, train_loss: 0.03077608160674572, val_loss: 0.02474948577582836\n",
      "Step = 327, train_loss: 0.03352689370512962, val_loss: 0.02513907290995121\n",
      "Step = 328, train_loss: 0.03357379883527756, val_loss: 0.02590973488986492\n",
      "Step = 329, train_loss: 0.02633357048034668, val_loss: 0.026997515931725502\n",
      "Step = 330, train_loss: 0.03404945507645607, val_loss: 0.028484102338552475\n",
      "Step = 331, train_loss: 0.03022628277540207, val_loss: 0.030211634933948517\n",
      "Step = 332, train_loss: 0.028604377061128616, val_loss: 0.03210850805044174\n",
      "Step = 333, train_loss: 0.028346803039312363, val_loss: 0.033854663372039795\n",
      "Step = 334, train_loss: 0.03215745463967323, val_loss: 0.03581540286540985\n",
      "Step = 335, train_loss: 0.03145898878574371, val_loss: 0.03796646371483803\n",
      "Step = 336, train_loss: 0.03139270097017288, val_loss: 0.039968498051166534\n",
      "Step = 337, train_loss: 0.027848269790410995, val_loss: 0.04198191687464714\n",
      "Step = 338, train_loss: 0.030651535838842392, val_loss: 0.04430583864450455\n",
      "Step = 339, train_loss: 0.03010566718876362, val_loss: 0.045759011059999466\n",
      "Step = 340, train_loss: 0.035130683332681656, val_loss: 0.046129725873470306\n",
      "Step = 341, train_loss: 0.0303034670650959, val_loss: 0.04579107463359833\n",
      "Step = 342, train_loss: 0.03410346806049347, val_loss: 0.045300841331481934\n",
      "Step = 343, train_loss: 0.02908087708055973, val_loss: 0.04392107203602791\n",
      "Step = 344, train_loss: 0.031296856701374054, val_loss: 0.042575716972351074\n",
      "Step = 345, train_loss: 0.03327278420329094, val_loss: 0.041759002953767776\n",
      "Step = 346, train_loss: 0.029494622722268105, val_loss: 0.04178164526820183\n",
      "Step = 347, train_loss: 0.033315062522888184, val_loss: 0.04176412150263786\n",
      "Step = 348, train_loss: 0.031152447685599327, val_loss: 0.04150640219449997\n",
      "Step = 349, train_loss: 0.027402885258197784, val_loss: 0.042223021388053894\n",
      "Step = 350, train_loss: 0.029613925144076347, val_loss: 0.043608132749795914\n",
      "Step = 351, train_loss: 0.03081997111439705, val_loss: 0.04506514593958855\n",
      "Step = 352, train_loss: 0.036059606820344925, val_loss: 0.04588191211223602\n",
      "Step = 353, train_loss: 0.034147683531045914, val_loss: 0.046436283737421036\n",
      "Step = 354, train_loss: 0.031769659370183945, val_loss: 0.04698283225297928\n",
      "Step = 355, train_loss: 0.0277731753885746, val_loss: 0.04803762212395668\n",
      "Step = 356, train_loss: 0.021799344569444656, val_loss: 0.048608385026454926\n",
      "Step = 357, train_loss: 0.026874523609876633, val_loss: 0.047558754682540894\n",
      "Step = 358, train_loss: 0.03202470391988754, val_loss: 0.044430118054151535\n",
      "Step = 359, train_loss: 0.028464745730161667, val_loss: 0.04070207476615906\n",
      "Step = 360, train_loss: 0.029715437442064285, val_loss: 0.03656892478466034\n",
      "Step = 361, train_loss: 0.028945203870534897, val_loss: 0.03334026038646698\n",
      "Step = 362, train_loss: 0.02609284035861492, val_loss: 0.029764698818325996\n",
      "Step = 363, train_loss: 0.02772882767021656, val_loss: 0.026184475049376488\n",
      "Step = 364, train_loss: 0.02187059074640274, val_loss: 0.02355506457388401\n",
      "Step = 365, train_loss: 0.0305926613509655, val_loss: 0.02342698723077774\n",
      "Step = 366, train_loss: 0.031414326280355453, val_loss: 0.022646136581897736\n",
      "Step = 367, train_loss: 0.028129689395427704, val_loss: 0.02385437674820423\n",
      "Step = 368, train_loss: 0.030117183923721313, val_loss: 0.025458594784140587\n",
      "Step = 369, train_loss: 0.030483098700642586, val_loss: 0.027696428820490837\n",
      "Step = 370, train_loss: 0.028540123254060745, val_loss: 0.029246730729937553\n",
      "Step = 371, train_loss: 0.034188177436590195, val_loss: 0.03224676102399826\n",
      "Step = 372, train_loss: 0.03148741275072098, val_loss: 0.03577960282564163\n",
      "Step = 373, train_loss: 0.03280462697148323, val_loss: 0.039801012724637985\n",
      "Step = 374, train_loss: 0.03584735840559006, val_loss: 0.04381539672613144\n",
      "Step = 375, train_loss: 0.02677491307258606, val_loss: 0.0464523546397686\n",
      "Step = 376, train_loss: 0.03438996523618698, val_loss: 0.04617723822593689\n",
      "Step = 377, train_loss: 0.0323176346719265, val_loss: 0.04536576196551323\n",
      "Step = 378, train_loss: 0.029412558302283287, val_loss: 0.044851258397102356\n",
      "Step = 379, train_loss: 0.02945009060204029, val_loss: 0.04570060968399048\n",
      "Step = 380, train_loss: 0.033650755882263184, val_loss: 0.04714451730251312\n",
      "Step = 381, train_loss: 0.03183684125542641, val_loss: 0.04818342253565788\n",
      "Step = 382, train_loss: 0.0313752144575119, val_loss: 0.048716068267822266\n",
      "Step = 383, train_loss: 0.03040640614926815, val_loss: 0.04900491610169411\n",
      "Step = 384, train_loss: 0.03151445463299751, val_loss: 0.04885326325893402\n",
      "Step = 385, train_loss: 0.03410959988832474, val_loss: 0.04770725592970848\n",
      "Step = 386, train_loss: 0.036391738802194595, val_loss: 0.044971562922000885\n",
      "Step = 387, train_loss: 0.028591997921466827, val_loss: 0.041610538959503174\n",
      "Step = 388, train_loss: 0.02954936772584915, val_loss: 0.036101121455430984\n",
      "Step = 389, train_loss: 0.031152507290244102, val_loss: 0.027229780331254005\n",
      "Step = 390, train_loss: 0.025257088243961334, val_loss: 0.022039178758859634\n",
      "Step = 391, train_loss: 0.027666883543133736, val_loss: 0.01989518105983734\n",
      "Step = 392, train_loss: 0.029864294454455376, val_loss: 0.019071025773882866\n",
      "Step = 393, train_loss: 0.027883747592568398, val_loss: 0.019136838614940643\n",
      "Step = 394, train_loss: 0.029803358018398285, val_loss: 0.01915132626891136\n",
      "Step = 395, train_loss: 0.025965791195631027, val_loss: 0.019202858209609985\n",
      "Step = 396, train_loss: 0.028557468205690384, val_loss: 0.019115025177598\n",
      "Step = 397, train_loss: 0.024489082396030426, val_loss: 0.019818229600787163\n",
      "Step = 398, train_loss: 0.028453446924686432, val_loss: 0.023553159087896347\n",
      "Step = 399, train_loss: 0.02634436823427677, val_loss: 0.03282390162348747\n",
      "Step = 400, train_loss: 0.023917363956570625, val_loss: 0.03907686471939087\n",
      "Step = 401, train_loss: 0.03038198873400688, val_loss: 0.041725125163793564\n",
      "Step = 402, train_loss: 0.026471367105841637, val_loss: 0.04079237952828407\n",
      "Step = 403, train_loss: 0.023896118625998497, val_loss: 0.039242275059223175\n",
      "Step = 404, train_loss: 0.03213872015476227, val_loss: 0.03674957528710365\n",
      "Step = 405, train_loss: 0.03003801219165325, val_loss: 0.03346190229058266\n",
      "Step = 406, train_loss: 0.029053259640932083, val_loss: 0.030626023188233376\n",
      "Step = 407, train_loss: 0.03164159879088402, val_loss: 0.029216332361102104\n",
      "Step = 408, train_loss: 0.02854679338634014, val_loss: 0.025852441787719727\n",
      "Step = 409, train_loss: 0.026372086256742477, val_loss: 0.020432084798812866\n",
      "Step = 410, train_loss: 0.03363269940018654, val_loss: 0.01855263113975525\n",
      "Step = 411, train_loss: 0.027462078258395195, val_loss: 0.01804015226662159\n",
      "Step = 412, train_loss: 0.029577741399407387, val_loss: 0.020258795469999313\n",
      "Step = 413, train_loss: 0.036596499383449554, val_loss: 0.022570712491869926\n",
      "Step = 414, train_loss: 0.031764741986989975, val_loss: 0.026905810460448265\n",
      "Step = 415, train_loss: 0.030198734253644943, val_loss: 0.03350204601883888\n",
      "Step = 416, train_loss: 0.02765943855047226, val_loss: 0.03804885596036911\n",
      "Step = 417, train_loss: 0.027679510414600372, val_loss: 0.04162723198533058\n",
      "Step = 418, train_loss: 0.026748357340693474, val_loss: 0.0427263006567955\n",
      "Step = 419, train_loss: 0.03285311907529831, val_loss: 0.044115614145994186\n",
      "Step = 420, train_loss: 0.030899295583367348, val_loss: 0.04539567604660988\n",
      "Step = 421, train_loss: 0.030616510659456253, val_loss: 0.04605761915445328\n",
      "Step = 422, train_loss: 0.0303022600710392, val_loss: 0.04522532969713211\n",
      "Step = 423, train_loss: 0.025706706568598747, val_loss: 0.04350072517991066\n",
      "Step = 424, train_loss: 0.03246261924505234, val_loss: 0.04235908016562462\n",
      "Step = 425, train_loss: 0.027871381491422653, val_loss: 0.041183482855558395\n",
      "Step = 426, train_loss: 0.025825083255767822, val_loss: 0.039464280009269714\n",
      "Step = 427, train_loss: 0.030536238104104996, val_loss: 0.03775515779852867\n",
      "Step = 428, train_loss: 0.029871422797441483, val_loss: 0.03576364368200302\n",
      "Step = 429, train_loss: 0.024742087349295616, val_loss: 0.033873897045850754\n",
      "Step = 430, train_loss: 0.027329249307513237, val_loss: 0.030730625614523888\n",
      "Step = 431, train_loss: 0.02638264186680317, val_loss: 0.030443817377090454\n",
      "Step = 432, train_loss: 0.03023586794734001, val_loss: 0.027440065518021584\n",
      "Step = 433, train_loss: 0.028942150995135307, val_loss: 0.02362385392189026\n",
      "Step = 434, train_loss: 0.026546789333224297, val_loss: 0.019946414977312088\n",
      "Step = 435, train_loss: 0.03107886202633381, val_loss: 0.020097477361559868\n",
      "Step = 436, train_loss: 0.029052169993519783, val_loss: 0.024719679728150368\n",
      "Step = 437, train_loss: 0.026162343099713326, val_loss: 0.030254125595092773\n",
      "Step = 438, train_loss: 0.02966236136853695, val_loss: 0.03413821756839752\n",
      "Step = 439, train_loss: 0.0279732383787632, val_loss: 0.034347180277109146\n",
      "Step = 440, train_loss: 0.027808528393507004, val_loss: 0.031482405960559845\n",
      "Step = 441, train_loss: 0.030275337398052216, val_loss: 0.027103392407298088\n",
      "Step = 442, train_loss: 0.02769116684794426, val_loss: 0.02097318507730961\n",
      "Step = 443, train_loss: 0.03129070624709129, val_loss: 0.019053936004638672\n",
      "Step = 444, train_loss: 0.025439010933041573, val_loss: 0.019977813586592674\n",
      "Step = 445, train_loss: 0.03386109322309494, val_loss: 0.021365193650126457\n",
      "Step = 446, train_loss: 0.029666511341929436, val_loss: 0.02747287042438984\n",
      "Step = 447, train_loss: 0.02982160449028015, val_loss: 0.032690536230802536\n",
      "Step = 448, train_loss: 0.027519844472408295, val_loss: 0.03790169209241867\n",
      "Step = 449, train_loss: 0.0290155541151762, val_loss: 0.04130485653877258\n",
      "Step = 450, train_loss: 0.024880370125174522, val_loss: 0.04100760072469711\n",
      "Step = 451, train_loss: 0.028605904430150986, val_loss: 0.04051600769162178\n",
      "Step = 452, train_loss: 0.025458842515945435, val_loss: 0.0401977077126503\n",
      "Step = 453, train_loss: 0.02891090326011181, val_loss: 0.04018954932689667\n",
      "Step = 454, train_loss: 0.03000026009976864, val_loss: 0.03862191364169121\n",
      "Step = 455, train_loss: 0.024610204622149467, val_loss: 0.039192426949739456\n",
      "Step = 456, train_loss: 0.02837713435292244, val_loss: 0.03834303840994835\n",
      "Step = 457, train_loss: 0.03317100927233696, val_loss: 0.04073924571275711\n",
      "Step = 458, train_loss: 0.02725888416171074, val_loss: 0.04213947430253029\n",
      "Step = 459, train_loss: 0.033802200108766556, val_loss: 0.04341421648859978\n",
      "Step = 460, train_loss: 0.03169279173016548, val_loss: 0.04363306984305382\n",
      "Step = 461, train_loss: 0.030023498460650444, val_loss: 0.04224025830626488\n",
      "Step = 462, train_loss: 0.025677736848592758, val_loss: 0.040431566536426544\n",
      "Step = 463, train_loss: 0.02701474539935589, val_loss: 0.03828170523047447\n",
      "Step = 464, train_loss: 0.030053172260522842, val_loss: 0.0356198325753212\n",
      "Step = 465, train_loss: 0.029673408716917038, val_loss: 0.032760582864284515\n",
      "Step = 466, train_loss: 0.023584842681884766, val_loss: 0.03034975193440914\n",
      "Step = 467, train_loss: 0.02387232705950737, val_loss: 0.030407121405005455\n",
      "Step = 468, train_loss: 0.027728937566280365, val_loss: 0.030497409403324127\n",
      "Step = 469, train_loss: 0.025031525641679764, val_loss: 0.029398426413536072\n",
      "Step = 470, train_loss: 0.03115684725344181, val_loss: 0.025180978700518608\n",
      "Step = 471, train_loss: 0.02440999262034893, val_loss: 0.027250174432992935\n",
      "Step = 472, train_loss: 0.028579112142324448, val_loss: 0.03011130355298519\n",
      "Step = 473, train_loss: 0.030772242695093155, val_loss: 0.03267206624150276\n",
      "Step = 474, train_loss: 0.023998767137527466, val_loss: 0.03919174149632454\n",
      "Step = 475, train_loss: 0.025167804211378098, val_loss: 0.041977398097515106\n",
      "Step = 476, train_loss: 0.026942988857626915, val_loss: 0.043639227747917175\n",
      "Step = 477, train_loss: 0.026373840868473053, val_loss: 0.043641649186611176\n",
      "Step = 478, train_loss: 0.0246470607817173, val_loss: 0.04299907013773918\n",
      "Step = 479, train_loss: 0.033468976616859436, val_loss: 0.042605578899383545\n",
      "Step = 480, train_loss: 0.03324483335018158, val_loss: 0.0421895757317543\n",
      "Step = 481, train_loss: 0.024349229410290718, val_loss: 0.0408707894384861\n",
      "Step = 482, train_loss: 0.029181551188230515, val_loss: 0.04001713916659355\n",
      "Step = 483, train_loss: 0.024154404178261757, val_loss: 0.03922804817557335\n",
      "Step = 484, train_loss: 0.028421254828572273, val_loss: 0.037285227328538895\n",
      "Step = 485, train_loss: 0.027348892763257027, val_loss: 0.03415640816092491\n",
      "Step = 486, train_loss: 0.024729382246732712, val_loss: 0.029321499168872833\n",
      "Step = 487, train_loss: 0.02980814501643181, val_loss: 0.028266645967960358\n",
      "Step = 488, train_loss: 0.028634438291192055, val_loss: 0.025846922770142555\n",
      "Step = 489, train_loss: 0.03181912750005722, val_loss: 0.02346106246113777\n",
      "Step = 490, train_loss: 0.02715217135846615, val_loss: 0.019931994378566742\n",
      "Step = 491, train_loss: 0.030059777200222015, val_loss: 0.01710795983672142\n",
      "Step = 492, train_loss: 0.027966391295194626, val_loss: 0.02163567580282688\n",
      "Step = 493, train_loss: 0.027399621903896332, val_loss: 0.03399587422609329\n",
      "Step = 494, train_loss: 0.028443297371268272, val_loss: 0.04040730744600296\n",
      "Step = 495, train_loss: 0.028843529522418976, val_loss: 0.04150284081697464\n",
      "Step = 496, train_loss: 0.026623914018273354, val_loss: 0.04029424488544464\n",
      "Step = 497, train_loss: 0.02637271024286747, val_loss: 0.03821227326989174\n",
      "Step = 498, train_loss: 0.031047403812408447, val_loss: 0.034688834100961685\n",
      "Step = 499, train_loss: 0.029851552098989487, val_loss: 0.030139368027448654\n",
      "Step = 500, train_loss: 0.024550804868340492, val_loss: 0.024114416912198067\n",
      "Step = 501, train_loss: 0.031161172315478325, val_loss: 0.018329473212361336\n",
      "Step = 502, train_loss: 0.028651641681790352, val_loss: 0.016262443736195564\n",
      "Step = 503, train_loss: 0.029460448771715164, val_loss: 0.016389068216085434\n",
      "Step = 504, train_loss: 0.02309131994843483, val_loss: 0.018131470307707787\n",
      "Step = 505, train_loss: 0.026313375681638718, val_loss: 0.022432029247283936\n",
      "Step = 506, train_loss: 0.024037688970565796, val_loss: 0.026498645544052124\n",
      "Step = 507, train_loss: 0.0277514960616827, val_loss: 0.03146490454673767\n",
      "Step = 508, train_loss: 0.026676734909415245, val_loss: 0.03670654073357582\n",
      "Step = 509, train_loss: 0.027242232114076614, val_loss: 0.04173443466424942\n",
      "Step = 510, train_loss: 0.02361404523253441, val_loss: 0.043481044471263885\n",
      "Step = 511, train_loss: 0.03192976862192154, val_loss: 0.043325427919626236\n",
      "Step = 512, train_loss: 0.026251541450619698, val_loss: 0.04213783144950867\n",
      "Step = 513, train_loss: 0.01927391067147255, val_loss: 0.04005898907780647\n",
      "Step = 514, train_loss: 0.025649040937423706, val_loss: 0.03716185316443443\n",
      "Step = 515, train_loss: 0.02287439815700054, val_loss: 0.032123863697052\n",
      "Step = 516, train_loss: 0.028558731079101562, val_loss: 0.02778041735291481\n",
      "Step = 517, train_loss: 0.02176557295024395, val_loss: 0.02216515876352787\n",
      "Step = 518, train_loss: 0.02515614777803421, val_loss: 0.018694216385483742\n",
      "Step = 519, train_loss: 0.026884911581873894, val_loss: 0.019911475479602814\n",
      "Step = 520, train_loss: 0.0247567780315876, val_loss: 0.022687306627631187\n",
      "Step = 521, train_loss: 0.022027762606739998, val_loss: 0.02673245035111904\n",
      "Step = 522, train_loss: 0.024739278480410576, val_loss: 0.031203296035528183\n",
      "Step = 523, train_loss: 0.025327865034341812, val_loss: 0.03469521179795265\n",
      "Step = 524, train_loss: 0.026246262714266777, val_loss: 0.03687995672225952\n",
      "Step = 525, train_loss: 0.030645955353975296, val_loss: 0.04158110171556473\n",
      "Step = 526, train_loss: 0.03215790539979935, val_loss: 0.04767508804798126\n",
      "Step = 527, train_loss: 0.026559224352240562, val_loss: 0.051385462284088135\n",
      "Step = 528, train_loss: 0.028836466372013092, val_loss: 0.052244339138269424\n",
      "Step = 529, train_loss: 0.025994282215833664, val_loss: 0.05142156779766083\n",
      "Step = 530, train_loss: 0.027426371350884438, val_loss: 0.0483301505446434\n",
      "Step = 531, train_loss: 0.02657897211611271, val_loss: 0.04268082231283188\n",
      "Step = 532, train_loss: 0.026642870157957077, val_loss: 0.03829697147011757\n",
      "Step = 533, train_loss: 0.02834801748394966, val_loss: 0.03782421723008156\n",
      "Step = 534, train_loss: 0.022220192477107048, val_loss: 0.037550199776887894\n",
      "Step = 535, train_loss: 0.02773597463965416, val_loss: 0.04022093489766121\n",
      "Step = 536, train_loss: 0.03177175298333168, val_loss: 0.04307648167014122\n",
      "Step = 537, train_loss: 0.0319572277367115, val_loss: 0.04212518036365509\n",
      "Step = 538, train_loss: 0.02161707729101181, val_loss: 0.041681546717882156\n",
      "Step = 539, train_loss: 0.025101197883486748, val_loss: 0.03983350843191147\n",
      "Step = 540, train_loss: 0.025137152522802353, val_loss: 0.039537131786346436\n",
      "Step = 541, train_loss: 0.02517784759402275, val_loss: 0.039712946861982346\n",
      "Step = 542, train_loss: 0.031097985804080963, val_loss: 0.04171779751777649\n",
      "Step = 543, train_loss: 0.031851813197135925, val_loss: 0.04390592873096466\n",
      "Step = 544, train_loss: 0.02512199431657791, val_loss: 0.04425749555230141\n",
      "Step = 545, train_loss: 0.029614459723234177, val_loss: 0.04368521273136139\n",
      "Step = 546, train_loss: 0.029215801507234573, val_loss: 0.04204309731721878\n",
      "Step = 547, train_loss: 0.03170129284262657, val_loss: 0.040120113641023636\n",
      "Step = 548, train_loss: 0.03212582319974899, val_loss: 0.038767941296100616\n",
      "Step = 549, train_loss: 0.026783596724271774, val_loss: 0.03658631443977356\n",
      "Step = 550, train_loss: 0.02706782892346382, val_loss: 0.035320915281772614\n",
      "Step = 551, train_loss: 0.023361442610621452, val_loss: 0.03351344168186188\n",
      "Step = 552, train_loss: 0.02805336005985737, val_loss: 0.032908640801906586\n",
      "Step = 553, train_loss: 0.026134364306926727, val_loss: 0.032128140330314636\n",
      "Step = 554, train_loss: 0.024835677817463875, val_loss: 0.033372700214385986\n",
      "Step = 555, train_loss: 0.023485396057367325, val_loss: 0.03569236770272255\n",
      "Step = 556, train_loss: 0.027751358225941658, val_loss: 0.036804068833589554\n",
      "Step = 557, train_loss: 0.03007563203573227, val_loss: 0.03587239235639572\n",
      "Step = 558, train_loss: 0.024067269638180733, val_loss: 0.03223178908228874\n",
      "Step = 559, train_loss: 0.028748340904712677, val_loss: 0.029570750892162323\n",
      "Step = 560, train_loss: 0.02340746857225895, val_loss: 0.025352997705340385\n",
      "Step = 561, train_loss: 0.03310352936387062, val_loss: 0.022370129823684692\n",
      "Step = 562, train_loss: 0.026753131300210953, val_loss: 0.023941004648804665\n",
      "Step = 563, train_loss: 0.02572246640920639, val_loss: 0.028971858322620392\n",
      "Step = 564, train_loss: 0.028379814699292183, val_loss: 0.035827260464429855\n",
      "Step = 565, train_loss: 0.027651365846395493, val_loss: 0.039829399436712265\n",
      "Step = 566, train_loss: 0.02605140209197998, val_loss: 0.041467636823654175\n",
      "Step = 567, train_loss: 0.02497856877744198, val_loss: 0.04204155132174492\n",
      "Step = 568, train_loss: 0.024231495335698128, val_loss: 0.041748542338609695\n",
      "Step = 569, train_loss: 0.027903228998184204, val_loss: 0.0409279391169548\n",
      "Step = 570, train_loss: 0.0268726609647274, val_loss: 0.038531944155693054\n",
      "Step = 571, train_loss: 0.02514943853020668, val_loss: 0.03619001433253288\n",
      "Step = 572, train_loss: 0.025405574589967728, val_loss: 0.03388993442058563\n",
      "Step = 573, train_loss: 0.023525578901171684, val_loss: 0.03238576278090477\n",
      "Step = 574, train_loss: 0.03051973134279251, val_loss: 0.030805524438619614\n",
      "Step = 575, train_loss: 0.02813124656677246, val_loss: 0.029939448460936546\n",
      "Step = 576, train_loss: 0.027901258319616318, val_loss: 0.03189700469374657\n",
      "Step = 577, train_loss: 0.026826243847608566, val_loss: 0.03510323911905289\n",
      "Step = 578, train_loss: 0.021985424682497978, val_loss: 0.03933478891849518\n",
      "Step = 579, train_loss: 0.028328787535429, val_loss: 0.042323045432567596\n",
      "Step = 580, train_loss: 0.02600877545773983, val_loss: 0.043914347887039185\n",
      "Step = 581, train_loss: 0.026921464130282402, val_loss: 0.04270036891102791\n",
      "Step = 582, train_loss: 0.02863449603319168, val_loss: 0.04237549751996994\n",
      "Step = 583, train_loss: 0.026909979060292244, val_loss: 0.040125709027051926\n",
      "Step = 584, train_loss: 0.0247555673122406, val_loss: 0.037411779165267944\n",
      "Step = 585, train_loss: 0.03028136119246483, val_loss: 0.03641388565301895\n",
      "Step = 586, train_loss: 0.02256515435874462, val_loss: 0.03448084741830826\n",
      "Step = 587, train_loss: 0.03148133307695389, val_loss: 0.03309875726699829\n",
      "Step = 588, train_loss: 0.02710581198334694, val_loss: 0.03288188576698303\n",
      "Step = 589, train_loss: 0.02547300234436989, val_loss: 0.03325273469090462\n",
      "Step = 590, train_loss: 0.026616651564836502, val_loss: 0.033360838890075684\n",
      "Step = 591, train_loss: 0.027005635201931, val_loss: 0.03313311189413071\n",
      "Step = 592, train_loss: 0.026088416576385498, val_loss: 0.03160511702299118\n",
      "Step = 593, train_loss: 0.029563354328274727, val_loss: 0.030248519033193588\n",
      "Step = 594, train_loss: 0.027536965906620026, val_loss: 0.029647869989275932\n",
      "Step = 595, train_loss: 0.027346305549144745, val_loss: 0.030920304358005524\n",
      "Step = 596, train_loss: 0.02243913896381855, val_loss: 0.03321388363838196\n",
      "Step = 597, train_loss: 0.02541344240307808, val_loss: 0.03580938279628754\n",
      "Step = 598, train_loss: 0.02204432152211666, val_loss: 0.03797521814703941\n",
      "Step = 599, train_loss: 0.02248305082321167, val_loss: 0.038419440388679504\n",
      "Best model at index: 502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_7148\\615185307.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('Best_target_model/net_parameters.pkl')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "# summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "# 每20次下降一半的函数\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.75 ** (epoch // 45))\n",
    "\n",
    "val_losses=[]\n",
    "\n",
    "for t in range(600):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step()\n",
    "\n",
    "    '''save all the model in each epoch'''    \n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, 'Target_model/net_parameters'+str(t)+'.pkl')\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print(f'Step = {t}, train_loss: {train_loss.item()}, val_loss: {val_loss.item()}')\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "   \n",
    "'''find the best model'''\n",
    "best_index = val_losses.index(np.min(val_losses))\n",
    "print(f'Best model at index: {best_index}')\n",
    "\n",
    "'''copy the best model from model file to the model_test file'''\n",
    "shutil.copyfile(f'Target_model/net_parameters{best_index}.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "'''Reload the best model''' \n",
    "model = Net()\n",
    "checkpoint = torch.load('Best_target_model/net_parameters.pkl')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.40475962\n",
      "mpe_val: 0.38676083\n",
      "mpe_a: 0.48212149867377163\n",
      "mpe_b: 0.1723339534455059\n",
      "rmse_train: 317.64136\n",
      "rmse_val: 284.8161\n",
      "rmse_a: 344.3292434676101\n",
      "rmse_b: 260.352933534462\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.30434650182724\n",
      "r2_val: 0.22069615125656128\n",
      "r2_a: 0.22310581756942527\n",
      "r2_b: 0.16613398905814303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
