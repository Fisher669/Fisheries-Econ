{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "import math\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self,in_channel,reduction=16,kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        #通道注意力机制\n",
    "        self.max_pool=nn.AdaptiveMaxPool2d(output_size=1)\n",
    "        self.avg_pool=nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.mlp=nn.Sequential(\n",
    "            nn.Linear(in_features=in_channel,out_features=in_channel//reduction,bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=in_channel//reduction,out_features=in_channel,bias=False)\n",
    "        )\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        #空间注意力机制\n",
    "        self.conv=nn.Conv2d(in_channels=2,out_channels=1,kernel_size=kernel_size ,stride=1,padding=kernel_size//2,bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        #通道注意力机制\n",
    "        maxout=self.max_pool(x)\n",
    "        maxout=self.mlp(maxout.view(maxout.size(0),-1))\n",
    "        avgout=self.avg_pool(x)\n",
    "        avgout=self.mlp(avgout.view(avgout.size(0),-1))\n",
    "        channel_out=self.sigmoid(maxout+avgout)\n",
    "        channel_out=channel_out.view(x.size(0),x.size(1),1,1)\n",
    "        channel_out=channel_out*x\n",
    "        #空间注意力机制\n",
    "        max_out,_=torch.max(channel_out,dim=1,keepdim=True)\n",
    "        mean_out=torch.mean(channel_out,dim=1,keepdim=True)\n",
    "        out=torch.cat((max_out,mean_out),dim=1)\n",
    "        out=self.sigmoid(self.conv(out))\n",
    "        out=out*channel_out\n",
    "        return out\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(8, 16, kernel_size=(2, 3)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Conv2d(16, 32, kernel_size=(2, 3)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.ebam = CBAM(32)\n",
    "        self.layer2 =nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(2, 2)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=(1, 2)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(128,256,kernel_size=(1,2)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=(1, 2)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            # nn.Conv2d(256, 512, kernel_size=(1, 2)),\n",
    "            # nn.BatchNorm2d(512),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "            # nn.Dropout(p=0.2),  \n",
    "            \n",
    "            # nn.Conv2d(512, 512, kernel_size=(1, 2)),\n",
    "            # nn.BatchNorm2d(512),\n",
    "            # nn.LeakyReLU(inplace=True),\n",
    "            # nn.Dropout(p=0.2),  \n",
    "        )\n",
    "        \n",
    "        # self.lstm = nn.LSTM(544, 544,num_layers=2, bidirectional=True, batch_first=True)\n",
    "        # self.fc1 = nn.Linear(1088, 32)  # 修正为与LSTM输出相匹配\n",
    "        self.lstm = nn.LSTM(256+32, 288,num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.fc1 = nn.Linear(576, 1)  # 修正为与LSTM输出相匹配\n",
    "        # self.fc2 = nn.Linear(32, 1)\n",
    "        self.dropout = nn.Dropout(p=0.2)  # 添加Dropout层\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            #让模型先经过layers1，在分开去做ebam和layer2\n",
    "            x = self.layers1(x)\n",
    "            ebam_output = self.ebam(x)\n",
    "            layer2_output = self.layer2(x)\n",
    "            cropped_ebam_output = ebam_output[:, :, :7, :15]\n",
    "            # print(ebam_output.shape)\n",
    "            # print(layer2_output.shape)\n",
    "            # print(cropped_layer2_output.shape)\n",
    "            x = torch.cat((layer2_output, cropped_ebam_output), dim=1)\n",
    "            \n",
    "            x = to_3d(x)  # 转换为3D\n",
    "            x, _ = self.lstm(x)  # LSTM处理\n",
    "            x = x[:, -1, :]  # 取最后一个时序输出作为特征\n",
    "            # x = self.eca1(x)\n",
    "            # x = x[:,-1,:]\n",
    "            # x = torch.relu(self.fc1(x))  # 压缩多余维度\n",
    "            # x = self.dropout(x)  # 使用定义的Dropout层\n",
    "            return torch.sigmoid(self.fc1(x))\n",
    "        except RuntimeError as e:\n",
    "            print(f\"运行时错误：{e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"发生意外错误：{e}\")\n",
    "            raise  # 重新抛出异常，以便进一步处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (12): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ebam): CBAM(\n",
      "    (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=2, bias=False)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=2, out_features=32, bias=False)\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "    (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Conv2d(256, 256, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (15): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(288, 288, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc1): Linear(in_features=576, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Step = 0 train_loss: 0.0652942 val_loss: 0.061388947\n",
      "Step = 1 train_loss: 0.06083361 val_loss: 0.060796864\n",
      "Step = 2 train_loss: 0.0558148 val_loss: 0.060128294\n",
      "Step = 3 train_loss: 0.052006356 val_loss: 0.059363306\n",
      "Step = 4 train_loss: 0.04818207 val_loss: 0.058505423\n",
      "Step = 5 train_loss: 0.044170924 val_loss: 0.057567317\n",
      "Step = 6 train_loss: 0.040475294 val_loss: 0.05653094\n",
      "Step = 7 train_loss: 0.03752878 val_loss: 0.05541602\n",
      "Step = 8 train_loss: 0.036622103 val_loss: 0.054208163\n",
      "Step = 9 train_loss: 0.030644875 val_loss: 0.052955896\n",
      "Step = 10 train_loss: 0.030233173 val_loss: 0.051679682\n",
      "Step = 11 train_loss: 0.029172473 val_loss: 0.05042268\n",
      "Step = 12 train_loss: 0.028942985 val_loss: 0.04912052\n",
      "Step = 13 train_loss: 0.0289802 val_loss: 0.04786546\n",
      "Step = 14 train_loss: 0.027332613 val_loss: 0.04672512\n",
      "Step = 15 train_loss: 0.029060232 val_loss: 0.045648888\n",
      "Step = 16 train_loss: 0.03008301 val_loss: 0.044727772\n",
      "Step = 17 train_loss: 0.031109782 val_loss: 0.043990433\n",
      "Step = 18 train_loss: 0.03090858 val_loss: 0.04346749\n",
      "Step = 19 train_loss: 0.02877229 val_loss: 0.04317879\n",
      "Step = 20 train_loss: 0.027817033 val_loss: 0.042907394\n",
      "Step = 21 train_loss: 0.030466314 val_loss: 0.04278108\n",
      "Step = 22 train_loss: 0.0280988 val_loss: 0.042681426\n",
      "Step = 23 train_loss: 0.030656965 val_loss: 0.04259942\n",
      "Step = 24 train_loss: 0.031036202 val_loss: 0.042587843\n",
      "Step = 25 train_loss: 0.028784601 val_loss: 0.042707633\n",
      "Step = 26 train_loss: 0.027601125 val_loss: 0.042868197\n",
      "Step = 27 train_loss: 0.02816723 val_loss: 0.04308033\n",
      "Step = 28 train_loss: 0.029248716 val_loss: 0.043302953\n",
      "Step = 29 train_loss: 0.026734184 val_loss: 0.04335147\n",
      "Step = 30 train_loss: 0.028626166 val_loss: 0.043195467\n",
      "Step = 31 train_loss: 0.028885288 val_loss: 0.04279357\n",
      "Step = 32 train_loss: 0.029915495 val_loss: 0.042350944\n",
      "Step = 33 train_loss: 0.029733883 val_loss: 0.041884545\n",
      "Step = 34 train_loss: 0.02956339 val_loss: 0.041236367\n",
      "Step = 35 train_loss: 0.025674734 val_loss: 0.040663842\n",
      "Step = 36 train_loss: 0.030140389 val_loss: 0.04014566\n",
      "Step = 37 train_loss: 0.032038417 val_loss: 0.039544903\n",
      "Step = 38 train_loss: 0.028569302 val_loss: 0.03867586\n",
      "Step = 39 train_loss: 0.028793238 val_loss: 0.03774881\n",
      "Step = 40 train_loss: 0.03038161 val_loss: 0.03703608\n",
      "Step = 41 train_loss: 0.028581988 val_loss: 0.03612572\n",
      "Step = 42 train_loss: 0.029939227 val_loss: 0.035289068\n",
      "Step = 43 train_loss: 0.030126939 val_loss: 0.035109445\n",
      "Step = 44 train_loss: 0.027789328 val_loss: 0.034278847\n",
      "Step = 45 train_loss: 0.027367728 val_loss: 0.035127804\n",
      "Step = 46 train_loss: 0.025679132 val_loss: 0.035455327\n",
      "Step = 47 train_loss: 0.03199999 val_loss: 0.035740662\n",
      "Step = 48 train_loss: 0.028726935 val_loss: 0.035742853\n",
      "Step = 49 train_loss: 0.027249752 val_loss: 0.035226397\n",
      "Step = 50 train_loss: 0.027719142 val_loss: 0.035368152\n",
      "Step = 51 train_loss: 0.029025812 val_loss: 0.035519984\n",
      "Step = 52 train_loss: 0.028907668 val_loss: 0.03586744\n",
      "Step = 53 train_loss: 0.028054567 val_loss: 0.03667607\n",
      "Step = 54 train_loss: 0.027455531 val_loss: 0.037253957\n",
      "Step = 55 train_loss: 0.028223617 val_loss: 0.037941903\n",
      "Step = 56 train_loss: 0.030090835 val_loss: 0.038228404\n",
      "Step = 57 train_loss: 0.029928885 val_loss: 0.03840935\n",
      "Step = 58 train_loss: 0.028767027 val_loss: 0.038526013\n",
      "Step = 59 train_loss: 0.02990669 val_loss: 0.038568456\n",
      "Step = 60 train_loss: 0.029018907 val_loss: 0.038621556\n",
      "Step = 61 train_loss: 0.030631192 val_loss: 0.038946647\n",
      "Step = 62 train_loss: 0.028392918 val_loss: 0.03934244\n",
      "Step = 63 train_loss: 0.025498416 val_loss: 0.03969639\n",
      "Step = 64 train_loss: 0.025849115 val_loss: 0.039940137\n",
      "Step = 65 train_loss: 0.029012373 val_loss: 0.040051136\n",
      "Step = 66 train_loss: 0.031445228 val_loss: 0.04007446\n",
      "Step = 67 train_loss: 0.028362839 val_loss: 0.040116046\n",
      "Step = 68 train_loss: 0.026656479 val_loss: 0.040163312\n",
      "Step = 69 train_loss: 0.027234921 val_loss: 0.04018243\n",
      "Step = 70 train_loss: 0.02812591 val_loss: 0.040197052\n",
      "Step = 71 train_loss: 0.03093772 val_loss: 0.040318552\n",
      "Step = 72 train_loss: 0.025932021 val_loss: 0.0404\n",
      "Step = 73 train_loss: 0.028195176 val_loss: 0.040412776\n",
      "Step = 74 train_loss: 0.027930375 val_loss: 0.04045387\n",
      "Step = 75 train_loss: 0.027211258 val_loss: 0.04043743\n",
      "Step = 76 train_loss: 0.027770182 val_loss: 0.040448483\n",
      "Step = 77 train_loss: 0.030946892 val_loss: 0.04048029\n",
      "Step = 78 train_loss: 0.028789295 val_loss: 0.04046435\n",
      "Step = 79 train_loss: 0.02971377 val_loss: 0.040463775\n",
      "Step = 80 train_loss: 0.029696127 val_loss: 0.0404312\n",
      "Step = 81 train_loss: 0.02786581 val_loss: 0.040409338\n",
      "Step = 82 train_loss: 0.02903926 val_loss: 0.04043242\n",
      "Step = 83 train_loss: 0.02881041 val_loss: 0.040471848\n",
      "Step = 84 train_loss: 0.02905019 val_loss: 0.0404645\n",
      "Step = 85 train_loss: 0.029343467 val_loss: 0.04046591\n",
      "Step = 86 train_loss: 0.028628044 val_loss: 0.040469237\n",
      "Step = 87 train_loss: 0.030464338 val_loss: 0.0404818\n",
      "Step = 88 train_loss: 0.031202372 val_loss: 0.04051265\n",
      "Step = 89 train_loss: 0.029456941 val_loss: 0.040556736\n",
      "Step = 90 train_loss: 0.029196244 val_loss: 0.040574558\n",
      "Step = 91 train_loss: 0.028755108 val_loss: 0.040596075\n",
      "Step = 92 train_loss: 0.027506594 val_loss: 0.040620897\n",
      "Step = 93 train_loss: 0.028897246 val_loss: 0.040584683\n",
      "Step = 94 train_loss: 0.029691832 val_loss: 0.040603776\n",
      "Step = 95 train_loss: 0.029695304 val_loss: 0.040638108\n",
      "Step = 96 train_loss: 0.028204964 val_loss: 0.040647715\n",
      "Step = 97 train_loss: 0.031462178 val_loss: 0.040662646\n",
      "Step = 98 train_loss: 0.02917531 val_loss: 0.040686443\n",
      "Step = 99 train_loss: 0.030562371 val_loss: 0.040662933\n",
      "Step = 100 train_loss: 0.029128982 val_loss: 0.0406618\n",
      "Step = 101 train_loss: 0.027225317 val_loss: 0.040652104\n",
      "Step = 102 train_loss: 0.029071866 val_loss: 0.04064661\n",
      "Step = 103 train_loss: 0.02462142 val_loss: 0.04065489\n",
      "Step = 104 train_loss: 0.029671926 val_loss: 0.040668003\n",
      "Step = 105 train_loss: 0.029713495 val_loss: 0.040641457\n",
      "Step = 106 train_loss: 0.027617492 val_loss: 0.040642187\n",
      "Step = 107 train_loss: 0.02581907 val_loss: 0.040638678\n",
      "Step = 108 train_loss: 0.027219687 val_loss: 0.04063738\n",
      "Step = 109 train_loss: 0.028445987 val_loss: 0.04066255\n",
      "Step = 110 train_loss: 0.02938334 val_loss: 0.040678665\n",
      "Step = 111 train_loss: 0.026496576 val_loss: 0.040665567\n",
      "Step = 112 train_loss: 0.02711811 val_loss: 0.04067215\n",
      "Step = 113 train_loss: 0.027789483 val_loss: 0.040689096\n",
      "Step = 114 train_loss: 0.027918829 val_loss: 0.040684603\n",
      "Step = 115 train_loss: 0.028042294 val_loss: 0.040694155\n",
      "Step = 116 train_loss: 0.028351953 val_loss: 0.04071172\n",
      "Step = 117 train_loss: 0.030024659 val_loss: 0.040714934\n",
      "Step = 118 train_loss: 0.027349468 val_loss: 0.040732745\n",
      "Step = 119 train_loss: 0.028623534 val_loss: 0.040747855\n",
      "Step = 120 train_loss: 0.02699055 val_loss: 0.040747963\n",
      "Step = 121 train_loss: 0.02858365 val_loss: 0.040762644\n",
      "Step = 122 train_loss: 0.027584562 val_loss: 0.040803857\n",
      "Step = 123 train_loss: 0.026241105 val_loss: 0.04081265\n",
      "Step = 124 train_loss: 0.027358754 val_loss: 0.0408647\n",
      "Step = 125 train_loss: 0.029202245 val_loss: 0.04094375\n",
      "Step = 126 train_loss: 0.028638016 val_loss: 0.04104451\n",
      "Step = 127 train_loss: 0.02878687 val_loss: 0.041251004\n",
      "Step = 128 train_loss: 0.029723754 val_loss: 0.041546807\n",
      "Step = 129 train_loss: 0.028729467 val_loss: 0.041949365\n",
      "Step = 130 train_loss: 0.027370967 val_loss: 0.042331334\n",
      "Step = 131 train_loss: 0.026774164 val_loss: 0.042600848\n",
      "Step = 132 train_loss: 0.030525412 val_loss: 0.043226816\n",
      "Step = 133 train_loss: 0.031213032 val_loss: 0.042899713\n",
      "Step = 134 train_loss: 0.028541464 val_loss: 0.04198785\n",
      "Step = 135 train_loss: 0.02859946 val_loss: 0.043050837\n",
      "Step = 136 train_loss: 0.029383816 val_loss: 0.042233877\n",
      "Step = 137 train_loss: 0.02800249 val_loss: 0.03986554\n",
      "Step = 138 train_loss: 0.02535792 val_loss: 0.03821604\n",
      "Step = 139 train_loss: 0.029522896 val_loss: 0.0359457\n",
      "Step = 140 train_loss: 0.028789576 val_loss: 0.029779604\n",
      "Step = 141 train_loss: 0.028409926 val_loss: 0.023986747\n",
      "Step = 142 train_loss: 0.02848414 val_loss: 0.02076558\n",
      "Step = 143 train_loss: 0.02572566 val_loss: 0.026370483\n",
      "Step = 144 train_loss: 0.025340429 val_loss: 0.02934232\n",
      "Step = 145 train_loss: 0.027767798 val_loss: 0.029089494\n",
      "Step = 146 train_loss: 0.027807716 val_loss: 0.026006715\n",
      "Step = 147 train_loss: 0.025254028 val_loss: 0.024320707\n",
      "Step = 148 train_loss: 0.024705362 val_loss: 0.029530656\n",
      "Step = 149 train_loss: 0.025301684 val_loss: 0.021793783\n",
      "Step = 150 train_loss: 0.028440805 val_loss: 0.02038841\n",
      "Step = 151 train_loss: 0.026716772 val_loss: 0.021431368\n",
      "Step = 152 train_loss: 0.026469473 val_loss: 0.022844167\n",
      "Step = 153 train_loss: 0.02594002 val_loss: 0.02381687\n",
      "Step = 154 train_loss: 0.027779859 val_loss: 0.024297724\n",
      "Step = 155 train_loss: 0.025418557 val_loss: 0.024187211\n",
      "Step = 156 train_loss: 0.02811562 val_loss: 0.022919253\n",
      "Step = 157 train_loss: 0.028287183 val_loss: 0.021437058\n",
      "Step = 158 train_loss: 0.028939197 val_loss: 0.022927046\n",
      "Step = 159 train_loss: 0.027623696 val_loss: 0.022881227\n",
      "Step = 160 train_loss: 0.026495712 val_loss: 0.0224656\n",
      "Step = 161 train_loss: 0.027419178 val_loss: 0.022348842\n",
      "Step = 162 train_loss: 0.02961516 val_loss: 0.023276573\n",
      "Step = 163 train_loss: 0.027017483 val_loss: 0.024744147\n",
      "Step = 164 train_loss: 0.027227748 val_loss: 0.025286777\n",
      "Step = 165 train_loss: 0.028306972 val_loss: 0.024939388\n",
      "Step = 166 train_loss: 0.02845918 val_loss: 0.024160761\n",
      "Step = 167 train_loss: 0.029485224 val_loss: 0.023167856\n",
      "Step = 168 train_loss: 0.029707815 val_loss: 0.022681441\n",
      "Step = 169 train_loss: 0.027722994 val_loss: 0.02274077\n",
      "Step = 170 train_loss: 0.032763664 val_loss: 0.02348827\n",
      "Step = 171 train_loss: 0.030695673 val_loss: 0.024755085\n",
      "Step = 172 train_loss: 0.03047634 val_loss: 0.026989123\n",
      "Step = 173 train_loss: 0.028956164 val_loss: 0.029563744\n",
      "Step = 174 train_loss: 0.029830135 val_loss: 0.032072436\n",
      "Step = 175 train_loss: 0.02785634 val_loss: 0.03443546\n",
      "Step = 176 train_loss: 0.028685864 val_loss: 0.036543693\n",
      "Step = 177 train_loss: 0.032598548 val_loss: 0.038102828\n",
      "Step = 178 train_loss: 0.028225206 val_loss: 0.039201163\n",
      "Step = 179 train_loss: 0.03034561 val_loss: 0.039914753\n",
      "Step = 180 train_loss: 0.028941898 val_loss: 0.040128686\n",
      "Step = 181 train_loss: 0.025748925 val_loss: 0.039274115\n",
      "Step = 182 train_loss: 0.02821223 val_loss: 0.038481243\n",
      "Step = 183 train_loss: 0.029259486 val_loss: 0.03160804\n",
      "Step = 184 train_loss: 0.02825472 val_loss: 0.02487675\n",
      "Step = 185 train_loss: 0.029995086 val_loss: 0.022014257\n",
      "Step = 186 train_loss: 0.03099407 val_loss: 0.021511257\n",
      "Step = 187 train_loss: 0.027758203 val_loss: 0.021552518\n",
      "Step = 188 train_loss: 0.029942125 val_loss: 0.022168256\n",
      "Step = 189 train_loss: 0.028819986 val_loss: 0.023532752\n",
      "Step = 190 train_loss: 0.030213611 val_loss: 0.026012057\n",
      "Step = 191 train_loss: 0.029369563 val_loss: 0.029212516\n",
      "Step = 192 train_loss: 0.028858634 val_loss: 0.032301888\n",
      "Step = 193 train_loss: 0.02940029 val_loss: 0.033847705\n",
      "Step = 194 train_loss: 0.02736504 val_loss: 0.034453116\n",
      "Step = 195 train_loss: 0.028924711 val_loss: 0.03459681\n",
      "Step = 196 train_loss: 0.028525207 val_loss: 0.035083584\n",
      "Step = 197 train_loss: 0.025438823 val_loss: 0.030584082\n",
      "Step = 198 train_loss: 0.028380893 val_loss: 0.022250148\n",
      "Step = 199 train_loss: 0.029194277 val_loss: 0.020604352\n",
      "Step = 200 train_loss: 0.030231012 val_loss: 0.021178752\n",
      "Step = 201 train_loss: 0.0263202 val_loss: 0.02171028\n",
      "Step = 202 train_loss: 0.026447516 val_loss: 0.022109263\n",
      "Step = 203 train_loss: 0.02690043 val_loss: 0.022479713\n",
      "Step = 204 train_loss: 0.027799265 val_loss: 0.022550292\n",
      "Step = 205 train_loss: 0.026604678 val_loss: 0.022459589\n",
      "Step = 206 train_loss: 0.025244305 val_loss: 0.022271551\n",
      "Step = 207 train_loss: 0.026816377 val_loss: 0.022060988\n",
      "Step = 208 train_loss: 0.028391104 val_loss: 0.021747189\n",
      "Step = 209 train_loss: 0.026273616 val_loss: 0.021423599\n",
      "Step = 210 train_loss: 0.02489831 val_loss: 0.02128204\n",
      "Step = 211 train_loss: 0.02347345 val_loss: 0.02174091\n",
      "Step = 212 train_loss: 0.028492955 val_loss: 0.02199422\n",
      "Step = 213 train_loss: 0.025742153 val_loss: 0.022058064\n",
      "Step = 214 train_loss: 0.024297653 val_loss: 0.020919068\n",
      "Step = 215 train_loss: 0.030832877 val_loss: 0.02105605\n",
      "Step = 216 train_loss: 0.028915538 val_loss: 0.024168488\n",
      "Step = 217 train_loss: 0.031274028 val_loss: 0.027519263\n",
      "Step = 218 train_loss: 0.028831037 val_loss: 0.029986268\n",
      "Step = 219 train_loss: 0.029414624 val_loss: 0.03157445\n",
      "Step = 220 train_loss: 0.030211339 val_loss: 0.03279791\n",
      "Step = 221 train_loss: 0.030880399 val_loss: 0.033561613\n",
      "Step = 222 train_loss: 0.02886797 val_loss: 0.034095176\n",
      "Step = 223 train_loss: 0.028768403 val_loss: 0.034341767\n",
      "Step = 224 train_loss: 0.030125806 val_loss: 0.03435654\n",
      "Step = 225 train_loss: 0.029470576 val_loss: 0.03421138\n",
      "Step = 226 train_loss: 0.028729318 val_loss: 0.03422937\n",
      "Step = 227 train_loss: 0.028765054 val_loss: 0.034330495\n",
      "Step = 228 train_loss: 0.029462252 val_loss: 0.03455687\n",
      "Step = 229 train_loss: 0.029484976 val_loss: 0.034717623\n",
      "Step = 230 train_loss: 0.027939184 val_loss: 0.034787945\n",
      "Step = 231 train_loss: 0.028682195 val_loss: 0.034897536\n",
      "Step = 232 train_loss: 0.026564794 val_loss: 0.035031796\n",
      "Step = 233 train_loss: 0.027809383 val_loss: 0.03525585\n",
      "Step = 234 train_loss: 0.027156638 val_loss: 0.03520538\n",
      "Step = 235 train_loss: 0.028634844 val_loss: 0.035069786\n",
      "Step = 236 train_loss: 0.02889985 val_loss: 0.034669783\n",
      "Step = 237 train_loss: 0.031303633 val_loss: 0.03438244\n",
      "Step = 238 train_loss: 0.028845884 val_loss: 0.03396521\n",
      "Step = 239 train_loss: 0.026962526 val_loss: 0.033322245\n",
      "Step = 240 train_loss: 0.029802568 val_loss: 0.032799475\n",
      "Step = 241 train_loss: 0.027985096 val_loss: 0.032171786\n",
      "Step = 242 train_loss: 0.02583212 val_loss: 0.03161624\n",
      "Step = 243 train_loss: 0.03004887 val_loss: 0.031035159\n",
      "Step = 244 train_loss: 0.028003454 val_loss: 0.030367974\n",
      "Step = 245 train_loss: 0.026397508 val_loss: 0.029827807\n",
      "Step = 246 train_loss: 0.026738603 val_loss: 0.029412389\n",
      "Step = 247 train_loss: 0.03036347 val_loss: 0.029111158\n",
      "Step = 248 train_loss: 0.024894686 val_loss: 0.028688295\n",
      "Step = 249 train_loss: 0.029121187 val_loss: 0.028500615\n",
      "Step = 250 train_loss: 0.029033048 val_loss: 0.028356722\n",
      "Step = 251 train_loss: 0.028347658 val_loss: 0.028243737\n",
      "Step = 252 train_loss: 0.027555503 val_loss: 0.028261632\n",
      "Step = 253 train_loss: 0.027838092 val_loss: 0.028173191\n",
      "Step = 254 train_loss: 0.030566564 val_loss: 0.027939992\n",
      "Step = 255 train_loss: 0.028701635 val_loss: 0.027800612\n",
      "Step = 256 train_loss: 0.027422564 val_loss: 0.027581116\n",
      "Step = 257 train_loss: 0.02541995 val_loss: 0.027605603\n",
      "Step = 258 train_loss: 0.035378024 val_loss: 0.030425856\n",
      "Step = 259 train_loss: 0.027714537 val_loss: 0.030335177\n",
      "Step = 260 train_loss: 0.026902974 val_loss: 0.02983018\n",
      "Step = 261 train_loss: 0.028851716 val_loss: 0.03317757\n",
      "Step = 262 train_loss: 0.025934208 val_loss: 0.034879226\n",
      "Step = 263 train_loss: 0.02608089 val_loss: 0.035122827\n",
      "Step = 264 train_loss: 0.026393604 val_loss: 0.034922734\n",
      "Step = 265 train_loss: 0.027312 val_loss: 0.03446158\n",
      "Step = 266 train_loss: 0.027160667 val_loss: 0.03370385\n",
      "Step = 267 train_loss: 0.024209302 val_loss: 0.032267388\n",
      "Step = 268 train_loss: 0.027476698 val_loss: 0.02922981\n",
      "Step = 269 train_loss: 0.026375579 val_loss: 0.024526943\n",
      "Step = 270 train_loss: 0.028736891 val_loss: 0.02334154\n",
      "Step = 271 train_loss: 0.02452632 val_loss: 0.02293289\n",
      "Step = 272 train_loss: 0.023732726 val_loss: 0.022515638\n",
      "Step = 273 train_loss: 0.024676455 val_loss: 0.022194007\n",
      "Step = 274 train_loss: 0.026477773 val_loss: 0.021966252\n",
      "Step = 275 train_loss: 0.028691944 val_loss: 0.02400082\n",
      "Step = 276 train_loss: 0.030400695 val_loss: 0.025532758\n",
      "Step = 277 train_loss: 0.023750374 val_loss: 0.025135715\n",
      "Step = 278 train_loss: 0.022843804 val_loss: 0.024544027\n",
      "Step = 279 train_loss: 0.02578951 val_loss: 0.024094177\n",
      "Step = 280 train_loss: 0.026016554 val_loss: 0.02346879\n",
      "Step = 281 train_loss: 0.023793243 val_loss: 0.023048263\n",
      "Step = 282 train_loss: 0.024003549 val_loss: 0.022527015\n",
      "Step = 283 train_loss: 0.024758358 val_loss: 0.022187449\n",
      "Step = 284 train_loss: 0.020694567 val_loss: 0.021967897\n",
      "Step = 285 train_loss: 0.023284316 val_loss: 0.021801317\n",
      "Step = 286 train_loss: 0.021397142 val_loss: 0.021641484\n",
      "Step = 287 train_loss: 0.020959355 val_loss: 0.021489402\n",
      "Step = 288 train_loss: 0.022445379 val_loss: 0.021323174\n",
      "Step = 289 train_loss: 0.022345964 val_loss: 0.021108102\n",
      "Step = 290 train_loss: 0.023131093 val_loss: 0.020929895\n",
      "Step = 291 train_loss: 0.022630341 val_loss: 0.02083327\n",
      "Step = 292 train_loss: 0.021395452 val_loss: 0.020785132\n",
      "Step = 293 train_loss: 0.019561777 val_loss: 0.020787021\n",
      "Step = 294 train_loss: 0.02160605 val_loss: 0.020805785\n",
      "Step = 295 train_loss: 0.021669652 val_loss: 0.020464601\n",
      "Step = 296 train_loss: 0.023255788 val_loss: 0.019821815\n",
      "Step = 297 train_loss: 0.024812022 val_loss: 0.01916122\n",
      "Step = 298 train_loss: 0.021485899 val_loss: 0.018522603\n",
      "Step = 299 train_loss: 0.021241812 val_loss: 0.017919939\n",
      "Step = 300 train_loss: 0.021581156 val_loss: 0.017395858\n",
      "Step = 301 train_loss: 0.02235158 val_loss: 0.016940977\n",
      "Step = 302 train_loss: 0.020182176 val_loss: 0.016557286\n",
      "Step = 303 train_loss: 0.021106167 val_loss: 0.016197802\n",
      "Step = 304 train_loss: 0.019918703 val_loss: 0.015946008\n",
      "Step = 305 train_loss: 0.021383978 val_loss: 0.015866991\n",
      "Step = 306 train_loss: 0.019352265 val_loss: 0.01589864\n",
      "Step = 307 train_loss: 0.020394307 val_loss: 0.01621564\n",
      "Step = 308 train_loss: 0.023953285 val_loss: 0.016128007\n",
      "Step = 309 train_loss: 0.023362242 val_loss: 0.0161765\n",
      "Step = 310 train_loss: 0.021191629 val_loss: 0.016317168\n",
      "Step = 311 train_loss: 0.020111667 val_loss: 0.016236693\n",
      "Step = 312 train_loss: 0.021971323 val_loss: 0.015245679\n",
      "Step = 313 train_loss: 0.019303612 val_loss: 0.015326233\n",
      "Step = 314 train_loss: 0.02134272 val_loss: 0.015560614\n",
      "Step = 315 train_loss: 0.021892438 val_loss: 0.017377151\n",
      "Step = 316 train_loss: 0.02034873 val_loss: 0.01837446\n",
      "Step = 317 train_loss: 0.019639215 val_loss: 0.016211174\n",
      "Step = 318 train_loss: 0.022061568 val_loss: 0.019805036\n",
      "Step = 319 train_loss: 0.021595258 val_loss: 0.03440014\n",
      "Step = 320 train_loss: 0.024695043 val_loss: 0.034529142\n",
      "Step = 321 train_loss: 0.020841599 val_loss: 0.024793051\n",
      "Step = 322 train_loss: 0.021100007 val_loss: 0.015524561\n",
      "Step = 323 train_loss: 0.019037547 val_loss: 0.0194639\n",
      "Step = 324 train_loss: 0.022285314 val_loss: 0.020810563\n",
      "Step = 325 train_loss: 0.019757302 val_loss: 0.01688033\n",
      "Step = 326 train_loss: 0.019810556 val_loss: 0.024361236\n",
      "Step = 327 train_loss: 0.02082219 val_loss: 0.016732486\n",
      "Step = 328 train_loss: 0.019073796 val_loss: 0.018930634\n",
      "Step = 329 train_loss: 0.020902636 val_loss: 0.025167134\n",
      "Step = 330 train_loss: 0.020103931 val_loss: 0.049136702\n",
      "Step = 331 train_loss: 0.0179605 val_loss: 0.061704326\n",
      "Step = 332 train_loss: 0.019902779 val_loss: 0.05792627\n",
      "Step = 333 train_loss: 0.017889366 val_loss: 0.035258617\n",
      "Step = 334 train_loss: 0.01800869 val_loss: 0.024841407\n",
      "Step = 335 train_loss: 0.019014837 val_loss: 0.024185352\n",
      "Step = 336 train_loss: 0.021159519 val_loss: 0.027891153\n",
      "Step = 337 train_loss: 0.019416789 val_loss: 0.07360126\n",
      "Step = 338 train_loss: 0.020298908 val_loss: 0.058660287\n",
      "Step = 339 train_loss: 0.020419545 val_loss: 0.053343274\n",
      "Step = 340 train_loss: 0.022774786 val_loss: 0.05633608\n",
      "Step = 341 train_loss: 0.02684049 val_loss: 0.062381063\n",
      "Step = 342 train_loss: 0.020073473 val_loss: 0.07042449\n",
      "Step = 343 train_loss: 0.024391882 val_loss: 0.07890852\n",
      "Step = 344 train_loss: 0.028711908 val_loss: 0.08377145\n",
      "Step = 345 train_loss: 0.022167552 val_loss: 0.082369246\n",
      "Step = 346 train_loss: 0.021456484 val_loss: 0.07993072\n",
      "Step = 347 train_loss: 0.021725334 val_loss: 0.07735532\n",
      "Step = 348 train_loss: 0.030256582 val_loss: 0.074478\n",
      "Step = 349 train_loss: 0.020996777 val_loss: 0.07095661\n",
      "Step = 350 train_loss: 0.02589641 val_loss: 0.05531174\n",
      "Step = 351 train_loss: 0.019485284 val_loss: 0.04532806\n",
      "Step = 352 train_loss: 0.025270052 val_loss: 0.02183113\n",
      "Step = 353 train_loss: 0.017131204 val_loss: 0.021395572\n",
      "Step = 354 train_loss: 0.01718628 val_loss: 0.02329833\n",
      "Step = 355 train_loss: 0.019162178 val_loss: 0.043266784\n",
      "Step = 356 train_loss: 0.021915989 val_loss: 0.08501982\n",
      "Step = 357 train_loss: 0.019278407 val_loss: 0.09602363\n",
      "Step = 358 train_loss: 0.024871118 val_loss: 0.09345321\n",
      "Step = 359 train_loss: 0.019490544 val_loss: 0.09111103\n",
      "Step = 360 train_loss: 0.022468437 val_loss: 0.09417577\n",
      "Step = 361 train_loss: 0.02462398 val_loss: 0.0979359\n",
      "Step = 362 train_loss: 0.018303348 val_loss: 0.09847873\n",
      "Step = 363 train_loss: 0.020556537 val_loss: 0.09970493\n",
      "Step = 364 train_loss: 0.018379869 val_loss: 0.102181286\n",
      "Step = 365 train_loss: 0.021178467 val_loss: 0.10454877\n",
      "Step = 366 train_loss: 0.027437378 val_loss: 0.10572684\n",
      "Step = 367 train_loss: 0.017712083 val_loss: 0.10283624\n",
      "Step = 368 train_loss: 0.015580774 val_loss: 0.09637895\n",
      "Step = 369 train_loss: 0.027609168 val_loss: 0.09510521\n",
      "Step = 370 train_loss: 0.023418926 val_loss: 0.094809905\n",
      "Step = 371 train_loss: 0.032886617 val_loss: 0.09449852\n",
      "Step = 372 train_loss: 0.02266415 val_loss: 0.095221825\n",
      "Step = 373 train_loss: 0.016110701 val_loss: 0.09688265\n",
      "Step = 374 train_loss: 0.03241296 val_loss: 0.09795397\n",
      "Step = 375 train_loss: 0.018310305 val_loss: 0.09973217\n",
      "Step = 376 train_loss: 0.017527271 val_loss: 0.10617868\n",
      "Step = 377 train_loss: 0.0097557455 val_loss: 0.116080865\n",
      "Step = 378 train_loss: 0.011295482 val_loss: 0.11931895\n",
      "Step = 379 train_loss: 0.012594435 val_loss: 0.12195307\n",
      "Step = 380 train_loss: 0.014869965 val_loss: 0.1249756\n",
      "Step = 381 train_loss: 0.017190786 val_loss: 0.12795393\n",
      "Step = 382 train_loss: 0.010972404 val_loss: 0.13025574\n",
      "Step = 383 train_loss: 0.009046846 val_loss: 0.13176176\n",
      "Step = 384 train_loss: 0.025323657 val_loss: 0.13010086\n",
      "Step = 385 train_loss: 0.018728621 val_loss: 0.12748456\n",
      "Step = 386 train_loss: 0.025639027 val_loss: 0.1211456\n",
      "Step = 387 train_loss: 0.021769395 val_loss: 0.11125484\n",
      "Step = 388 train_loss: 0.015301023 val_loss: 0.100518845\n",
      "Step = 389 train_loss: 0.016126228 val_loss: 0.07594843\n",
      "Step = 390 train_loss: 0.015518004 val_loss: 0.02015964\n",
      "Step = 391 train_loss: 0.015478464 val_loss: 0.023100765\n",
      "Step = 392 train_loss: 0.022142418 val_loss: 0.030236386\n",
      "Step = 393 train_loss: 0.0109393615 val_loss: 0.030216374\n",
      "Step = 394 train_loss: 0.014410614 val_loss: 0.029894006\n",
      "Step = 395 train_loss: 0.020697674 val_loss: 0.02854411\n",
      "Step = 396 train_loss: 0.009603063 val_loss: 0.028119113\n",
      "Step = 397 train_loss: 0.016034175 val_loss: 0.028568957\n",
      "Step = 398 train_loss: 0.010117466 val_loss: 0.029456658\n",
      "Step = 399 train_loss: 0.0129220085 val_loss: 0.030690635\n",
      "Step = 400 train_loss: 0.01645771 val_loss: 0.031865936\n",
      "Step = 401 train_loss: 0.0099809915 val_loss: 0.03265279\n",
      "Step = 402 train_loss: 0.0074632936 val_loss: 0.03323014\n",
      "Step = 403 train_loss: 0.024151964 val_loss: 0.032382004\n",
      "Step = 404 train_loss: 0.014501325 val_loss: 0.031520244\n",
      "Step = 405 train_loss: 0.007625965 val_loss: 0.01708745\n",
      "Step = 406 train_loss: 0.014658768 val_loss: 0.11451149\n",
      "Step = 407 train_loss: 0.01713494 val_loss: 0.14524394\n",
      "Step = 408 train_loss: 0.013506825 val_loss: 0.15461823\n",
      "Step = 409 train_loss: 0.0183016 val_loss: 0.15683417\n",
      "Step = 410 train_loss: 0.015540345 val_loss: 0.15639475\n",
      "Step = 411 train_loss: 0.020722555 val_loss: 0.15242973\n",
      "Step = 412 train_loss: 0.0140931215 val_loss: 0.14578365\n",
      "Step = 413 train_loss: 0.011743094 val_loss: 0.13649063\n",
      "Step = 414 train_loss: 0.015503938 val_loss: 0.11764973\n",
      "Step = 415 train_loss: 0.011452895 val_loss: 0.09256324\n",
      "Step = 416 train_loss: 0.008905611 val_loss: 0.056252286\n",
      "Step = 417 train_loss: 0.012486533 val_loss: 0.012266955\n",
      "Step = 418 train_loss: 0.014831195 val_loss: 0.019910537\n",
      "Step = 419 train_loss: 0.009154607 val_loss: 0.03166791\n",
      "Step = 420 train_loss: 0.0138953645 val_loss: 0.0397725\n",
      "Step = 421 train_loss: 0.012705977 val_loss: 0.04335375\n",
      "Step = 422 train_loss: 0.019471148 val_loss: 0.044280987\n",
      "Step = 423 train_loss: 0.010481229 val_loss: 0.0424929\n",
      "Step = 424 train_loss: 0.011654918 val_loss: 0.039995234\n",
      "Step = 425 train_loss: 0.008466998 val_loss: 0.03838263\n",
      "Step = 426 train_loss: 0.010291339 val_loss: 0.03724354\n",
      "Step = 427 train_loss: 0.012484694 val_loss: 0.03665166\n",
      "Step = 428 train_loss: 0.010577341 val_loss: 0.03598534\n",
      "Step = 429 train_loss: 0.028294606 val_loss: 0.035064638\n",
      "Step = 430 train_loss: 0.025284521 val_loss: 0.034299783\n",
      "Step = 431 train_loss: 0.03139369 val_loss: 0.033408545\n",
      "Step = 432 train_loss: 0.027023759 val_loss: 0.03247992\n",
      "Step = 433 train_loss: 0.026207171 val_loss: 0.03159449\n",
      "Step = 434 train_loss: 0.019085104 val_loss: 0.03065997\n",
      "Step = 435 train_loss: 0.027893232 val_loss: 0.0298433\n",
      "Step = 436 train_loss: 0.025179762 val_loss: 0.028988294\n",
      "Step = 437 train_loss: 0.027447041 val_loss: 0.02843309\n",
      "Step = 438 train_loss: 0.027802488 val_loss: 0.028088221\n",
      "Step = 439 train_loss: 0.03522219 val_loss: 0.027791977\n",
      "Step = 440 train_loss: 0.035451822 val_loss: 0.02736994\n",
      "Step = 441 train_loss: 0.03219571 val_loss: 0.02670668\n",
      "Step = 442 train_loss: 0.031557914 val_loss: 0.025927227\n",
      "Step = 443 train_loss: 0.028263858 val_loss: 0.024932342\n",
      "Step = 444 train_loss: 0.030326955 val_loss: 0.023913499\n",
      "Step = 445 train_loss: 0.029077968 val_loss: 0.022913374\n",
      "Step = 446 train_loss: 0.02776618 val_loss: 0.0220689\n",
      "Step = 447 train_loss: 0.026494833 val_loss: 0.021401465\n",
      "Step = 448 train_loss: 0.028648023 val_loss: 0.020955082\n",
      "Step = 449 train_loss: 0.029575553 val_loss: 0.020702073\n",
      "Step = 450 train_loss: 0.030169906 val_loss: 0.02058615\n",
      "Step = 451 train_loss: 0.031703237 val_loss: 0.020559156\n",
      "Step = 452 train_loss: 0.031569578 val_loss: 0.020578697\n",
      "Step = 453 train_loss: 0.026288211 val_loss: 0.02060214\n",
      "Step = 454 train_loss: 0.030600708 val_loss: 0.020619214\n",
      "Step = 455 train_loss: 0.031190079 val_loss: 0.020628445\n",
      "Step = 456 train_loss: 0.02785376 val_loss: 0.020610524\n",
      "Step = 457 train_loss: 0.02818598 val_loss: 0.020576209\n",
      "Step = 458 train_loss: 0.028319336 val_loss: 0.020547977\n",
      "Step = 459 train_loss: 0.029211013 val_loss: 0.020516548\n",
      "Step = 460 train_loss: 0.028480439 val_loss: 0.02048894\n",
      "Step = 461 train_loss: 0.02729074 val_loss: 0.020467572\n",
      "Step = 462 train_loss: 0.02868753 val_loss: 0.020448992\n",
      "Step = 463 train_loss: 0.028729727 val_loss: 0.020438606\n",
      "Step = 464 train_loss: 0.028751543 val_loss: 0.02043386\n",
      "Step = 465 train_loss: 0.02655266 val_loss: 0.020431701\n",
      "Step = 466 train_loss: 0.02736951 val_loss: 0.020437669\n",
      "Step = 467 train_loss: 0.031240083 val_loss: 0.020443708\n",
      "Step = 468 train_loss: 0.027975243 val_loss: 0.020456323\n",
      "Step = 469 train_loss: 0.027292378 val_loss: 0.02046801\n",
      "Step = 470 train_loss: 0.027334332 val_loss: 0.020481702\n",
      "Step = 471 train_loss: 0.032710522 val_loss: 0.020500822\n",
      "Step = 472 train_loss: 0.030376036 val_loss: 0.020526247\n",
      "Step = 473 train_loss: 0.028253369 val_loss: 0.020546358\n",
      "Step = 474 train_loss: 0.029247519 val_loss: 0.02056746\n",
      "Step = 475 train_loss: 0.027821232 val_loss: 0.020586638\n",
      "Step = 476 train_loss: 0.027053267 val_loss: 0.02059958\n",
      "Step = 477 train_loss: 0.026889293 val_loss: 0.020609302\n",
      "Step = 478 train_loss: 0.03041436 val_loss: 0.020607654\n",
      "Step = 479 train_loss: 0.028127728 val_loss: 0.020609189\n",
      "Step = 480 train_loss: 0.031790838 val_loss: 0.020616913\n",
      "Step = 481 train_loss: 0.028558612 val_loss: 0.020622484\n",
      "Step = 482 train_loss: 0.027775727 val_loss: 0.020628605\n",
      "Step = 483 train_loss: 0.028053757 val_loss: 0.020635819\n",
      "Step = 484 train_loss: 0.02947519 val_loss: 0.020643804\n",
      "Step = 485 train_loss: 0.027195606 val_loss: 0.020634554\n",
      "Step = 486 train_loss: 0.028467715 val_loss: 0.020630507\n",
      "Step = 487 train_loss: 0.029226724 val_loss: 0.020635806\n",
      "Step = 488 train_loss: 0.026639344 val_loss: 0.020627886\n",
      "Step = 489 train_loss: 0.027972419 val_loss: 0.02061829\n",
      "Step = 490 train_loss: 0.028447133 val_loss: 0.020623038\n",
      "Step = 491 train_loss: 0.029561887 val_loss: 0.020618549\n",
      "Step = 492 train_loss: 0.027807266 val_loss: 0.0206192\n",
      "Step = 493 train_loss: 0.032295883 val_loss: 0.020625992\n",
      "Step = 494 train_loss: 0.03027116 val_loss: 0.020625547\n",
      "Step = 495 train_loss: 0.02860301 val_loss: 0.020627627\n",
      "Step = 496 train_loss: 0.028595934 val_loss: 0.020621382\n",
      "Step = 497 train_loss: 0.031126317 val_loss: 0.020619018\n",
      "Step = 498 train_loss: 0.029329915 val_loss: 0.02061883\n",
      "Step = 499 train_loss: 0.027808798 val_loss: 0.020620396\n",
      "Step = 500 train_loss: 0.02822827 val_loss: 0.020621352\n",
      "Step = 501 train_loss: 0.030534368 val_loss: 0.020621605\n",
      "Step = 502 train_loss: 0.029515954 val_loss: 0.020620909\n",
      "Step = 503 train_loss: 0.028015953 val_loss: 0.020619845\n",
      "Step = 504 train_loss: 0.025603514 val_loss: 0.020617332\n",
      "Step = 505 train_loss: 0.029325066 val_loss: 0.020617459\n",
      "Step = 506 train_loss: 0.030324565 val_loss: 0.020616032\n",
      "Step = 507 train_loss: 0.028414775 val_loss: 0.020612402\n",
      "Step = 508 train_loss: 0.028337508 val_loss: 0.020617278\n",
      "Step = 509 train_loss: 0.026945503 val_loss: 0.02062247\n",
      "Step = 510 train_loss: 0.02604247 val_loss: 0.020618461\n",
      "Step = 511 train_loss: 0.028046457 val_loss: 0.020618416\n",
      "Step = 512 train_loss: 0.030430946 val_loss: 0.020622\n",
      "Step = 513 train_loss: 0.025846269 val_loss: 0.020620126\n",
      "Step = 514 train_loss: 0.0287827 val_loss: 0.020614546\n",
      "Step = 515 train_loss: 0.029913412 val_loss: 0.020613678\n",
      "Step = 516 train_loss: 0.025119336 val_loss: 0.020614773\n",
      "Step = 517 train_loss: 0.028702047 val_loss: 0.020596648\n",
      "Step = 518 train_loss: 0.028702168 val_loss: 0.020577641\n",
      "Step = 519 train_loss: 0.028089903 val_loss: 0.02056545\n",
      "Step = 520 train_loss: 0.0268734 val_loss: 0.020558082\n",
      "Step = 521 train_loss: 0.027743181 val_loss: 0.020546831\n",
      "Step = 522 train_loss: 0.028966121 val_loss: 0.020531742\n",
      "Step = 523 train_loss: 0.028435227 val_loss: 0.020513745\n",
      "Step = 524 train_loss: 0.02731483 val_loss: 0.020502822\n",
      "Step = 525 train_loss: 0.028548906 val_loss: 0.020488642\n",
      "Step = 526 train_loss: 0.02684666 val_loss: 0.020489758\n",
      "Step = 527 train_loss: 0.023635259 val_loss: 0.020474633\n",
      "Step = 528 train_loss: 0.027820747 val_loss: 0.020457791\n",
      "Step = 529 train_loss: 0.02669712 val_loss: 0.020440478\n",
      "Step = 530 train_loss: 0.029228795 val_loss: 0.020423809\n",
      "Step = 531 train_loss: 0.028567182 val_loss: 0.020412143\n",
      "Step = 532 train_loss: 0.031513702 val_loss: 0.020397317\n",
      "Step = 533 train_loss: 0.027960153 val_loss: 0.020379206\n",
      "Step = 534 train_loss: 0.02771553 val_loss: 0.0203583\n",
      "Step = 535 train_loss: 0.028546978 val_loss: 0.020329406\n",
      "Step = 536 train_loss: 0.030989375 val_loss: 0.020299308\n",
      "Step = 537 train_loss: 0.030741774 val_loss: 0.020263188\n",
      "Step = 538 train_loss: 0.030703757 val_loss: 0.020225156\n",
      "Step = 539 train_loss: 0.02798651 val_loss: 0.020186901\n",
      "Step = 540 train_loss: 0.030584939 val_loss: 0.020145075\n",
      "Step = 541 train_loss: 0.02833692 val_loss: 0.020094834\n",
      "Step = 542 train_loss: 0.02878765 val_loss: 0.020030998\n",
      "Step = 543 train_loss: 0.025233377 val_loss: 0.019987093\n",
      "Step = 544 train_loss: 0.027658956 val_loss: 0.019919274\n",
      "Step = 545 train_loss: 0.028939325 val_loss: 0.019846588\n",
      "Step = 546 train_loss: 0.027039351 val_loss: 0.019767893\n",
      "Step = 547 train_loss: 0.028487554 val_loss: 0.019680878\n",
      "Step = 548 train_loss: 0.02796223 val_loss: 0.019571235\n",
      "Step = 549 train_loss: 0.02967447 val_loss: 0.019415295\n",
      "Step = 550 train_loss: 0.025353936 val_loss: 0.019186089\n",
      "Step = 551 train_loss: 0.029575307 val_loss: 0.019665394\n",
      "Step = 552 train_loss: 0.029683782 val_loss: 0.023315314\n",
      "Step = 553 train_loss: 0.026166888 val_loss: 0.05489372\n",
      "Step = 554 train_loss: 0.03107061 val_loss: 0.10556784\n",
      "Step = 555 train_loss: 0.02600094 val_loss: 0.14818916\n",
      "Step = 556 train_loss: 0.026191164 val_loss: 0.17014863\n",
      "Step = 557 train_loss: 0.026665978 val_loss: 0.18153787\n",
      "Step = 558 train_loss: 0.016625954 val_loss: 0.19474478\n",
      "Step = 559 train_loss: 0.021529507 val_loss: 0.18754663\n",
      "Step = 560 train_loss: 0.022111718 val_loss: 0.19574173\n",
      "Step = 561 train_loss: 0.020449612 val_loss: 0.20097396\n",
      "Step = 562 train_loss: 0.0218487 val_loss: 0.20456854\n",
      "Step = 563 train_loss: 0.022954809 val_loss: 0.20557643\n",
      "Step = 564 train_loss: 0.017240407 val_loss: 0.20549956\n",
      "Step = 565 train_loss: 0.022996483 val_loss: 0.20650719\n",
      "Step = 566 train_loss: 0.029352298 val_loss: 0.20612568\n",
      "Step = 567 train_loss: 0.01983265 val_loss: 0.20568334\n",
      "Step = 568 train_loss: 0.028037604 val_loss: 0.20306699\n",
      "Step = 569 train_loss: 0.025145067 val_loss: 0.19939879\n",
      "Step = 570 train_loss: 0.019447826 val_loss: 0.1953067\n",
      "Step = 571 train_loss: 0.02855438 val_loss: 0.19077605\n",
      "Step = 572 train_loss: 0.023543846 val_loss: 0.1859936\n",
      "Step = 573 train_loss: 0.025857547 val_loss: 0.18095715\n",
      "Step = 574 train_loss: 0.019326359 val_loss: 0.17654425\n",
      "Step = 575 train_loss: 0.029233979 val_loss: 0.17068765\n",
      "Step = 576 train_loss: 0.024365554 val_loss: 0.164197\n",
      "Step = 577 train_loss: 0.023495216 val_loss: 0.15780276\n",
      "Step = 578 train_loss: 0.020053646 val_loss: 0.1519199\n",
      "Step = 579 train_loss: 0.028256437 val_loss: 0.14536129\n",
      "Step = 580 train_loss: 0.020247575 val_loss: 0.1391841\n",
      "Step = 581 train_loss: 0.024227452 val_loss: 0.1329415\n",
      "Step = 582 train_loss: 0.031823818 val_loss: 0.12918472\n",
      "Step = 583 train_loss: 0.023425646 val_loss: 0.13194717\n",
      "Step = 584 train_loss: 0.019455187 val_loss: 0.1393591\n",
      "Step = 585 train_loss: 0.025622139 val_loss: 0.14671789\n",
      "Step = 586 train_loss: 0.024913162 val_loss: 0.10627595\n",
      "Step = 587 train_loss: 0.023555491 val_loss: 0.10951589\n",
      "Step = 588 train_loss: 0.024866385 val_loss: 0.10663012\n",
      "Step = 589 train_loss: 0.025325608 val_loss: 0.09543875\n",
      "Step = 590 train_loss: 0.027483907 val_loss: 0.084286354\n",
      "Step = 591 train_loss: 0.02279862 val_loss: 0.07331016\n",
      "Step = 592 train_loss: 0.029777708 val_loss: 0.061345316\n",
      "Step = 593 train_loss: 0.018481968 val_loss: 0.055969153\n",
      "Step = 594 train_loss: 0.021802781 val_loss: 0.052164786\n",
      "Step = 595 train_loss: 0.023953272 val_loss: 0.04692138\n",
      "Step = 596 train_loss: 0.020897444 val_loss: 0.041274555\n",
      "Step = 597 train_loss: 0.022871068 val_loss: 0.034978125\n",
      "Step = 598 train_loss: 0.022619072 val_loss: 0.0294966\n",
      "Step = 599 train_loss: 0.026260782 val_loss: 0.024455138\n",
      "Best model at index: 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_16832\\574829257.py:104: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('Best_target_model/net_parameters.pkl')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "# summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "# # 动态调整学习率\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.8 ** (epoch // 40))\n",
    "\n",
    "# # 将学习率调度器修改为指数下降\n",
    "# initial_lr = 0.0001  # 初始学习率\n",
    "# decay_rate = 0.96  # 每个epoch衰减的比例\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: decay_rate ** epoch)\n",
    "\n",
    "# 假设你已经定义了模型和优化器\n",
    "# 当验证损失不下降时，减少学习率\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# 创建历史记录DataFrame\n",
    "history = pd.DataFrame(columns=['index', 'train_loss', 'val_loss'])\n",
    "val_losses = []\n",
    "max_models_to_keep = 10\n",
    "saved_models=[]\n",
    "import os\n",
    "\n",
    "for t in range(600):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    # 保存模型\n",
    "    model_path = 'Target_model/net_parameters'+str(t)+'.pkl'\n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, model_path)\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print('Step = %d' % t, 'train_loss:', train_loss.data.numpy(), 'val_loss:', val_loss.data.numpy())\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    # 添加当前模型及其验证损失\n",
    "    saved_models.append((val_loss.item(), model_path))\n",
    "\n",
    "    # 保持模型数量不超过max_models_to_keep\n",
    "    if len(saved_models) > max_models_to_keep:\n",
    "        # 找到验证损失最大的一组模型并删除\n",
    "        saved_models.sort(key=lambda x: x[0])  # 排序，根据损失\n",
    "        os.remove(saved_models.pop()[1])  # 删除损失最大的模型\n",
    "    # 加入训练历史记录\n",
    "    # 创建新的一行数据\n",
    "    new_entry = pd.DataFrame({'index': [t], 'train_loss': [train_loss.item()], 'val_loss': [val_loss.item()]})\n",
    "    \n",
    "    # 使用concat进行数据合并\n",
    "    history = pd.concat([history, new_entry], ignore_index=True)\n",
    "\n",
    "'''find the best model'''\n",
    "best_index = val_losses.index(np.min(val_losses))\n",
    "print(f'Best model at index: {best_index}')\n",
    "\n",
    "'''copy the best model from model file to the model_test file'''\n",
    "shutil.copyfile(f'Target_model/net_parameters{best_index}.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "'''Reload the best model''' \n",
    "model = Net()\n",
    "checkpoint = torch.load('Best_target_model/net_parameters.pkl')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# history = np.DataFrame(columns=['index','train_loss', 'val_loss'])\n",
    "#画出训练过程中的loss变化曲线\n",
    "plt.plot(history['train_loss'], label='train_loss')\n",
    "plt.plot(history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.34095532\n",
      "mpe_val: 0.31471056\n",
      "mpe_a: 0.35636905552028614\n",
      "mpe_b: 0.11729715302432538\n",
      "rmse_train: 286.42108\n",
      "rmse_val: 247.46791\n",
      "rmse_a: 290.02445766954145\n",
      "rmse_b: 228.78701886252202\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.43437469005584717\n",
      "r2_val: 0.4116774797439575\n",
      "r2_a: 0.4488328830675947\n",
      "r2_b: 0.3560767892591341\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
