{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型效果\n",
    "## 增加卷积层数目，拟合程度严重下降，作者第二数据集他妈的是不是造假的\n",
    "r2_train: 0.9049155116081238\n",
    "r2_val: 0.8364689946174622\n",
    "r2_a: 0.9160007727042746\n",
    "r2_b: -0.204700199031272\n",
    "\n",
    "对比（非lstm架构）\n",
    "r2_train: 0.9947811961174011\n",
    "r2_val: 0.9749422669410706\n",
    "r2_a: 0.9464100454957816\n",
    "r2_b: 0.6836000724611211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 修改模型需要改层数，回传部分，以及lstm部分\n",
    "'''Define the CNN model''' \n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(8, 16, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(32, 64, kernel_size=(2, 2), stride=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(64, 128, kernel_size=(1, 2), stride=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(128, 128, kernel_size=(1, 2), stride=1),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "        )\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(128, 64, batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(14336, 100)\n",
    "        self.drop_layer6 = torch.nn.Dropout(p=0.2)\n",
    "        self.fc2 = torch.nn.Linear(100, 1)   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            x = self.layers(x) \n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.drop_layer6(x)                     \n",
    "            x = torch.sigmoid(self.fc2(x))        \n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during forward pass: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (12): Dropout(p=0.2, inplace=False)\n",
      "    (13): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (16): Dropout(p=0.2, inplace=False)\n",
      "    (17): Conv2d(64, 128, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (20): Dropout(p=0.2, inplace=False)\n",
      "    (21): Conv2d(128, 128, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (24): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(128, 64, batch_first=True)\n",
      "  (fc1): Linear(in_features=14336, out_features=100, bias=True)\n",
      "  (drop_layer6): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 11, 24]          20,008\n",
      "       BatchNorm2d-2            [-1, 8, 11, 24]              16\n",
      "         LeakyReLU-3            [-1, 8, 11, 24]               0\n",
      "         MaxPool2d-4            [-1, 8, 10, 23]               0\n",
      "           Dropout-5            [-1, 8, 10, 23]               0\n",
      "            Conv2d-6            [-1, 16, 9, 21]             784\n",
      "       BatchNorm2d-7            [-1, 16, 9, 21]              32\n",
      "         LeakyReLU-8            [-1, 16, 9, 21]               0\n",
      "           Dropout-9            [-1, 16, 9, 21]               0\n",
      "           Conv2d-10            [-1, 32, 8, 19]           3,104\n",
      "      BatchNorm2d-11            [-1, 32, 8, 19]              64\n",
      "        LeakyReLU-12            [-1, 32, 8, 19]               0\n",
      "          Dropout-13            [-1, 32, 8, 19]               0\n",
      "           Conv2d-14            [-1, 64, 7, 18]           8,256\n",
      "      BatchNorm2d-15            [-1, 64, 7, 18]             128\n",
      "        LeakyReLU-16            [-1, 64, 7, 18]               0\n",
      "          Dropout-17            [-1, 64, 7, 18]               0\n",
      "           Conv2d-18           [-1, 128, 7, 17]          16,512\n",
      "      BatchNorm2d-19           [-1, 128, 7, 17]             256\n",
      "        LeakyReLU-20           [-1, 128, 7, 17]               0\n",
      "          Dropout-21           [-1, 128, 7, 17]               0\n",
      "           Conv2d-22           [-1, 128, 7, 16]          32,896\n",
      "      BatchNorm2d-23           [-1, 128, 7, 16]             256\n",
      "        LeakyReLU-24           [-1, 128, 7, 16]               0\n",
      "          Dropout-25           [-1, 128, 7, 16]               0\n",
      "           Linear-26                  [-1, 100]       1,433,700\n",
      "          Dropout-27                  [-1, 100]               0\n",
      "           Linear-28                    [-1, 1]             101\n",
      "================================================================\n",
      "Total params: 1,516,113\n",
      "Trainable params: 1,516,113\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 1.47\n",
      "Params size (MB): 5.78\n",
      "Estimated Total Size (MB): 7.63\n",
      "----------------------------------------------------------------\n",
      "Step = 0 train_loss: 0.08291534 val_loss: 0.06549045\n",
      "Step = 1 train_loss: 0.064083554 val_loss: 0.06515793\n",
      "Step = 2 train_loss: 0.056453865 val_loss: 0.06448084\n",
      "Step = 3 train_loss: 0.046022154 val_loss: 0.06353711\n",
      "Step = 4 train_loss: 0.043033782 val_loss: 0.062362704\n",
      "Step = 5 train_loss: 0.036985382 val_loss: 0.06108933\n",
      "Step = 6 train_loss: 0.036965106 val_loss: 0.05965753\n",
      "Step = 7 train_loss: 0.030705182 val_loss: 0.058075417\n",
      "Step = 8 train_loss: 0.032926105 val_loss: 0.056487553\n",
      "Step = 9 train_loss: 0.035088353 val_loss: 0.054892622\n",
      "Step = 10 train_loss: 0.028762948 val_loss: 0.05328689\n",
      "Step = 11 train_loss: 0.032414164 val_loss: 0.05169705\n",
      "Step = 12 train_loss: 0.02633365 val_loss: 0.050171684\n",
      "Step = 13 train_loss: 0.03357164 val_loss: 0.04871495\n",
      "Step = 14 train_loss: 0.032132607 val_loss: 0.047397878\n",
      "Step = 15 train_loss: 0.03300536 val_loss: 0.046164915\n",
      "Step = 16 train_loss: 0.03527029 val_loss: 0.044993605\n",
      "Step = 17 train_loss: 0.034635834 val_loss: 0.043905023\n",
      "Step = 18 train_loss: 0.034306817 val_loss: 0.04293435\n",
      "Step = 19 train_loss: 0.03745428 val_loss: 0.04214093\n",
      "Step = 20 train_loss: 0.031436 val_loss: 0.041515626\n",
      "Step = 21 train_loss: 0.032074537 val_loss: 0.04109609\n",
      "Step = 22 train_loss: 0.031946886 val_loss: 0.040831365\n",
      "Step = 23 train_loss: 0.030447163 val_loss: 0.04069582\n",
      "Step = 24 train_loss: 0.031640533 val_loss: 0.040601764\n",
      "Step = 25 train_loss: 0.028295573 val_loss: 0.040568464\n",
      "Step = 26 train_loss: 0.029500736 val_loss: 0.040544167\n",
      "Step = 27 train_loss: 0.027581573 val_loss: 0.040595897\n",
      "Step = 28 train_loss: 0.025819559 val_loss: 0.04067904\n",
      "Step = 29 train_loss: 0.027413517 val_loss: 0.04085636\n",
      "Step = 30 train_loss: 0.024630211 val_loss: 0.0409282\n",
      "Step = 31 train_loss: 0.035431817 val_loss: 0.041044626\n",
      "Step = 32 train_loss: 0.03207036 val_loss: 0.041074153\n",
      "Step = 33 train_loss: 0.033476386 val_loss: 0.04111017\n",
      "Step = 34 train_loss: 0.029721964 val_loss: 0.04103071\n",
      "Step = 35 train_loss: 0.02833262 val_loss: 0.040844485\n",
      "Step = 36 train_loss: 0.033821777 val_loss: 0.040627982\n",
      "Step = 37 train_loss: 0.025147814 val_loss: 0.04047478\n",
      "Step = 38 train_loss: 0.029610079 val_loss: 0.04027531\n",
      "Step = 39 train_loss: 0.028309379 val_loss: 0.040133413\n",
      "Step = 40 train_loss: 0.026463639 val_loss: 0.040072314\n",
      "Step = 41 train_loss: 0.030453298 val_loss: 0.039926603\n",
      "Step = 42 train_loss: 0.03700909 val_loss: 0.039759293\n",
      "Step = 43 train_loss: 0.030770553 val_loss: 0.039607745\n",
      "Step = 44 train_loss: 0.031321492 val_loss: 0.039313063\n",
      "Step = 45 train_loss: 0.031221451 val_loss: 0.038963214\n",
      "Step = 46 train_loss: 0.032874078 val_loss: 0.038653072\n",
      "Step = 47 train_loss: 0.03206686 val_loss: 0.038398914\n",
      "Step = 48 train_loss: 0.032128796 val_loss: 0.03809062\n",
      "Step = 49 train_loss: 0.033544876 val_loss: 0.037789423\n",
      "Step = 50 train_loss: 0.029433869 val_loss: 0.03757671\n",
      "Step = 51 train_loss: 0.023878686 val_loss: 0.037376218\n",
      "Step = 52 train_loss: 0.029507497 val_loss: 0.037219465\n",
      "Step = 53 train_loss: 0.028856374 val_loss: 0.03693491\n",
      "Step = 54 train_loss: 0.032218345 val_loss: 0.03652199\n",
      "Step = 55 train_loss: 0.035736438 val_loss: 0.036180895\n",
      "Step = 56 train_loss: 0.03303866 val_loss: 0.035839394\n",
      "Step = 57 train_loss: 0.030523488 val_loss: 0.03537424\n",
      "Step = 58 train_loss: 0.028387146 val_loss: 0.03487347\n",
      "Step = 59 train_loss: 0.030408263 val_loss: 0.034836855\n",
      "Step = 60 train_loss: 0.029047795 val_loss: 0.034813557\n",
      "Step = 61 train_loss: 0.029892724 val_loss: 0.034906726\n",
      "Step = 62 train_loss: 0.030985227 val_loss: 0.034969576\n",
      "Step = 63 train_loss: 0.029316906 val_loss: 0.035003632\n",
      "Step = 64 train_loss: 0.029912343 val_loss: 0.03495252\n",
      "Step = 65 train_loss: 0.02971194 val_loss: 0.034906417\n",
      "Step = 66 train_loss: 0.027795354 val_loss: 0.03496217\n",
      "Step = 67 train_loss: 0.026875744 val_loss: 0.034987204\n",
      "Step = 68 train_loss: 0.03096224 val_loss: 0.035389815\n",
      "Step = 69 train_loss: 0.03151018 val_loss: 0.035694722\n",
      "Step = 70 train_loss: 0.026811775 val_loss: 0.035800878\n",
      "Step = 71 train_loss: 0.026585022 val_loss: 0.03594954\n",
      "Step = 72 train_loss: 0.03240318 val_loss: 0.036172368\n",
      "Step = 73 train_loss: 0.029540261 val_loss: 0.036260854\n",
      "Step = 74 train_loss: 0.031879626 val_loss: 0.036229093\n",
      "Step = 75 train_loss: 0.025150387 val_loss: 0.036251705\n",
      "Step = 76 train_loss: 0.028131455 val_loss: 0.036294766\n",
      "Step = 77 train_loss: 0.030114697 val_loss: 0.035985067\n",
      "Step = 78 train_loss: 0.029984087 val_loss: 0.03554299\n",
      "Step = 79 train_loss: 0.030729234 val_loss: 0.035337094\n",
      "Step = 80 train_loss: 0.032478426 val_loss: 0.03514839\n",
      "Step = 81 train_loss: 0.02749322 val_loss: 0.035052016\n",
      "Step = 82 train_loss: 0.032153707 val_loss: 0.0349979\n",
      "Step = 83 train_loss: 0.031218056 val_loss: 0.03494306\n",
      "Step = 84 train_loss: 0.03386103 val_loss: 0.034923494\n",
      "Step = 85 train_loss: 0.034042217 val_loss: 0.034884028\n",
      "Step = 86 train_loss: 0.028136702 val_loss: 0.03511016\n",
      "Step = 87 train_loss: 0.028917233 val_loss: 0.03524851\n",
      "Step = 88 train_loss: 0.031260155 val_loss: 0.035291623\n",
      "Step = 89 train_loss: 0.024121381 val_loss: 0.035218034\n",
      "Step = 90 train_loss: 0.027229488 val_loss: 0.03506767\n",
      "Step = 91 train_loss: 0.030098744 val_loss: 0.03486917\n",
      "Step = 92 train_loss: 0.030046262 val_loss: 0.03458393\n",
      "Step = 93 train_loss: 0.031650797 val_loss: 0.034335475\n",
      "Step = 94 train_loss: 0.03276023 val_loss: 0.03439442\n",
      "Step = 95 train_loss: 0.033271857 val_loss: 0.034392018\n",
      "Step = 96 train_loss: 0.03386245 val_loss: 0.034352057\n",
      "Step = 97 train_loss: 0.031842433 val_loss: 0.034127045\n",
      "Step = 98 train_loss: 0.030558543 val_loss: 0.03396361\n",
      "Step = 99 train_loss: 0.028528512 val_loss: 0.033790663\n",
      "Step = 100 train_loss: 0.027227897 val_loss: 0.0339726\n",
      "Step = 101 train_loss: 0.03061709 val_loss: 0.03419461\n",
      "Step = 102 train_loss: 0.029476963 val_loss: 0.034401447\n",
      "Step = 103 train_loss: 0.027412457 val_loss: 0.034561746\n",
      "Step = 104 train_loss: 0.026244882 val_loss: 0.034658514\n",
      "Step = 105 train_loss: 0.027107712 val_loss: 0.034805223\n",
      "Step = 106 train_loss: 0.026911559 val_loss: 0.03488467\n",
      "Step = 107 train_loss: 0.03325437 val_loss: 0.03496216\n",
      "Step = 108 train_loss: 0.029921617 val_loss: 0.03501115\n",
      "Step = 109 train_loss: 0.03176979 val_loss: 0.03510386\n",
      "Step = 110 train_loss: 0.03316718 val_loss: 0.035101116\n",
      "Step = 111 train_loss: 0.035327457 val_loss: 0.035240803\n",
      "Step = 112 train_loss: 0.02304104 val_loss: 0.035496537\n",
      "Step = 113 train_loss: 0.02696811 val_loss: 0.035673346\n",
      "Step = 114 train_loss: 0.027256133 val_loss: 0.035533085\n",
      "Step = 115 train_loss: 0.031672426 val_loss: 0.035454728\n",
      "Step = 116 train_loss: 0.029272445 val_loss: 0.03538669\n",
      "Step = 117 train_loss: 0.029328417 val_loss: 0.03512778\n",
      "Step = 118 train_loss: 0.028878182 val_loss: 0.034944728\n",
      "Step = 119 train_loss: 0.030069806 val_loss: 0.03460155\n",
      "Step = 120 train_loss: 0.027909467 val_loss: 0.034452077\n",
      "Step = 121 train_loss: 0.029106319 val_loss: 0.03418111\n",
      "Step = 122 train_loss: 0.03102641 val_loss: 0.03403006\n",
      "Step = 123 train_loss: 0.026587818 val_loss: 0.033882163\n",
      "Step = 124 train_loss: 0.032758918 val_loss: 0.033639625\n",
      "Step = 125 train_loss: 0.031182297 val_loss: 0.033566166\n",
      "Step = 126 train_loss: 0.03313533 val_loss: 0.033586223\n",
      "Step = 127 train_loss: 0.029157849 val_loss: 0.033603076\n",
      "Step = 128 train_loss: 0.0269005 val_loss: 0.03358274\n",
      "Step = 129 train_loss: 0.029225154 val_loss: 0.03365276\n",
      "Step = 130 train_loss: 0.030029444 val_loss: 0.033619456\n",
      "Step = 131 train_loss: 0.026479963 val_loss: 0.033543754\n",
      "Step = 132 train_loss: 0.028853742 val_loss: 0.03344561\n",
      "Step = 133 train_loss: 0.028870134 val_loss: 0.033386845\n",
      "Step = 134 train_loss: 0.02828585 val_loss: 0.033247013\n",
      "Step = 135 train_loss: 0.031595048 val_loss: 0.033053983\n",
      "Step = 136 train_loss: 0.030804621 val_loss: 0.032695014\n",
      "Step = 137 train_loss: 0.030221747 val_loss: 0.032339547\n",
      "Step = 138 train_loss: 0.035185453 val_loss: 0.03190739\n",
      "Step = 139 train_loss: 0.031385202 val_loss: 0.032059316\n",
      "Step = 140 train_loss: 0.028315574 val_loss: 0.032528773\n",
      "Step = 141 train_loss: 0.03202054 val_loss: 0.032662824\n",
      "Step = 142 train_loss: 0.026929023 val_loss: 0.034402974\n",
      "Step = 143 train_loss: 0.028318992 val_loss: 0.033714816\n",
      "Step = 144 train_loss: 0.028298618 val_loss: 0.025935967\n",
      "Step = 145 train_loss: 0.029941026 val_loss: 0.021291057\n",
      "Step = 146 train_loss: 0.03168607 val_loss: 0.020214604\n",
      "Step = 147 train_loss: 0.029521473 val_loss: 0.021411542\n",
      "Step = 148 train_loss: 0.03141698 val_loss: 0.022415023\n",
      "Step = 149 train_loss: 0.025974333 val_loss: 0.022277437\n",
      "Step = 150 train_loss: 0.034621324 val_loss: 0.021593975\n",
      "Step = 151 train_loss: 0.03185251 val_loss: 0.020881968\n",
      "Step = 152 train_loss: 0.026320849 val_loss: 0.020643163\n",
      "Step = 153 train_loss: 0.034106664 val_loss: 0.021091072\n",
      "Step = 154 train_loss: 0.027582942 val_loss: 0.022417199\n",
      "Step = 155 train_loss: 0.028562622 val_loss: 0.024169687\n",
      "Step = 156 train_loss: 0.033861652 val_loss: 0.02590041\n",
      "Step = 157 train_loss: 0.030565271 val_loss: 0.027335621\n",
      "Step = 158 train_loss: 0.025704714 val_loss: 0.028612431\n",
      "Step = 159 train_loss: 0.030969836 val_loss: 0.029273612\n",
      "Step = 160 train_loss: 0.03211612 val_loss: 0.029724529\n",
      "Step = 161 train_loss: 0.031794265 val_loss: 0.030025724\n",
      "Step = 162 train_loss: 0.028855424 val_loss: 0.030584993\n",
      "Step = 163 train_loss: 0.029339047 val_loss: 0.031353295\n",
      "Step = 164 train_loss: 0.03234257 val_loss: 0.03215994\n",
      "Step = 165 train_loss: 0.026485465 val_loss: 0.032700814\n",
      "Step = 166 train_loss: 0.02790733 val_loss: 0.033300858\n",
      "Step = 167 train_loss: 0.027845256 val_loss: 0.03350155\n",
      "Step = 168 train_loss: 0.028228976 val_loss: 0.03318365\n",
      "Step = 169 train_loss: 0.03309673 val_loss: 0.033002615\n",
      "Step = 170 train_loss: 0.028905401 val_loss: 0.03304532\n",
      "Step = 171 train_loss: 0.028492738 val_loss: 0.033396766\n",
      "Step = 172 train_loss: 0.028779788 val_loss: 0.03356091\n",
      "Step = 173 train_loss: 0.02959704 val_loss: 0.032869637\n",
      "Step = 174 train_loss: 0.03330696 val_loss: 0.028860025\n",
      "Step = 175 train_loss: 0.028820904 val_loss: 0.03299266\n",
      "Step = 176 train_loss: 0.030348817 val_loss: 0.033148058\n",
      "Step = 177 train_loss: 0.028605204 val_loss: 0.031724665\n",
      "Step = 178 train_loss: 0.031066379 val_loss: 0.033351164\n",
      "Step = 179 train_loss: 0.028221518 val_loss: 0.03355113\n",
      "Step = 180 train_loss: 0.026348697 val_loss: 0.03319094\n",
      "Step = 181 train_loss: 0.025809772 val_loss: 0.034597535\n",
      "Step = 182 train_loss: 0.025394235 val_loss: 0.035497043\n",
      "Step = 183 train_loss: 0.028542168 val_loss: 0.03552306\n",
      "Step = 184 train_loss: 0.023792531 val_loss: 0.034772288\n",
      "Step = 185 train_loss: 0.031885587 val_loss: 0.034239896\n",
      "Step = 186 train_loss: 0.028926888 val_loss: 0.03180357\n",
      "Step = 187 train_loss: 0.025935235 val_loss: 0.03001441\n",
      "Step = 188 train_loss: 0.028921435 val_loss: 0.028410662\n",
      "Step = 189 train_loss: 0.030829022 val_loss: 0.027346887\n",
      "Step = 190 train_loss: 0.029851634 val_loss: 0.026951613\n",
      "Step = 191 train_loss: 0.029931724 val_loss: 0.027036875\n",
      "Step = 192 train_loss: 0.031156544 val_loss: 0.027876444\n",
      "Step = 193 train_loss: 0.033021543 val_loss: 0.028809402\n",
      "Step = 194 train_loss: 0.036004957 val_loss: 0.029837614\n",
      "Step = 195 train_loss: 0.030962845 val_loss: 0.031069348\n",
      "Step = 196 train_loss: 0.031616606 val_loss: 0.032192852\n",
      "Step = 197 train_loss: 0.02933273 val_loss: 0.03294254\n",
      "Step = 198 train_loss: 0.029933853 val_loss: 0.033742886\n",
      "Step = 199 train_loss: 0.027751919 val_loss: 0.03437089\n",
      "Step = 200 train_loss: 0.022592297 val_loss: 0.034558754\n",
      "Step = 201 train_loss: 0.028077994 val_loss: 0.034387488\n",
      "Step = 202 train_loss: 0.027190626 val_loss: 0.034250606\n",
      "Step = 203 train_loss: 0.027319508 val_loss: 0.033840302\n",
      "Step = 204 train_loss: 0.030274762 val_loss: 0.03362371\n",
      "Step = 205 train_loss: 0.029695738 val_loss: 0.033584177\n",
      "Step = 206 train_loss: 0.03140632 val_loss: 0.03364014\n",
      "Step = 207 train_loss: 0.027377227 val_loss: 0.033787515\n",
      "Step = 208 train_loss: 0.029837083 val_loss: 0.03396312\n",
      "Step = 209 train_loss: 0.03144525 val_loss: 0.034173187\n",
      "Step = 210 train_loss: 0.031215291 val_loss: 0.03435301\n",
      "Step = 211 train_loss: 0.030419167 val_loss: 0.03457457\n",
      "Step = 212 train_loss: 0.026410518 val_loss: 0.034601543\n",
      "Step = 213 train_loss: 0.029524244 val_loss: 0.034485172\n",
      "Step = 214 train_loss: 0.026725259 val_loss: 0.034388665\n",
      "Step = 215 train_loss: 0.02693058 val_loss: 0.03417699\n",
      "Step = 216 train_loss: 0.031323936 val_loss: 0.034007356\n",
      "Step = 217 train_loss: 0.03372091 val_loss: 0.03381277\n",
      "Step = 218 train_loss: 0.031326775 val_loss: 0.0336945\n",
      "Step = 219 train_loss: 0.028690862 val_loss: 0.033706486\n",
      "Step = 220 train_loss: 0.027309647 val_loss: 0.033656474\n",
      "Step = 221 train_loss: 0.031976465 val_loss: 0.033556163\n",
      "Step = 222 train_loss: 0.02646399 val_loss: 0.03344667\n",
      "Step = 223 train_loss: 0.029995006 val_loss: 0.03340613\n",
      "Step = 224 train_loss: 0.03024398 val_loss: 0.03325881\n",
      "Step = 225 train_loss: 0.029641181 val_loss: 0.033142567\n",
      "Step = 226 train_loss: 0.031293675 val_loss: 0.033092167\n",
      "Step = 227 train_loss: 0.0322371 val_loss: 0.032986023\n",
      "Step = 228 train_loss: 0.03220723 val_loss: 0.032878697\n",
      "Step = 229 train_loss: 0.02886082 val_loss: 0.0326963\n",
      "Step = 230 train_loss: 0.028717257 val_loss: 0.032528386\n",
      "Step = 231 train_loss: 0.029714782 val_loss: 0.03232387\n",
      "Step = 232 train_loss: 0.028860228 val_loss: 0.03215983\n",
      "Step = 233 train_loss: 0.027996717 val_loss: 0.032103095\n",
      "Step = 234 train_loss: 0.02904103 val_loss: 0.032114647\n",
      "Step = 235 train_loss: 0.027403768 val_loss: 0.03197922\n",
      "Step = 236 train_loss: 0.029747922 val_loss: 0.031998612\n",
      "Step = 237 train_loss: 0.029342541 val_loss: 0.03199584\n",
      "Step = 238 train_loss: 0.026781457 val_loss: 0.031947196\n",
      "Step = 239 train_loss: 0.029500723 val_loss: 0.031950198\n",
      "Step = 240 train_loss: 0.030661361 val_loss: 0.032027934\n",
      "Step = 241 train_loss: 0.029030986 val_loss: 0.032015238\n",
      "Step = 242 train_loss: 0.030146066 val_loss: 0.032089822\n",
      "Step = 243 train_loss: 0.029016932 val_loss: 0.032349844\n",
      "Step = 244 train_loss: 0.029952463 val_loss: 0.032519918\n",
      "Step = 245 train_loss: 0.029042725 val_loss: 0.032463796\n",
      "Step = 246 train_loss: 0.026258994 val_loss: 0.032533348\n",
      "Step = 247 train_loss: 0.027070146 val_loss: 0.032765307\n",
      "Step = 248 train_loss: 0.030274494 val_loss: 0.033120558\n",
      "Step = 249 train_loss: 0.027118491 val_loss: 0.03386675\n",
      "Step = 250 train_loss: 0.029566256 val_loss: 0.034562465\n",
      "Step = 251 train_loss: 0.030288864 val_loss: 0.034995336\n",
      "Step = 252 train_loss: 0.028396394 val_loss: 0.035189435\n",
      "Step = 253 train_loss: 0.025464961 val_loss: 0.035276923\n",
      "Step = 254 train_loss: 0.0276226 val_loss: 0.035317466\n",
      "Step = 255 train_loss: 0.03145496 val_loss: 0.034985058\n",
      "Step = 256 train_loss: 0.026248923 val_loss: 0.034561265\n",
      "Step = 257 train_loss: 0.031027285 val_loss: 0.034207925\n",
      "Step = 258 train_loss: 0.025948707 val_loss: 0.034035154\n",
      "Step = 259 train_loss: 0.028525041 val_loss: 0.033892468\n",
      "Step = 260 train_loss: 0.023996087 val_loss: 0.033881538\n",
      "Step = 261 train_loss: 0.02835795 val_loss: 0.033883184\n",
      "Step = 262 train_loss: 0.02913932 val_loss: 0.033832703\n",
      "Step = 263 train_loss: 0.023021039 val_loss: 0.033710707\n",
      "Step = 264 train_loss: 0.027170938 val_loss: 0.032893963\n",
      "Step = 265 train_loss: 0.02589698 val_loss: 0.03245612\n",
      "Step = 266 train_loss: 0.025239239 val_loss: 0.033053767\n",
      "Step = 267 train_loss: 0.022627901 val_loss: 0.03340143\n",
      "Step = 268 train_loss: 0.0246269 val_loss: 0.033864833\n",
      "Step = 269 train_loss: 0.025278348 val_loss: 0.033944063\n",
      "Step = 270 train_loss: 0.02562394 val_loss: 0.032240294\n",
      "Step = 271 train_loss: 0.023100732 val_loss: 0.030765388\n",
      "Step = 272 train_loss: 0.031523876 val_loss: 0.03507698\n",
      "Step = 273 train_loss: 0.022614025 val_loss: 0.03611381\n",
      "Step = 274 train_loss: 0.027962666 val_loss: 0.03686108\n",
      "Step = 275 train_loss: 0.025237093 val_loss: 0.036784843\n",
      "Step = 276 train_loss: 0.024620079 val_loss: 0.034480102\n",
      "Step = 277 train_loss: 0.024824474 val_loss: 0.027397845\n",
      "Step = 278 train_loss: 0.027263442 val_loss: 0.026340624\n",
      "Step = 279 train_loss: 0.029594826 val_loss: 0.027775101\n",
      "Step = 280 train_loss: 0.030278774 val_loss: 0.03467502\n",
      "Step = 281 train_loss: 0.022940181 val_loss: 0.037717428\n",
      "Step = 282 train_loss: 0.024565268 val_loss: 0.03871567\n",
      "Step = 283 train_loss: 0.026231501 val_loss: 0.038686246\n",
      "Step = 284 train_loss: 0.025911033 val_loss: 0.038154423\n",
      "Step = 285 train_loss: 0.02599319 val_loss: 0.0373525\n",
      "Step = 286 train_loss: 0.02656496 val_loss: 0.032476876\n",
      "Step = 287 train_loss: 0.027305685 val_loss: 0.026879288\n",
      "Step = 288 train_loss: 0.023451477 val_loss: 0.026474658\n",
      "Step = 289 train_loss: 0.02300375 val_loss: 0.025817841\n",
      "Step = 290 train_loss: 0.026393175 val_loss: 0.026010253\n",
      "Step = 291 train_loss: 0.02286806 val_loss: 0.027763024\n",
      "Step = 292 train_loss: 0.025079088 val_loss: 0.03687113\n",
      "Step = 293 train_loss: 0.024813486 val_loss: 0.038411304\n",
      "Step = 294 train_loss: 0.023793552 val_loss: 0.038281772\n",
      "Step = 295 train_loss: 0.0231748 val_loss: 0.037441354\n",
      "Step = 296 train_loss: 0.024246957 val_loss: 0.033203144\n",
      "Step = 297 train_loss: 0.026945032 val_loss: 0.034292795\n",
      "Step = 298 train_loss: 0.024002127 val_loss: 0.029827278\n",
      "Step = 299 train_loss: 0.022949338 val_loss: 0.025984364\n",
      "Step = 300 train_loss: 0.021626104 val_loss: 0.028797464\n",
      "Step = 301 train_loss: 0.025271006 val_loss: 0.038238417\n",
      "Step = 302 train_loss: 0.023307888 val_loss: 0.052148674\n",
      "Step = 303 train_loss: 0.024085911 val_loss: 0.05109573\n",
      "Step = 304 train_loss: 0.02418066 val_loss: 0.05144531\n",
      "Step = 305 train_loss: 0.022912595 val_loss: 0.052412905\n",
      "Step = 306 train_loss: 0.02385189 val_loss: 0.047053438\n",
      "Step = 307 train_loss: 0.0242455 val_loss: 0.03567832\n",
      "Step = 308 train_loss: 0.024431057 val_loss: 0.031577908\n",
      "Step = 309 train_loss: 0.023448769 val_loss: 0.030598193\n",
      "Step = 310 train_loss: 0.024138931 val_loss: 0.03334772\n",
      "Step = 311 train_loss: 0.024243431 val_loss: 0.03800595\n",
      "Step = 312 train_loss: 0.020051388 val_loss: 0.047531348\n",
      "Step = 313 train_loss: 0.020742638 val_loss: 0.046717614\n",
      "Step = 314 train_loss: 0.01899336 val_loss: 0.047385078\n",
      "Step = 315 train_loss: 0.020914713 val_loss: 0.04684893\n",
      "Step = 316 train_loss: 0.022620637 val_loss: 0.04593896\n",
      "Step = 317 train_loss: 0.023263555 val_loss: 0.046668872\n",
      "Step = 318 train_loss: 0.022474196 val_loss: 0.046137884\n",
      "Step = 319 train_loss: 0.023496829 val_loss: 0.03894049\n",
      "Step = 320 train_loss: 0.022370132 val_loss: 0.02875666\n",
      "Step = 321 train_loss: 0.02102447 val_loss: 0.027398648\n",
      "Step = 322 train_loss: 0.024893114 val_loss: 0.027737401\n",
      "Step = 323 train_loss: 0.024151893 val_loss: 0.035815388\n",
      "Step = 324 train_loss: 0.020598583 val_loss: 0.042272504\n",
      "Step = 325 train_loss: 0.025194732 val_loss: 0.0427134\n",
      "Step = 326 train_loss: 0.019889314 val_loss: 0.045419473\n",
      "Step = 327 train_loss: 0.023192057 val_loss: 0.046534423\n",
      "Step = 328 train_loss: 0.023190565 val_loss: 0.029405942\n",
      "Step = 329 train_loss: 0.02244902 val_loss: 0.017108286\n",
      "Step = 330 train_loss: 0.01737043 val_loss: 0.01649455\n",
      "Step = 331 train_loss: 0.022525044 val_loss: 0.016771639\n",
      "Step = 332 train_loss: 0.020472685 val_loss: 0.016908308\n",
      "Step = 333 train_loss: 0.024160353 val_loss: 0.017325982\n",
      "Step = 334 train_loss: 0.020427989 val_loss: 0.02903536\n",
      "Step = 335 train_loss: 0.025756396 val_loss: 0.04467668\n",
      "Step = 336 train_loss: 0.019388266 val_loss: 0.047649857\n",
      "Step = 337 train_loss: 0.021228956 val_loss: 0.047439482\n",
      "Step = 338 train_loss: 0.023286134 val_loss: 0.040508673\n",
      "Step = 339 train_loss: 0.023490192 val_loss: 0.026509678\n",
      "Step = 340 train_loss: 0.021797366 val_loss: 0.01991398\n",
      "Step = 341 train_loss: 0.024850545 val_loss: 0.02231828\n",
      "Step = 342 train_loss: 0.018966774 val_loss: 0.03696309\n",
      "Step = 343 train_loss: 0.020780133 val_loss: 0.050128605\n",
      "Step = 344 train_loss: 0.02062672 val_loss: 0.05670883\n",
      "Step = 345 train_loss: 0.02208502 val_loss: 0.058106083\n",
      "Step = 346 train_loss: 0.020613093 val_loss: 0.048541717\n",
      "Step = 347 train_loss: 0.020278292 val_loss: 0.023159655\n",
      "Step = 348 train_loss: 0.019609382 val_loss: 0.017493041\n",
      "Step = 349 train_loss: 0.021377504 val_loss: 0.018140431\n",
      "Step = 350 train_loss: 0.028065924 val_loss: 0.021305505\n",
      "Step = 351 train_loss: 0.021021428 val_loss: 0.0533133\n",
      "Step = 352 train_loss: 0.020244326 val_loss: 0.061199218\n",
      "Step = 353 train_loss: 0.020926714 val_loss: 0.06143536\n",
      "Step = 354 train_loss: 0.023844091 val_loss: 0.060628008\n",
      "Step = 355 train_loss: 0.021273576 val_loss: 0.0585722\n",
      "Step = 356 train_loss: 0.025786828 val_loss: 0.05544741\n",
      "Step = 357 train_loss: 0.020878922 val_loss: 0.03801373\n",
      "Step = 358 train_loss: 0.022747941 val_loss: 0.017555367\n",
      "Step = 359 train_loss: 0.023895651 val_loss: 0.017096348\n",
      "Step = 360 train_loss: 0.02289034 val_loss: 0.017149944\n",
      "Step = 361 train_loss: 0.02373314 val_loss: 0.019393852\n",
      "Step = 362 train_loss: 0.022068623 val_loss: 0.04902423\n",
      "Step = 363 train_loss: 0.022067599 val_loss: 0.054654006\n",
      "Step = 364 train_loss: 0.023548033 val_loss: 0.05466142\n",
      "Step = 365 train_loss: 0.021945003 val_loss: 0.052586086\n",
      "Step = 366 train_loss: 0.02031245 val_loss: 0.04936117\n",
      "Step = 367 train_loss: 0.02306952 val_loss: 0.03781805\n",
      "Step = 368 train_loss: 0.020823756 val_loss: 0.019452296\n",
      "Step = 369 train_loss: 0.023014963 val_loss: 0.016403625\n",
      "Step = 370 train_loss: 0.019521961 val_loss: 0.01615701\n",
      "Step = 371 train_loss: 0.020706683 val_loss: 0.016946284\n",
      "Step = 372 train_loss: 0.018885544 val_loss: 0.021674523\n",
      "Step = 373 train_loss: 0.021781337 val_loss: 0.030958913\n",
      "Step = 374 train_loss: 0.01701944 val_loss: 0.03597223\n",
      "Step = 375 train_loss: 0.016241426 val_loss: 0.02653535\n",
      "Step = 376 train_loss: 0.022507686 val_loss: 0.018942498\n",
      "Step = 377 train_loss: 0.02037237 val_loss: 0.016896376\n",
      "Step = 378 train_loss: 0.01774496 val_loss: 0.016972298\n",
      "Step = 379 train_loss: 0.016120141 val_loss: 0.018915223\n",
      "Step = 380 train_loss: 0.01647063 val_loss: 0.02856819\n",
      "Step = 381 train_loss: 0.017711889 val_loss: 0.045290906\n",
      "Step = 382 train_loss: 0.01622522 val_loss: 0.052867156\n",
      "Step = 383 train_loss: 0.01805452 val_loss: 0.048080076\n",
      "Step = 384 train_loss: 0.018165322 val_loss: 0.030747455\n",
      "Step = 385 train_loss: 0.017733708 val_loss: 0.019842764\n",
      "Step = 386 train_loss: 0.018565543 val_loss: 0.017418694\n",
      "Step = 387 train_loss: 0.016576339 val_loss: 0.016664317\n",
      "Step = 388 train_loss: 0.018657306 val_loss: 0.018046852\n",
      "Step = 389 train_loss: 0.015812553 val_loss: 0.017643813\n",
      "Step = 390 train_loss: 0.015322983 val_loss: 0.022556866\n",
      "Step = 391 train_loss: 0.019401468 val_loss: 0.031800143\n",
      "Step = 392 train_loss: 0.017648175 val_loss: 0.036797326\n",
      "Step = 393 train_loss: 0.01636388 val_loss: 0.024406988\n",
      "Step = 394 train_loss: 0.019504089 val_loss: 0.016510477\n",
      "Step = 395 train_loss: 0.017518975 val_loss: 0.015168737\n",
      "Step = 396 train_loss: 0.016650634 val_loss: 0.014957326\n",
      "Step = 397 train_loss: 0.016544286 val_loss: 0.014842616\n",
      "Step = 398 train_loss: 0.021028964 val_loss: 0.016326368\n",
      "Step = 399 train_loss: 0.01752715 val_loss: 0.018919041\n",
      "Step = 400 train_loss: 0.01729323 val_loss: 0.036363225\n",
      "Step = 401 train_loss: 0.018516602 val_loss: 0.052939393\n",
      "Step = 402 train_loss: 0.017744334 val_loss: 0.05024407\n",
      "Step = 403 train_loss: 0.018594285 val_loss: 0.044117227\n",
      "Step = 404 train_loss: 0.017344456 val_loss: 0.0344445\n",
      "Step = 405 train_loss: 0.016607847 val_loss: 0.027307501\n",
      "Step = 406 train_loss: 0.017430563 val_loss: 0.022623124\n",
      "Step = 407 train_loss: 0.017167317 val_loss: 0.016078042\n",
      "Step = 408 train_loss: 0.018965775 val_loss: 0.01650991\n",
      "Step = 409 train_loss: 0.016033757 val_loss: 0.019796329\n",
      "Step = 410 train_loss: 0.015959512 val_loss: 0.028769562\n",
      "Step = 411 train_loss: 0.015665513 val_loss: 0.029047064\n",
      "Step = 412 train_loss: 0.015415843 val_loss: 0.023338422\n",
      "Step = 413 train_loss: 0.01980988 val_loss: 0.016742446\n",
      "Step = 414 train_loss: 0.015296776 val_loss: 0.01690983\n",
      "Step = 415 train_loss: 0.017119877 val_loss: 0.023303892\n",
      "Step = 416 train_loss: 0.01530472 val_loss: 0.04663073\n",
      "Step = 417 train_loss: 0.017248241 val_loss: 0.046799567\n",
      "Step = 418 train_loss: 0.01935214 val_loss: 0.038185496\n",
      "Step = 419 train_loss: 0.017161477 val_loss: 0.021424118\n",
      "Step = 420 train_loss: 0.013478236 val_loss: 0.014999818\n",
      "Step = 421 train_loss: 0.017407103 val_loss: 0.013466853\n",
      "Step = 422 train_loss: 0.017306 val_loss: 0.014115822\n",
      "Step = 423 train_loss: 0.0154779395 val_loss: 0.013741416\n",
      "Step = 424 train_loss: 0.017952705 val_loss: 0.017136315\n",
      "Step = 425 train_loss: 0.01577561 val_loss: 0.03187722\n",
      "Step = 426 train_loss: 0.016543075 val_loss: 0.04887367\n",
      "Step = 427 train_loss: 0.015610929 val_loss: 0.04416428\n",
      "Step = 428 train_loss: 0.016035672 val_loss: 0.020669162\n",
      "Step = 429 train_loss: 0.013447936 val_loss: 0.013342084\n",
      "Step = 430 train_loss: 0.016491871 val_loss: 0.014181668\n",
      "Step = 431 train_loss: 0.016394803 val_loss: 0.026354335\n",
      "Step = 432 train_loss: 0.015701707 val_loss: 0.021303114\n",
      "Step = 433 train_loss: 0.011345247 val_loss: 0.029454751\n",
      "Step = 434 train_loss: 0.009622769 val_loss: 0.03505382\n",
      "Step = 435 train_loss: 0.015898399 val_loss: 0.033169303\n",
      "Step = 436 train_loss: 0.017543206 val_loss: 0.052248117\n",
      "Step = 437 train_loss: 0.014808375 val_loss: 0.05026096\n",
      "Step = 438 train_loss: 0.014715507 val_loss: 0.040621463\n",
      "Step = 439 train_loss: 0.016272854 val_loss: 0.014948402\n",
      "Step = 440 train_loss: 0.016119748 val_loss: 0.0098660095\n",
      "Step = 441 train_loss: 0.0146540385 val_loss: 0.011207921\n",
      "Step = 442 train_loss: 0.014982165 val_loss: 0.010049831\n",
      "Step = 443 train_loss: 0.013099268 val_loss: 0.008857798\n",
      "Step = 444 train_loss: 0.013648065 val_loss: 0.028422268\n",
      "Step = 445 train_loss: 0.011053742 val_loss: 0.041377272\n",
      "Step = 446 train_loss: 0.010944022 val_loss: 0.031113403\n",
      "Step = 447 train_loss: 0.017038604 val_loss: 0.0114675015\n",
      "Step = 448 train_loss: 0.013754219 val_loss: 0.010360696\n",
      "Step = 449 train_loss: 0.01385953 val_loss: 0.0129929595\n",
      "Step = 450 train_loss: 0.0182175 val_loss: 0.012069536\n",
      "Step = 451 train_loss: 0.019466758 val_loss: 0.013868146\n",
      "Step = 452 train_loss: 0.013485657 val_loss: 0.059818983\n",
      "Step = 453 train_loss: 0.017260386 val_loss: 0.064537644\n",
      "Step = 454 train_loss: 0.015736185 val_loss: 0.042373292\n",
      "Step = 455 train_loss: 0.013810673 val_loss: 0.009793349\n",
      "Step = 456 train_loss: 0.012852143 val_loss: 0.015569585\n",
      "Step = 457 train_loss: 0.011351482 val_loss: 0.013867428\n",
      "Step = 458 train_loss: 0.016993906 val_loss: 0.021565897\n",
      "Step = 459 train_loss: 0.013771687 val_loss: 0.07188001\n",
      "Step = 460 train_loss: 0.013686137 val_loss: 0.073443316\n",
      "Step = 461 train_loss: 0.01755736 val_loss: 0.07009426\n",
      "Step = 462 train_loss: 0.013790042 val_loss: 0.059597284\n",
      "Step = 463 train_loss: 0.015502722 val_loss: 0.028497716\n",
      "Step = 464 train_loss: 0.015348215 val_loss: 0.01029279\n",
      "Step = 465 train_loss: 0.01286749 val_loss: 0.017464768\n",
      "Step = 466 train_loss: 0.011239082 val_loss: 0.01660385\n",
      "Step = 467 train_loss: 0.012395818 val_loss: 0.010713089\n",
      "Step = 468 train_loss: 0.012275546 val_loss: 0.029401701\n",
      "Step = 469 train_loss: 0.013501798 val_loss: 0.08382779\n",
      "Step = 470 train_loss: 0.016694365 val_loss: 0.10556991\n",
      "Step = 471 train_loss: 0.016541619 val_loss: 0.108698726\n",
      "Step = 472 train_loss: 0.017103922 val_loss: 0.104614116\n",
      "Step = 473 train_loss: 0.015684938 val_loss: 0.09383193\n",
      "Step = 474 train_loss: 0.012216485 val_loss: 0.07454302\n",
      "Step = 475 train_loss: 0.016089763 val_loss: 0.014492407\n",
      "Step = 476 train_loss: 0.021262404 val_loss: 0.012131338\n",
      "Step = 477 train_loss: 0.010851524 val_loss: 0.016667517\n",
      "Step = 478 train_loss: 0.019607749 val_loss: 0.015415683\n",
      "Step = 479 train_loss: 0.020632386 val_loss: 0.015242304\n",
      "Step = 480 train_loss: 0.014977162 val_loss: 0.07205887\n",
      "Step = 481 train_loss: 0.0141666075 val_loss: 0.082671605\n",
      "Step = 482 train_loss: 0.019166816 val_loss: 0.08313861\n",
      "Step = 483 train_loss: 0.019509284 val_loss: 0.08355915\n",
      "Step = 484 train_loss: 0.022094151 val_loss: 0.07747579\n",
      "Step = 485 train_loss: 0.01661121 val_loss: 0.07072453\n",
      "Step = 486 train_loss: 0.016570292 val_loss: 0.057538632\n",
      "Step = 487 train_loss: 0.020518688 val_loss: 0.008349983\n",
      "Step = 488 train_loss: 0.016069593 val_loss: 0.02365025\n",
      "Step = 489 train_loss: 0.011556863 val_loss: 0.020199785\n",
      "Step = 490 train_loss: 0.016716186 val_loss: 0.018438218\n",
      "Step = 491 train_loss: 0.016650997 val_loss: 0.02034367\n",
      "Step = 492 train_loss: 0.018904978 val_loss: 0.008927104\n",
      "Step = 493 train_loss: 0.014059788 val_loss: 0.066152915\n",
      "Step = 494 train_loss: 0.01340627 val_loss: 0.09277083\n",
      "Step = 495 train_loss: 0.014924776 val_loss: 0.09397588\n",
      "Step = 496 train_loss: 0.01621513 val_loss: 0.08919201\n",
      "Step = 497 train_loss: 0.019651638 val_loss: 0.07186803\n",
      "Step = 498 train_loss: 0.014306449 val_loss: 0.028484056\n",
      "Step = 499 train_loss: 0.01380243 val_loss: 0.009375245\n",
      "Step = 500 train_loss: 0.013209649 val_loss: 0.019517154\n",
      "Step = 501 train_loss: 0.011988067 val_loss: 0.017660268\n",
      "Step = 502 train_loss: 0.014006784 val_loss: 0.009649021\n",
      "Step = 503 train_loss: 0.012216404 val_loss: 0.010038453\n",
      "Step = 504 train_loss: 0.010646931 val_loss: 0.008017467\n",
      "Step = 505 train_loss: 0.012061453 val_loss: 0.009980015\n",
      "Step = 506 train_loss: 0.01283052 val_loss: 0.009969032\n",
      "Step = 507 train_loss: 0.012755738 val_loss: 0.010590921\n",
      "Step = 508 train_loss: 0.010046287 val_loss: 0.01195805\n",
      "Step = 509 train_loss: 0.012050724 val_loss: 0.015527342\n",
      "Step = 510 train_loss: 0.011286799 val_loss: 0.0116106365\n",
      "Step = 511 train_loss: 0.013191983 val_loss: 0.008199977\n",
      "Step = 512 train_loss: 0.016272802 val_loss: 0.007836545\n",
      "Step = 513 train_loss: 0.008682232 val_loss: 0.0072506494\n",
      "Step = 514 train_loss: 0.013363848 val_loss: 0.011631884\n",
      "Step = 515 train_loss: 0.012565662 val_loss: 0.015045086\n",
      "Step = 516 train_loss: 0.013408263 val_loss: 0.0069482997\n",
      "Step = 517 train_loss: 0.0129041225 val_loss: 0.019227685\n",
      "Step = 518 train_loss: 0.012957215 val_loss: 0.015048338\n",
      "Step = 519 train_loss: 0.012780106 val_loss: 0.008553906\n",
      "Step = 520 train_loss: 0.010041758 val_loss: 0.008296948\n",
      "Step = 521 train_loss: 0.009963727 val_loss: 0.029248487\n",
      "Step = 522 train_loss: 0.011018754 val_loss: 0.031180955\n",
      "Step = 523 train_loss: 0.008813736 val_loss: 0.01549256\n",
      "Step = 524 train_loss: 0.010921673 val_loss: 0.009273567\n",
      "Step = 525 train_loss: 0.013768951 val_loss: 0.013053108\n",
      "Step = 526 train_loss: 0.009159331 val_loss: 0.041974645\n",
      "Step = 527 train_loss: 0.014663345 val_loss: 0.061067905\n",
      "Step = 528 train_loss: 0.0092078205 val_loss: 0.049927335\n",
      "Step = 529 train_loss: 0.011263185 val_loss: 0.03050246\n",
      "Step = 530 train_loss: 0.0109270755 val_loss: 0.008509761\n",
      "Step = 531 train_loss: 0.009319668 val_loss: 0.012125384\n",
      "Step = 532 train_loss: 0.014083061 val_loss: 0.007628033\n",
      "Step = 533 train_loss: 0.0087996805 val_loss: 0.023625834\n",
      "Step = 534 train_loss: 0.01274229 val_loss: 0.054558083\n",
      "Step = 535 train_loss: 0.009973301 val_loss: 0.05494658\n",
      "Step = 536 train_loss: 0.012769516 val_loss: 0.031773705\n",
      "Step = 537 train_loss: 0.009950896 val_loss: 0.009841131\n",
      "Step = 538 train_loss: 0.010646776 val_loss: 0.012496332\n",
      "Step = 539 train_loss: 0.009598569 val_loss: 0.009812393\n",
      "Step = 540 train_loss: 0.011177199 val_loss: 0.010044709\n",
      "Step = 541 train_loss: 0.010682882 val_loss: 0.017141055\n",
      "Step = 542 train_loss: 0.008058013 val_loss: 0.009418194\n",
      "Step = 543 train_loss: 0.009622544 val_loss: 0.0090488605\n",
      "Step = 544 train_loss: 0.008522442 val_loss: 0.008704818\n",
      "Step = 545 train_loss: 0.01128856 val_loss: 0.008950303\n",
      "Step = 546 train_loss: 0.01100575 val_loss: 0.013521171\n",
      "Step = 547 train_loss: 0.007695988 val_loss: 0.021930283\n",
      "Step = 548 train_loss: 0.010739312 val_loss: 0.02067586\n",
      "Step = 549 train_loss: 0.013469806 val_loss: 0.0070241266\n",
      "Step = 550 train_loss: 0.010377703 val_loss: 0.0106793605\n",
      "Step = 551 train_loss: 0.011247643 val_loss: 0.01215351\n",
      "Step = 552 train_loss: 0.010808275 val_loss: 0.033487666\n",
      "Step = 553 train_loss: 0.007469004 val_loss: 0.064560406\n",
      "Step = 554 train_loss: 0.007562965 val_loss: 0.07194862\n",
      "Step = 555 train_loss: 0.008978565 val_loss: 0.07396807\n",
      "Step = 556 train_loss: 0.008647377 val_loss: 0.052379794\n",
      "Step = 557 train_loss: 0.0106657315 val_loss: 0.040378407\n",
      "Step = 558 train_loss: 0.007234169 val_loss: 0.012219574\n",
      "Step = 559 train_loss: 0.013112049 val_loss: 0.010008547\n",
      "Step = 560 train_loss: 0.008759492 val_loss: 0.010348433\n",
      "Step = 561 train_loss: 0.009718331 val_loss: 0.010219284\n",
      "Step = 562 train_loss: 0.012043288 val_loss: 0.014441788\n",
      "Step = 563 train_loss: 0.008879744 val_loss: 0.021672664\n",
      "Step = 564 train_loss: 0.008888461 val_loss: 0.04507968\n",
      "Step = 565 train_loss: 0.0094757 val_loss: 0.08648406\n",
      "Step = 566 train_loss: 0.009982533 val_loss: 0.10388735\n",
      "Step = 567 train_loss: 0.012195895 val_loss: 0.1069287\n",
      "Step = 568 train_loss: 0.010133986 val_loss: 0.13242951\n",
      "Step = 569 train_loss: 0.013624708 val_loss: 0.07049386\n",
      "Step = 570 train_loss: 0.010949423 val_loss: 0.058587335\n",
      "Step = 571 train_loss: 0.008271157 val_loss: 0.017805984\n",
      "Step = 572 train_loss: 0.00983783 val_loss: 0.01571382\n",
      "Step = 573 train_loss: 0.007116855 val_loss: 0.020803899\n",
      "Step = 574 train_loss: 0.010931177 val_loss: 0.017198749\n",
      "Step = 575 train_loss: 0.00999077 val_loss: 0.007733401\n",
      "Step = 576 train_loss: 0.008126106 val_loss: 0.020846667\n",
      "Step = 577 train_loss: 0.008462728 val_loss: 0.020164542\n",
      "Step = 578 train_loss: 0.011816899 val_loss: 0.0067991074\n",
      "Step = 579 train_loss: 0.008049082 val_loss: 0.016065309\n",
      "Step = 580 train_loss: 0.010006905 val_loss: 0.015883122\n",
      "Step = 581 train_loss: 0.00989224 val_loss: 0.011998242\n",
      "Step = 582 train_loss: 0.007881313 val_loss: 0.007860566\n",
      "Step = 583 train_loss: 0.013461375 val_loss: 0.062198315\n",
      "Step = 584 train_loss: 0.010970795 val_loss: 0.068279415\n",
      "Step = 585 train_loss: 0.009618604 val_loss: 0.032964446\n",
      "Step = 586 train_loss: 0.009543527 val_loss: 0.006223716\n",
      "Step = 587 train_loss: 0.0076665017 val_loss: 0.01866921\n",
      "Step = 588 train_loss: 0.009999743 val_loss: 0.021960407\n",
      "Step = 589 train_loss: 0.014274274 val_loss: 0.017876979\n",
      "Step = 590 train_loss: 0.011880338 val_loss: 0.011789259\n",
      "Step = 591 train_loss: 0.011951625 val_loss: 0.024781024\n",
      "Step = 592 train_loss: 0.009648566 val_loss: 0.05854473\n",
      "Step = 593 train_loss: 0.01043511 val_loss: 0.06480692\n",
      "Step = 594 train_loss: 0.008026221 val_loss: 0.05009354\n",
      "Step = 595 train_loss: 0.0077251648 val_loss: 0.011295011\n",
      "Step = 596 train_loss: 0.009238606 val_loss: 0.012422181\n",
      "Step = 597 train_loss: 0.00941609 val_loss: 0.010418937\n",
      "Step = 598 train_loss: 0.009649809 val_loss: 0.035135873\n",
      "Step = 599 train_loss: 0.007867606 val_loss: 0.04341792\n",
      "Step = 600 train_loss: 0.007993005 val_loss: 0.049445298\n",
      "Step = 601 train_loss: 0.011431898 val_loss: 0.0562072\n",
      "Step = 602 train_loss: 0.008136944 val_loss: 0.031331018\n",
      "Step = 603 train_loss: 0.008690391 val_loss: 0.015170323\n",
      "Step = 604 train_loss: 0.0082861 val_loss: 0.01681882\n",
      "Step = 605 train_loss: 0.0077899923 val_loss: 0.015097359\n",
      "Step = 606 train_loss: 0.012172907 val_loss: 0.006078782\n",
      "Step = 607 train_loss: 0.012469909 val_loss: 0.008468046\n",
      "Step = 608 train_loss: 0.007366555 val_loss: 0.006763602\n",
      "Step = 609 train_loss: 0.008347666 val_loss: 0.014965784\n",
      "Step = 610 train_loss: 0.010109475 val_loss: 0.014464391\n",
      "Step = 611 train_loss: 0.010769919 val_loss: 0.005153009\n",
      "Step = 612 train_loss: 0.010863935 val_loss: 0.008346961\n",
      "Step = 613 train_loss: 0.008411924 val_loss: 0.006307226\n",
      "Step = 614 train_loss: 0.009948492 val_loss: 0.01009747\n",
      "Step = 615 train_loss: 0.0061838604 val_loss: 0.020375611\n",
      "Step = 616 train_loss: 0.011319572 val_loss: 0.020653706\n",
      "Step = 617 train_loss: 0.0085568335 val_loss: 0.019607097\n",
      "Step = 618 train_loss: 0.008065982 val_loss: 0.012324159\n",
      "Step = 619 train_loss: 0.008209533 val_loss: 0.019627204\n",
      "Step = 620 train_loss: 0.0068040085 val_loss: 0.06485592\n",
      "Step = 621 train_loss: 0.008423402 val_loss: 0.072054975\n",
      "Step = 622 train_loss: 0.008292811 val_loss: 0.06433744\n",
      "Step = 623 train_loss: 0.007910705 val_loss: 0.044980265\n",
      "Step = 624 train_loss: 0.00969004 val_loss: 0.0062540728\n",
      "Step = 625 train_loss: 0.009064255 val_loss: 0.019433867\n",
      "Step = 626 train_loss: 0.009123272 val_loss: 0.023018384\n",
      "Step = 627 train_loss: 0.013206686 val_loss: 0.022485867\n",
      "Step = 628 train_loss: 0.010002231 val_loss: 0.019699803\n",
      "Step = 629 train_loss: 0.009261841 val_loss: 0.008912218\n",
      "Step = 630 train_loss: 0.0075439787 val_loss: 0.06599511\n",
      "Step = 631 train_loss: 0.007221805 val_loss: 0.08103168\n",
      "Step = 632 train_loss: 0.011952892 val_loss: 0.10313904\n",
      "Step = 633 train_loss: 0.010761479 val_loss: 0.081264645\n",
      "Step = 634 train_loss: 0.008330203 val_loss: 0.03878752\n",
      "Step = 635 train_loss: 0.009533736 val_loss: 0.030556263\n",
      "Step = 636 train_loss: 0.006360395 val_loss: 0.026155237\n",
      "Step = 637 train_loss: 0.0053602466 val_loss: 0.028602382\n",
      "Step = 638 train_loss: 0.006967143 val_loss: 0.03933323\n",
      "Step = 639 train_loss: 0.006107033 val_loss: 0.05692254\n",
      "Step = 640 train_loss: 0.008455426 val_loss: 0.06339649\n",
      "Step = 641 train_loss: 0.005680114 val_loss: 0.067763075\n",
      "Step = 642 train_loss: 0.0068358933 val_loss: 0.053325094\n",
      "Step = 643 train_loss: 0.008250459 val_loss: 0.042195316\n",
      "Step = 644 train_loss: 0.0088960705 val_loss: 0.04946233\n",
      "Step = 645 train_loss: 0.0073871263 val_loss: 0.049207572\n",
      "Step = 646 train_loss: 0.004486883 val_loss: 0.040003687\n",
      "Step = 647 train_loss: 0.008089092 val_loss: 0.032059908\n",
      "Step = 648 train_loss: 0.007497311 val_loss: 0.025835115\n",
      "Step = 649 train_loss: 0.0070947534 val_loss: 0.024253365\n",
      "Step = 650 train_loss: 0.0077812304 val_loss: 0.026388029\n",
      "Step = 651 train_loss: 0.010638003 val_loss: 0.036937416\n",
      "Step = 652 train_loss: 0.008835096 val_loss: 0.03765895\n",
      "Step = 653 train_loss: 0.006680035 val_loss: 0.0739738\n",
      "Step = 654 train_loss: 0.00518477 val_loss: 0.12684701\n",
      "Step = 655 train_loss: 0.009380481 val_loss: 0.17179316\n",
      "Step = 656 train_loss: 0.006899998 val_loss: 0.15500988\n",
      "Step = 657 train_loss: 0.008283625 val_loss: 0.08361561\n",
      "Step = 658 train_loss: 0.005564364 val_loss: 0.058850992\n",
      "Step = 659 train_loss: 0.008579215 val_loss: 0.08568357\n",
      "Step = 660 train_loss: 0.008966156 val_loss: 0.16863897\n",
      "Step = 661 train_loss: 0.009421656 val_loss: 0.16968384\n",
      "Step = 662 train_loss: 0.010248292 val_loss: 0.16861048\n",
      "Step = 663 train_loss: 0.0075868783 val_loss: 0.073023655\n",
      "Step = 664 train_loss: 0.0066069583 val_loss: 0.0054821204\n",
      "Step = 665 train_loss: 0.007974398 val_loss: 0.0065895407\n",
      "Step = 666 train_loss: 0.008654285 val_loss: 0.0056561655\n",
      "Step = 667 train_loss: 0.008018937 val_loss: 0.006581781\n",
      "Step = 668 train_loss: 0.007702001 val_loss: 0.039643362\n",
      "Step = 669 train_loss: 0.0074553182 val_loss: 0.1092933\n",
      "Step = 670 train_loss: 0.008628109 val_loss: 0.07497313\n",
      "Step = 671 train_loss: 0.0073668477 val_loss: 0.03886249\n",
      "Step = 672 train_loss: 0.007200151 val_loss: 0.0102242185\n",
      "Step = 673 train_loss: 0.008582581 val_loss: 0.006087338\n",
      "Step = 674 train_loss: 0.007129819 val_loss: 0.007355961\n",
      "Step = 675 train_loss: 0.008982601 val_loss: 0.005595948\n",
      "Step = 676 train_loss: 0.0056914855 val_loss: 0.016172491\n",
      "Step = 677 train_loss: 0.006402908 val_loss: 0.031023301\n",
      "Step = 678 train_loss: 0.008425764 val_loss: 0.01595842\n",
      "Step = 679 train_loss: 0.004420389 val_loss: 0.014294209\n",
      "Step = 680 train_loss: 0.0071819467 val_loss: 0.00876455\n",
      "Step = 681 train_loss: 0.010407848 val_loss: 0.01211633\n",
      "Step = 682 train_loss: 0.006652328 val_loss: 0.029318655\n",
      "Step = 683 train_loss: 0.005297539 val_loss: 0.01871899\n",
      "Step = 684 train_loss: 0.009251976 val_loss: 0.00504537\n",
      "Step = 685 train_loss: 0.011333421 val_loss: 0.017235575\n",
      "Step = 686 train_loss: 0.011562342 val_loss: 0.020858848\n",
      "Step = 687 train_loss: 0.008515442 val_loss: 0.02132024\n",
      "Step = 688 train_loss: 0.00828716 val_loss: 0.018269932\n",
      "Step = 689 train_loss: 0.00941955 val_loss: 0.006680862\n",
      "Step = 690 train_loss: 0.006514367 val_loss: 0.013400295\n",
      "Step = 691 train_loss: 0.008520851 val_loss: 0.046648644\n",
      "Step = 692 train_loss: 0.007843398 val_loss: 0.024061117\n",
      "Step = 693 train_loss: 0.010793019 val_loss: 0.02497488\n",
      "Step = 694 train_loss: 0.008329468 val_loss: 0.029643996\n",
      "Step = 695 train_loss: 0.0095748035 val_loss: 0.02947672\n",
      "Step = 696 train_loss: 0.008279186 val_loss: 0.029708127\n",
      "Step = 697 train_loss: 0.007421482 val_loss: 0.02683016\n",
      "Step = 698 train_loss: 0.0073670107 val_loss: 0.008383443\n",
      "Step = 699 train_loss: 0.006312062 val_loss: 0.021622673\n",
      "Step = 700 train_loss: 0.0062588165 val_loss: 0.03980824\n",
      "Step = 701 train_loss: 0.0069543617 val_loss: 0.025776105\n",
      "Step = 702 train_loss: 0.006358971 val_loss: 0.0061032716\n",
      "Step = 703 train_loss: 0.007619433 val_loss: 0.00922348\n",
      "Step = 704 train_loss: 0.007645239 val_loss: 0.009637125\n",
      "Step = 705 train_loss: 0.008609309 val_loss: 0.025103508\n",
      "Step = 706 train_loss: 0.007997529 val_loss: 0.13366269\n",
      "Step = 707 train_loss: 0.0077756196 val_loss: 0.17724557\n",
      "Step = 708 train_loss: 0.009809877 val_loss: 0.16917424\n",
      "Step = 709 train_loss: 0.009753178 val_loss: 0.15248734\n",
      "Step = 710 train_loss: 0.008026946 val_loss: 0.15259168\n",
      "Step = 711 train_loss: 0.007214759 val_loss: 0.16033262\n",
      "Step = 712 train_loss: 0.010428837 val_loss: 0.18512294\n",
      "Step = 713 train_loss: 0.010849454 val_loss: 0.21985212\n",
      "Step = 714 train_loss: 0.007976218 val_loss: 0.2218229\n",
      "Step = 715 train_loss: 0.0057432377 val_loss: 0.21508092\n",
      "Step = 716 train_loss: 0.008746163 val_loss: 0.22170705\n",
      "Step = 717 train_loss: 0.008596695 val_loss: 0.20062704\n",
      "Step = 718 train_loss: 0.0076292744 val_loss: 0.15967546\n",
      "Step = 719 train_loss: 0.009244146 val_loss: 0.13493772\n",
      "Step = 720 train_loss: 0.008110112 val_loss: 0.13545088\n",
      "Step = 721 train_loss: 0.009536387 val_loss: 0.14321965\n",
      "Step = 722 train_loss: 0.008332586 val_loss: 0.10724403\n",
      "Step = 723 train_loss: 0.004547163 val_loss: 0.0792141\n",
      "Step = 724 train_loss: 0.006763153 val_loss: 0.06776475\n",
      "Step = 725 train_loss: 0.006577405 val_loss: 0.068220094\n",
      "Step = 726 train_loss: 0.006251538 val_loss: 0.07200187\n",
      "Step = 727 train_loss: 0.008769504 val_loss: 0.08232871\n",
      "Step = 728 train_loss: 0.006091268 val_loss: 0.08878046\n",
      "Step = 729 train_loss: 0.0065826634 val_loss: 0.08282491\n",
      "Step = 730 train_loss: 0.006369575 val_loss: 0.06304424\n",
      "Step = 731 train_loss: 0.0121196555 val_loss: 0.041719012\n",
      "Step = 732 train_loss: 0.008310726 val_loss: 0.06350468\n",
      "Step = 733 train_loss: 0.0066113356 val_loss: 0.063787036\n",
      "Step = 734 train_loss: 0.008389387 val_loss: 0.0659324\n",
      "Step = 735 train_loss: 0.0070957956 val_loss: 0.06049855\n",
      "Step = 736 train_loss: 0.0076778335 val_loss: 0.016864102\n",
      "Step = 737 train_loss: 0.0090872 val_loss: 0.007560108\n",
      "Step = 738 train_loss: 0.0064923335 val_loss: 0.011006642\n",
      "Step = 739 train_loss: 0.0070651267 val_loss: 0.011869989\n",
      "Step = 740 train_loss: 0.0078238975 val_loss: 0.0055248975\n",
      "Step = 741 train_loss: 0.007929977 val_loss: 0.017137842\n",
      "Step = 742 train_loss: 0.007094912 val_loss: 0.042347133\n",
      "Step = 743 train_loss: 0.0061412626 val_loss: 0.029763216\n",
      "Step = 744 train_loss: 0.0068644998 val_loss: 0.008874875\n",
      "Step = 745 train_loss: 0.009476604 val_loss: 0.0054197446\n",
      "Step = 746 train_loss: 0.006377463 val_loss: 0.011579906\n",
      "Step = 747 train_loss: 0.008894786 val_loss: 0.011050496\n",
      "Step = 748 train_loss: 0.0056021432 val_loss: 0.06664057\n",
      "Step = 749 train_loss: 0.010392264 val_loss: 0.02448325\n",
      "Step = 750 train_loss: 0.009536561 val_loss: 0.013908139\n",
      "Step = 751 train_loss: 0.005962197 val_loss: 0.015231217\n",
      "Step = 752 train_loss: 0.0069214855 val_loss: 0.0076127206\n",
      "Step = 753 train_loss: 0.0064110463 val_loss: 0.013831434\n",
      "Step = 754 train_loss: 0.007048841 val_loss: 0.007355494\n",
      "Step = 755 train_loss: 0.0070813103 val_loss: 0.15883936\n",
      "Step = 756 train_loss: 0.00634499 val_loss: 0.3457369\n",
      "Step = 757 train_loss: 0.009270916 val_loss: 0.4167747\n",
      "Step = 758 train_loss: 0.006244768 val_loss: 0.4429858\n",
      "Step = 759 train_loss: 0.006381835 val_loss: 0.42464992\n",
      "Step = 760 train_loss: 0.006627946 val_loss: 0.38235718\n",
      "Step = 761 train_loss: 0.00716414 val_loss: 0.30109602\n",
      "Step = 762 train_loss: 0.008840049 val_loss: 0.25255924\n",
      "Step = 763 train_loss: 0.004232222 val_loss: 0.25171322\n",
      "Step = 764 train_loss: 0.0066806814 val_loss: 0.28754568\n",
      "Step = 765 train_loss: 0.0053988523 val_loss: 0.24397588\n",
      "Step = 766 train_loss: 0.007874734 val_loss: 0.14397942\n",
      "Step = 767 train_loss: 0.006430867 val_loss: 0.053311486\n",
      "Step = 768 train_loss: 0.0066234935 val_loss: 0.029569559\n",
      "Step = 769 train_loss: 0.0065148375 val_loss: 0.05855162\n",
      "Step = 770 train_loss: 0.006629136 val_loss: 0.124096856\n",
      "Step = 771 train_loss: 0.008505894 val_loss: 0.17872049\n",
      "Step = 772 train_loss: 0.0068896106 val_loss: 0.14478426\n",
      "Step = 773 train_loss: 0.0060393508 val_loss: 0.056045208\n",
      "Step = 774 train_loss: 0.0056173685 val_loss: 0.004812428\n",
      "Step = 775 train_loss: 0.00875537 val_loss: 0.01019974\n",
      "Step = 776 train_loss: 0.00540143 val_loss: 0.010933902\n",
      "Step = 777 train_loss: 0.007344826 val_loss: 0.006791763\n",
      "Step = 778 train_loss: 0.004732064 val_loss: 0.05567225\n",
      "Step = 779 train_loss: 0.006041775 val_loss: 0.038150102\n",
      "Step = 780 train_loss: 0.0071955696 val_loss: 0.007913582\n",
      "Step = 781 train_loss: 0.0043517603 val_loss: 0.006445816\n",
      "Step = 782 train_loss: 0.008797158 val_loss: 0.011990727\n",
      "Step = 783 train_loss: 0.0053716046 val_loss: 0.008421767\n",
      "Step = 784 train_loss: 0.008461966 val_loss: 0.015279324\n",
      "Step = 785 train_loss: 0.005657089 val_loss: 0.09067997\n",
      "Step = 786 train_loss: 0.008231599 val_loss: 0.12274365\n",
      "Step = 787 train_loss: 0.009882529 val_loss: 0.09704589\n",
      "Step = 788 train_loss: 0.006372056 val_loss: 0.031081226\n",
      "Step = 789 train_loss: 0.008383619 val_loss: 0.0061681443\n",
      "Step = 790 train_loss: 0.011395932 val_loss: 0.004276438\n",
      "Step = 791 train_loss: 0.007270712 val_loss: 0.004904894\n",
      "Step = 792 train_loss: 0.006372225 val_loss: 0.0077492353\n",
      "Step = 793 train_loss: 0.0066119847 val_loss: 0.007650273\n",
      "Step = 794 train_loss: 0.004612281 val_loss: 0.005569226\n",
      "Step = 795 train_loss: 0.008221141 val_loss: 0.0047540125\n",
      "Step = 796 train_loss: 0.0057514384 val_loss: 0.004909298\n",
      "Step = 797 train_loss: 0.005321975 val_loss: 0.010431974\n",
      "Step = 798 train_loss: 0.007940834 val_loss: 0.0052038813\n",
      "Step = 799 train_loss: 0.005397075 val_loss: 0.0062237927\n",
      "Step = 800 train_loss: 0.004808181 val_loss: 0.008895932\n",
      "Step = 801 train_loss: 0.0048923544 val_loss: 0.0066346126\n",
      "Step = 802 train_loss: 0.0057646027 val_loss: 0.0054786988\n",
      "Step = 803 train_loss: 0.0067689274 val_loss: 0.009560867\n",
      "Step = 804 train_loss: 0.007310058 val_loss: 0.014283051\n",
      "Step = 805 train_loss: 0.00798226 val_loss: 0.0048549827\n",
      "Step = 806 train_loss: 0.008055244 val_loss: 0.010740656\n",
      "Step = 807 train_loss: 0.0061770286 val_loss: 0.008194287\n",
      "Step = 808 train_loss: 0.005319261 val_loss: 0.010824821\n",
      "Step = 809 train_loss: 0.0075899097 val_loss: 0.022574639\n",
      "Step = 810 train_loss: 0.0060383463 val_loss: 0.018575178\n",
      "Step = 811 train_loss: 0.005154023 val_loss: 0.012971959\n",
      "Step = 812 train_loss: 0.009613362 val_loss: 0.011073055\n",
      "Step = 813 train_loss: 0.007182314 val_loss: 0.1125798\n",
      "Step = 814 train_loss: 0.0063917655 val_loss: 0.17228557\n",
      "Step = 815 train_loss: 0.0066864016 val_loss: 0.16976587\n",
      "Step = 816 train_loss: 0.009072516 val_loss: 0.0958868\n",
      "Step = 817 train_loss: 0.007159936 val_loss: 0.005769357\n",
      "Step = 818 train_loss: 0.0074203317 val_loss: 0.0262804\n",
      "Step = 819 train_loss: 0.0052922163 val_loss: 0.029649995\n",
      "Step = 820 train_loss: 0.0052145133 val_loss: 0.030782435\n",
      "Step = 821 train_loss: 0.008932273 val_loss: 0.023666522\n",
      "Step = 822 train_loss: 0.005882309 val_loss: 0.0064183604\n",
      "Step = 823 train_loss: 0.0076526073 val_loss: 0.019507226\n",
      "Step = 824 train_loss: 0.007804387 val_loss: 0.026748892\n",
      "Step = 825 train_loss: 0.006429305 val_loss: 0.004595339\n",
      "Step = 826 train_loss: 0.009869227 val_loss: 0.0041636536\n",
      "Step = 827 train_loss: 0.0064659375 val_loss: 0.0074579692\n",
      "Step = 828 train_loss: 0.0034768083 val_loss: 0.022782322\n",
      "Step = 829 train_loss: 0.008067646 val_loss: 0.07140492\n",
      "Step = 830 train_loss: 0.0053377342 val_loss: 0.14370489\n",
      "Step = 831 train_loss: 0.006894894 val_loss: 0.10294721\n",
      "Step = 832 train_loss: 0.011249524 val_loss: 0.097815245\n",
      "Step = 833 train_loss: 0.0030850784 val_loss: 0.045598995\n",
      "Step = 834 train_loss: 0.009325185 val_loss: 0.0068460414\n",
      "Step = 835 train_loss: 0.0043253484 val_loss: 0.018906156\n",
      "Step = 836 train_loss: 0.0043501374 val_loss: 0.025282983\n",
      "Step = 837 train_loss: 0.008587337 val_loss: 0.02299417\n",
      "Step = 838 train_loss: 0.005021723 val_loss: 0.017773233\n",
      "Step = 839 train_loss: 0.007512331 val_loss: 0.007984368\n",
      "Step = 840 train_loss: 0.0051854393 val_loss: 0.028386282\n",
      "Step = 841 train_loss: 0.0058553237 val_loss: 0.036002733\n",
      "Step = 842 train_loss: 0.006626827 val_loss: 0.0068359943\n",
      "Step = 843 train_loss: 0.006681069 val_loss: 0.0065027243\n",
      "Step = 844 train_loss: 0.006415733 val_loss: 0.020770408\n",
      "Step = 845 train_loss: 0.0076884474 val_loss: 0.024494579\n",
      "Step = 846 train_loss: 0.007400716 val_loss: 0.019466253\n",
      "Step = 847 train_loss: 0.009316074 val_loss: 0.010122103\n",
      "Step = 848 train_loss: 0.0073347283 val_loss: 0.0050091688\n",
      "Step = 849 train_loss: 0.0050979224 val_loss: 0.0058447104\n",
      "Step = 850 train_loss: 0.004396812 val_loss: 0.0043302802\n",
      "Step = 851 train_loss: 0.008451114 val_loss: 0.0059035495\n",
      "Step = 852 train_loss: 0.007324184 val_loss: 0.0049188663\n",
      "Step = 853 train_loss: 0.006780113 val_loss: 0.005901477\n",
      "Step = 854 train_loss: 0.005008588 val_loss: 0.0050048674\n",
      "Step = 855 train_loss: 0.00736526 val_loss: 0.010674418\n",
      "Step = 856 train_loss: 0.006218214 val_loss: 0.012496987\n",
      "Step = 857 train_loss: 0.00447753 val_loss: 0.009721853\n",
      "Step = 858 train_loss: 0.005908985 val_loss: 0.0039731394\n",
      "Step = 859 train_loss: 0.0068147103 val_loss: 0.050531022\n",
      "Step = 860 train_loss: 0.005583132 val_loss: 0.058466144\n",
      "Step = 861 train_loss: 0.0050199684 val_loss: 0.023524772\n",
      "Step = 862 train_loss: 0.006311806 val_loss: 0.005637991\n",
      "Step = 863 train_loss: 0.008991483 val_loss: 0.006384545\n",
      "Step = 864 train_loss: 0.0042110155 val_loss: 0.0041779373\n",
      "Step = 865 train_loss: 0.0050535416 val_loss: 0.004033622\n",
      "Step = 866 train_loss: 0.0057664537 val_loss: 0.0041191587\n",
      "Step = 867 train_loss: 0.0054236483 val_loss: 0.008635827\n",
      "Step = 868 train_loss: 0.0047983006 val_loss: 0.007755636\n",
      "Step = 869 train_loss: 0.006605715 val_loss: 0.009934978\n",
      "Step = 870 train_loss: 0.005316942 val_loss: 0.009892855\n",
      "Step = 871 train_loss: 0.0071419394 val_loss: 0.00653401\n",
      "Step = 872 train_loss: 0.0068064625 val_loss: 0.006899099\n",
      "Step = 873 train_loss: 0.007076823 val_loss: 0.00450543\n",
      "Step = 874 train_loss: 0.0056510996 val_loss: 0.0043839407\n",
      "Step = 875 train_loss: 0.007848199 val_loss: 0.006038157\n",
      "Step = 876 train_loss: 0.005662665 val_loss: 0.008391978\n",
      "Step = 877 train_loss: 0.004158632 val_loss: 0.0039499183\n",
      "Step = 878 train_loss: 0.004485278 val_loss: 0.0065677697\n",
      "Step = 879 train_loss: 0.005712377 val_loss: 0.0077633704\n",
      "Step = 880 train_loss: 0.0063362364 val_loss: 0.00606854\n",
      "Step = 881 train_loss: 0.008357908 val_loss: 0.008748048\n",
      "Step = 882 train_loss: 0.0073860954 val_loss: 0.010758802\n",
      "Step = 883 train_loss: 0.005383551 val_loss: 0.010675734\n",
      "Step = 884 train_loss: 0.0049993093 val_loss: 0.010790573\n",
      "Step = 885 train_loss: 0.0065469844 val_loss: 0.008192388\n",
      "Step = 886 train_loss: 0.004780447 val_loss: 0.012875445\n",
      "Step = 887 train_loss: 0.0053536934 val_loss: 0.014629601\n",
      "Step = 888 train_loss: 0.004260105 val_loss: 0.005778701\n",
      "Step = 889 train_loss: 0.0036542886 val_loss: 0.0050657704\n",
      "Step = 890 train_loss: 0.0059770993 val_loss: 0.0045392695\n",
      "Step = 891 train_loss: 0.009844763 val_loss: 0.004405723\n",
      "Step = 892 train_loss: 0.0046666423 val_loss: 0.008537315\n",
      "Step = 893 train_loss: 0.005058461 val_loss: 0.015971715\n",
      "Step = 894 train_loss: 0.0043312144 val_loss: 0.01712619\n",
      "Step = 895 train_loss: 0.005199293 val_loss: 0.011695012\n",
      "Step = 896 train_loss: 0.0057647782 val_loss: 0.026939323\n",
      "Step = 897 train_loss: 0.0054189987 val_loss: 0.084148\n",
      "Step = 898 train_loss: 0.0046878406 val_loss: 0.05332395\n",
      "Step = 899 train_loss: 0.0066892514 val_loss: 0.016863538\n",
      "Step = 900 train_loss: 0.004089794 val_loss: 0.006580006\n",
      "Step = 901 train_loss: 0.0049123345 val_loss: 0.0056941467\n",
      "Step = 902 train_loss: 0.0042046946 val_loss: 0.015648011\n",
      "Step = 903 train_loss: 0.0038585914 val_loss: 0.018651137\n",
      "Step = 904 train_loss: 0.00632592 val_loss: 0.017999765\n",
      "Step = 905 train_loss: 0.0028585328 val_loss: 0.016245546\n",
      "Step = 906 train_loss: 0.0075937724 val_loss: 0.012248853\n",
      "Step = 907 train_loss: 0.005284345 val_loss: 0.0035729492\n",
      "Step = 908 train_loss: 0.0054413946 val_loss: 0.0076078707\n",
      "Step = 909 train_loss: 0.004052897 val_loss: 0.0037335095\n",
      "Step = 910 train_loss: 0.005589543 val_loss: 0.0063514425\n",
      "Step = 911 train_loss: 0.0042860005 val_loss: 0.008961218\n",
      "Step = 912 train_loss: 0.004993813 val_loss: 0.021741157\n",
      "Step = 913 train_loss: 0.0072979755 val_loss: 0.07603263\n",
      "Step = 914 train_loss: 0.0055365255 val_loss: 0.071125306\n",
      "Step = 915 train_loss: 0.0050707753 val_loss: 0.014419006\n",
      "Step = 916 train_loss: 0.0047928267 val_loss: 0.009824323\n",
      "Step = 917 train_loss: 0.0033998278 val_loss: 0.009202755\n",
      "Step = 918 train_loss: 0.007266323 val_loss: 0.0056993347\n",
      "Step = 919 train_loss: 0.005306354 val_loss: 0.00946134\n",
      "Step = 920 train_loss: 0.0037567825 val_loss: 0.010885111\n",
      "Step = 921 train_loss: 0.0027659899 val_loss: 0.00945057\n",
      "Step = 922 train_loss: 0.005515297 val_loss: 0.0063473918\n",
      "Step = 923 train_loss: 0.004122553 val_loss: 0.008466775\n",
      "Step = 924 train_loss: 0.0054017724 val_loss: 0.0064729694\n",
      "Step = 925 train_loss: 0.0039831377 val_loss: 0.009320824\n",
      "Step = 926 train_loss: 0.0055073206 val_loss: 0.013710029\n",
      "Step = 927 train_loss: 0.0037488656 val_loss: 0.014131017\n",
      "Step = 928 train_loss: 0.0033629336 val_loss: 0.012940591\n",
      "Step = 929 train_loss: 0.005693557 val_loss: 0.0044687563\n",
      "Step = 930 train_loss: 0.003829827 val_loss: 0.017318046\n",
      "Step = 931 train_loss: 0.0043736068 val_loss: 0.027187875\n",
      "Step = 932 train_loss: 0.0039597927 val_loss: 0.010421894\n",
      "Step = 933 train_loss: 0.004555386 val_loss: 0.017304923\n",
      "Step = 934 train_loss: 0.0039047708 val_loss: 0.027098851\n",
      "Step = 935 train_loss: 0.0040224376 val_loss: 0.025473382\n",
      "Step = 936 train_loss: 0.0044267247 val_loss: 0.011369272\n",
      "Step = 937 train_loss: 0.0063573224 val_loss: 0.011448762\n",
      "Step = 938 train_loss: 0.005464014 val_loss: 0.059282467\n",
      "Step = 939 train_loss: 0.0064944625 val_loss: 0.041238878\n",
      "Step = 940 train_loss: 0.005255061 val_loss: 0.038754623\n",
      "Step = 941 train_loss: 0.0040000756 val_loss: 0.022365205\n",
      "Step = 942 train_loss: 0.0040864698 val_loss: 0.008829287\n",
      "Step = 943 train_loss: 0.0050240755 val_loss: 0.0065164776\n",
      "Step = 944 train_loss: 0.0040368037 val_loss: 0.01710231\n",
      "Step = 945 train_loss: 0.006928411 val_loss: 0.014641836\n",
      "Step = 946 train_loss: 0.0050943093 val_loss: 0.006053551\n",
      "Step = 947 train_loss: 0.004645237 val_loss: 0.0069487044\n",
      "Step = 948 train_loss: 0.0034830633 val_loss: 0.004434906\n",
      "Step = 949 train_loss: 0.0054670847 val_loss: 0.0096804835\n",
      "Step = 950 train_loss: 0.0030221073 val_loss: 0.017244693\n",
      "Step = 951 train_loss: 0.005186175 val_loss: 0.016248113\n",
      "Step = 952 train_loss: 0.004379998 val_loss: 0.011747204\n",
      "Step = 953 train_loss: 0.004166104 val_loss: 0.007407489\n",
      "Step = 954 train_loss: 0.004193481 val_loss: 0.02570613\n",
      "Step = 955 train_loss: 0.0038888624 val_loss: 0.08078366\n",
      "Step = 956 train_loss: 0.005920961 val_loss: 0.052127037\n",
      "Step = 957 train_loss: 0.0075257984 val_loss: 0.008283059\n",
      "Step = 958 train_loss: 0.0056143277 val_loss: 0.023464758\n",
      "Step = 959 train_loss: 0.0039483984 val_loss: 0.02721651\n",
      "Step = 960 train_loss: 0.0055843 val_loss: 0.024325954\n",
      "Step = 961 train_loss: 0.004728438 val_loss: 0.01948923\n",
      "Step = 962 train_loss: 0.0043997127 val_loss: 0.012788228\n",
      "Step = 963 train_loss: 0.004376021 val_loss: 0.007278257\n",
      "Step = 964 train_loss: 0.0038215518 val_loss: 0.027232936\n",
      "Step = 965 train_loss: 0.005722774 val_loss: 0.021014657\n",
      "Step = 966 train_loss: 0.0066076433 val_loss: 0.038570274\n",
      "Step = 967 train_loss: 0.004593486 val_loss: 0.023295354\n",
      "Step = 968 train_loss: 0.004121487 val_loss: 0.0034110518\n",
      "Step = 969 train_loss: 0.0044329483 val_loss: 0.013814997\n",
      "Step = 970 train_loss: 0.0034509392 val_loss: 0.02029795\n",
      "Step = 971 train_loss: 0.0030450213 val_loss: 0.024093043\n",
      "Step = 972 train_loss: 0.0041249287 val_loss: 0.023141423\n",
      "Step = 973 train_loss: 0.0062561156 val_loss: 0.015259685\n",
      "Step = 974 train_loss: 0.0032907513 val_loss: 0.00431973\n",
      "Step = 975 train_loss: 0.0035970802 val_loss: 0.024144255\n",
      "Step = 976 train_loss: 0.006451082 val_loss: 0.040710762\n",
      "Step = 977 train_loss: 0.004729433 val_loss: 0.02464411\n",
      "Step = 978 train_loss: 0.0034493147 val_loss: 0.007881901\n",
      "Step = 979 train_loss: 0.004042257 val_loss: 0.012239914\n",
      "Step = 980 train_loss: 0.0029827654 val_loss: 0.019794129\n",
      "Step = 981 train_loss: 0.0065817367 val_loss: 0.024378011\n",
      "Step = 982 train_loss: 0.0035882948 val_loss: 0.020368943\n",
      "Step = 983 train_loss: 0.004651132 val_loss: 0.010913387\n",
      "Step = 984 train_loss: 0.0042021032 val_loss: 0.0035857824\n",
      "Step = 985 train_loss: 0.0037993204 val_loss: 0.04115227\n",
      "Step = 986 train_loss: 0.004294673 val_loss: 0.052466992\n",
      "Step = 987 train_loss: 0.0054000216 val_loss: 0.09673308\n",
      "Step = 988 train_loss: 0.0076042404 val_loss: 0.068239935\n",
      "Step = 989 train_loss: 0.0052837515 val_loss: 0.009400856\n",
      "Step = 990 train_loss: 0.004837185 val_loss: 0.025673015\n",
      "Step = 991 train_loss: 0.0055384906 val_loss: 0.028966993\n",
      "Step = 992 train_loss: 0.00520885 val_loss: 0.0253862\n",
      "Step = 993 train_loss: 0.006175882 val_loss: 0.014851912\n",
      "Step = 994 train_loss: 0.005355395 val_loss: 0.008411415\n",
      "Step = 995 train_loss: 0.0043247314 val_loss: 0.022507789\n",
      "Step = 996 train_loss: 0.0031315607 val_loss: 0.013626464\n",
      "Step = 997 train_loss: 0.0055378154 val_loss: 0.0038329267\n",
      "Step = 998 train_loss: 0.0049704015 val_loss: 0.0055798613\n",
      "Step = 999 train_loss: 0.0029564162 val_loss: 0.008261281\n",
      "968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_10860\\2290839578.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_10860\\2290839578.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "val_losses=[]\n",
    "\n",
    "for t in range(1000):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    '''save all the model in each epoch'''    \n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, 'Target_model/net_parameters'+str(t)+'.pkl')\n",
    "        \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print ('Step = %d' % t, 'train_loss:',train_loss.data.numpy(),'val_loss:',val_loss.data.numpy())\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "   \n",
    "'''find the best model'''\n",
    "best_index=val_losses.index(np.min(val_losses))\n",
    "print(best_index)\n",
    "\n",
    "'''copy the best model from model file to the model_test file'''\n",
    "if 1:\n",
    "    shutil.copyfile('Target_model/net_parameters'+str(best_index)+'.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "'''Reload the best model''' \n",
    "model = Net()\n",
    "a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
    "optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.113199726\n",
      "mpe_val: 0.122723885\n",
      "mpe_a: 0.12077664334697608\n",
      "mpe_b: 0.2809636539707568\n",
      "rmse_train: 117.43443\n",
      "rmse_val: 130.47015\n",
      "rmse_a: 113.22194708768076\n",
      "rmse_b: 312.93477754957183\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.9049155116081238\n",
      "r2_val: 0.8364689946174622\n",
      "r2_a: 0.9160007727042746\n",
      "r2_b: -0.204700199031272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
