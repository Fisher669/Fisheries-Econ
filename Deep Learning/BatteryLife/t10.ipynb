{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型效果\n",
    "对于googlenet baseline 的修改，详细见md文件\n",
    "\n",
    "\n",
    "## 对比（非lstm架构）\n",
    "r2_train: 0.9947811961174011\n",
    "r2_val: 0.9749422669410706\n",
    "r2_a: 0.9464100454957816\n",
    "r2_b: 0.6836000724611211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 公用的前3层卷积层\n",
    "        self.shared_cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(8, 16, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # 插入Google net Inception模块\n",
    "        self.inceptions = torch.nn.ModuleList([\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True),\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(3, 3), stride=1, padding=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True),\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(5, 5), stride=1, padding=2),  # Adjusted padding to 2\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.LeakyReLU(inplace=True)\n",
    "            ),\n",
    "            torch.nn.Sequential(\n",
    "                torch.nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.Conv2d(32, 32, kernel_size=(1, 1), stride=1),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "            )\n",
    "        ])\n",
    "        self.lstm = torch.nn.LSTM(input_size=32, hidden_size=128, num_layers=3, batch_first=True)\n",
    "        # Input size for fc1 will be calculated dynamically\n",
    "        # self.fc1 = None  \n",
    "        self.drop_layer = torch.nn.Dropout(p=0.2) \n",
    "        self.fc2 = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            # 公用卷积层\n",
    "            shared_out = self.shared_cnn(x)\n",
    "            \n",
    "            # Inception outputs\n",
    "            inception_outputs = [inception(shared_out) for inception in self.inceptions]\n",
    "            combined = torch.cat(inception_outputs, dim=1)  # Concatenate along channel dimension (dim=1)\n",
    "\n",
    "            # Reshape for LSTM: (batch_size, sequence_length, input_size)\n",
    "            combined = combined.permute(0, 2, 3, 1)  # (batch_size, height, width, channels)\n",
    "            combined = combined.contiguous().view(combined.size(0), -1, 32)  # Flatten height & width into sequence length\n",
    "\n",
    "            # Pass through LSTM\n",
    "            lstm_out, (hn, cn) = self.lstm(combined)\n",
    "\n",
    "            # Use LSTM output at the final time step\n",
    "            lstm_out_last = lstm_out[:, -1, :]  # (batch_size, hidden_size)\n",
    "\n",
    "            # # Calculate input size for fully connected layer dynamically\n",
    "            # if self.fc1 is None:\n",
    "            #     self.fc1 = torch.nn.Linear(lstm_out_last.size(1), 1000)\n",
    "\n",
    "            # Fully connected layers\n",
    "            x = torch.relu(lstm_out_last)\n",
    "            x = self.drop_layer(x)\n",
    "            x = self.fc2(x)\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during forward pass: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (shared_cnn): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  )\n",
      "  (inceptions): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (lstm): LSTM(32, 128, num_layers=3, batch_first=True)\n",
      "  (drop_layer): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Step = 0 train_loss: 0.07180521 val_loss: 0.06962865\n",
      "Step = 1 train_loss: 0.07119013 val_loss: 0.06960314\n",
      "Step = 2 train_loss: 0.071396306 val_loss: 0.069577456\n",
      "Step = 3 train_loss: 0.071165085 val_loss: 0.06954822\n",
      "Step = 4 train_loss: 0.07153624 val_loss: 0.06951768\n",
      "Step = 5 train_loss: 0.07208445 val_loss: 0.06948732\n",
      "Step = 6 train_loss: 0.07133219 val_loss: 0.069456145\n",
      "Step = 7 train_loss: 0.07136534 val_loss: 0.069425456\n",
      "Step = 8 train_loss: 0.07187858 val_loss: 0.069395736\n",
      "Step = 9 train_loss: 0.070934206 val_loss: 0.06936549\n",
      "Step = 10 train_loss: 0.07099777 val_loss: 0.06933455\n",
      "Step = 11 train_loss: 0.07126363 val_loss: 0.06930305\n",
      "Step = 12 train_loss: 0.07103552 val_loss: 0.06927008\n",
      "Step = 13 train_loss: 0.07088365 val_loss: 0.069235936\n",
      "Step = 14 train_loss: 0.07052946 val_loss: 0.069201164\n",
      "Step = 15 train_loss: 0.07120566 val_loss: 0.06916512\n",
      "Step = 16 train_loss: 0.07093908 val_loss: 0.06912721\n",
      "Step = 17 train_loss: 0.070997275 val_loss: 0.069087625\n",
      "Step = 18 train_loss: 0.071030654 val_loss: 0.06904553\n",
      "Step = 19 train_loss: 0.07099964 val_loss: 0.069000974\n",
      "Step = 20 train_loss: 0.07087143 val_loss: 0.068956\n",
      "Step = 21 train_loss: 0.070861205 val_loss: 0.068911195\n",
      "Step = 22 train_loss: 0.07025114 val_loss: 0.06886753\n",
      "Step = 23 train_loss: 0.07097285 val_loss: 0.06882328\n",
      "Step = 24 train_loss: 0.07014667 val_loss: 0.06877579\n",
      "Step = 25 train_loss: 0.070444524 val_loss: 0.06872422\n",
      "Step = 26 train_loss: 0.07083531 val_loss: 0.068669334\n",
      "Step = 27 train_loss: 0.070469424 val_loss: 0.06861293\n",
      "Step = 28 train_loss: 0.07097966 val_loss: 0.06855358\n",
      "Step = 29 train_loss: 0.07009379 val_loss: 0.068494484\n",
      "Step = 30 train_loss: 0.07053265 val_loss: 0.06843542\n",
      "Step = 31 train_loss: 0.07023429 val_loss: 0.06837665\n",
      "Step = 32 train_loss: 0.07024557 val_loss: 0.06831762\n",
      "Step = 33 train_loss: 0.07009308 val_loss: 0.068257086\n",
      "Step = 34 train_loss: 0.069581695 val_loss: 0.068199106\n",
      "Step = 35 train_loss: 0.07019552 val_loss: 0.068142146\n",
      "Step = 36 train_loss: 0.070406005 val_loss: 0.06808331\n",
      "Step = 37 train_loss: 0.06960419 val_loss: 0.06802355\n",
      "Step = 38 train_loss: 0.0698927 val_loss: 0.06796059\n",
      "Step = 39 train_loss: 0.07011502 val_loss: 0.06790225\n",
      "Step = 40 train_loss: 0.070245296 val_loss: 0.06784103\n",
      "Step = 41 train_loss: 0.06974451 val_loss: 0.06778488\n",
      "Step = 42 train_loss: 0.06922194 val_loss: 0.067731835\n",
      "Step = 43 train_loss: 0.06991069 val_loss: 0.06767526\n",
      "Step = 44 train_loss: 0.069823794 val_loss: 0.067619875\n",
      "Step = 45 train_loss: 0.069885984 val_loss: 0.06756455\n",
      "Step = 46 train_loss: 0.069720335 val_loss: 0.06751517\n",
      "Step = 47 train_loss: 0.069135554 val_loss: 0.06746553\n",
      "Step = 48 train_loss: 0.06930131 val_loss: 0.067420974\n",
      "Step = 49 train_loss: 0.06967466 val_loss: 0.0673729\n",
      "Step = 50 train_loss: 0.069678776 val_loss: 0.067322865\n",
      "Step = 51 train_loss: 0.06907926 val_loss: 0.06728616\n",
      "Step = 52 train_loss: 0.06960636 val_loss: 0.06724542\n",
      "Step = 53 train_loss: 0.068712935 val_loss: 0.06720422\n",
      "Step = 54 train_loss: 0.06895498 val_loss: 0.06716021\n",
      "Step = 55 train_loss: 0.06916639 val_loss: 0.06711773\n",
      "Step = 56 train_loss: 0.06878133 val_loss: 0.0670677\n",
      "Step = 57 train_loss: 0.06941627 val_loss: 0.06701978\n",
      "Step = 58 train_loss: 0.068677574 val_loss: 0.06697376\n",
      "Step = 59 train_loss: 0.069525875 val_loss: 0.06693089\n",
      "Step = 60 train_loss: 0.06866665 val_loss: 0.06688947\n",
      "Step = 61 train_loss: 0.068806075 val_loss: 0.066847265\n",
      "Step = 62 train_loss: 0.068485074 val_loss: 0.06680517\n",
      "Step = 63 train_loss: 0.069516614 val_loss: 0.0667592\n",
      "Step = 64 train_loss: 0.06872208 val_loss: 0.06671234\n",
      "Step = 65 train_loss: 0.06876966 val_loss: 0.06666493\n",
      "Step = 66 train_loss: 0.06830665 val_loss: 0.066619456\n",
      "Step = 67 train_loss: 0.06829694 val_loss: 0.066579916\n",
      "Step = 68 train_loss: 0.068534836 val_loss: 0.06654166\n",
      "Step = 69 train_loss: 0.06830266 val_loss: 0.06650526\n",
      "Step = 70 train_loss: 0.068436176 val_loss: 0.06646115\n",
      "Step = 71 train_loss: 0.068637565 val_loss: 0.0664203\n",
      "Step = 72 train_loss: 0.068515 val_loss: 0.06637379\n",
      "Step = 73 train_loss: 0.0684738 val_loss: 0.06632032\n",
      "Step = 74 train_loss: 0.06802936 val_loss: 0.06626549\n",
      "Step = 75 train_loss: 0.0678498 val_loss: 0.066213325\n",
      "Step = 76 train_loss: 0.06815176 val_loss: 0.066164985\n",
      "Step = 77 train_loss: 0.06794079 val_loss: 0.066130124\n",
      "Step = 78 train_loss: 0.06798153 val_loss: 0.066094436\n",
      "Step = 79 train_loss: 0.06800193 val_loss: 0.06606538\n",
      "Step = 80 train_loss: 0.06764023 val_loss: 0.06602289\n",
      "Step = 81 train_loss: 0.06792914 val_loss: 0.06596443\n",
      "Step = 82 train_loss: 0.068030566 val_loss: 0.0658918\n",
      "Step = 83 train_loss: 0.06811995 val_loss: 0.06582903\n",
      "Step = 84 train_loss: 0.06740705 val_loss: 0.06576472\n",
      "Step = 85 train_loss: 0.06743646 val_loss: 0.06569568\n",
      "Step = 86 train_loss: 0.06844664 val_loss: 0.065642744\n",
      "Step = 87 train_loss: 0.067193665 val_loss: 0.06559758\n",
      "Step = 88 train_loss: 0.06731468 val_loss: 0.06556621\n",
      "Step = 89 train_loss: 0.06722682 val_loss: 0.0655217\n",
      "Step = 90 train_loss: 0.06748601 val_loss: 0.065469675\n",
      "Step = 91 train_loss: 0.06761328 val_loss: 0.06540515\n",
      "Step = 92 train_loss: 0.06715341 val_loss: 0.06533731\n",
      "Step = 93 train_loss: 0.06729687 val_loss: 0.06525522\n",
      "Step = 94 train_loss: 0.06707316 val_loss: 0.06518209\n",
      "Step = 95 train_loss: 0.0668034 val_loss: 0.06512623\n",
      "Step = 96 train_loss: 0.06739267 val_loss: 0.065081276\n",
      "Step = 97 train_loss: 0.06657689 val_loss: 0.06503043\n",
      "Step = 98 train_loss: 0.06690249 val_loss: 0.064980805\n",
      "Step = 99 train_loss: 0.06730053 val_loss: 0.0649252\n",
      "Step = 100 train_loss: 0.06675185 val_loss: 0.064866565\n",
      "Step = 101 train_loss: 0.06633742 val_loss: 0.06479233\n",
      "Step = 102 train_loss: 0.06714593 val_loss: 0.06471116\n",
      "Step = 103 train_loss: 0.066714235 val_loss: 0.06462358\n",
      "Step = 104 train_loss: 0.065983415 val_loss: 0.064552166\n",
      "Step = 105 train_loss: 0.06610065 val_loss: 0.06447688\n",
      "Step = 106 train_loss: 0.066288754 val_loss: 0.064429335\n",
      "Step = 107 train_loss: 0.06660514 val_loss: 0.0643978\n",
      "Step = 108 train_loss: 0.06582998 val_loss: 0.06439916\n",
      "Step = 109 train_loss: 0.06631512 val_loss: 0.064371586\n",
      "Step = 110 train_loss: 0.066830516 val_loss: 0.06432728\n",
      "Step = 111 train_loss: 0.06596954 val_loss: 0.06427658\n",
      "Step = 112 train_loss: 0.0660819 val_loss: 0.06421663\n",
      "Step = 113 train_loss: 0.06546921 val_loss: 0.06413501\n",
      "Step = 114 train_loss: 0.066053025 val_loss: 0.06403911\n",
      "Step = 115 train_loss: 0.0656428 val_loss: 0.06393019\n",
      "Step = 116 train_loss: 0.06567232 val_loss: 0.06381807\n",
      "Step = 117 train_loss: 0.06602978 val_loss: 0.06373589\n",
      "Step = 118 train_loss: 0.06594541 val_loss: 0.063672096\n",
      "Step = 119 train_loss: 0.06605748 val_loss: 0.063624576\n",
      "Step = 120 train_loss: 0.06525796 val_loss: 0.063560836\n",
      "Step = 121 train_loss: 0.06510455 val_loss: 0.06346866\n",
      "Step = 122 train_loss: 0.06557146 val_loss: 0.0634025\n",
      "Step = 123 train_loss: 0.065528646 val_loss: 0.06333382\n",
      "Step = 124 train_loss: 0.06494908 val_loss: 0.06326245\n",
      "Step = 125 train_loss: 0.065498166 val_loss: 0.06318195\n",
      "Step = 126 train_loss: 0.0654085 val_loss: 0.0630945\n",
      "Step = 127 train_loss: 0.06458752 val_loss: 0.0630376\n",
      "Step = 128 train_loss: 0.06499921 val_loss: 0.062999636\n",
      "Step = 129 train_loss: 0.064877644 val_loss: 0.06294836\n",
      "Step = 130 train_loss: 0.0650779 val_loss: 0.06289849\n",
      "Step = 131 train_loss: 0.064624265 val_loss: 0.06284614\n",
      "Step = 132 train_loss: 0.06505903 val_loss: 0.062792964\n",
      "Step = 133 train_loss: 0.064681455 val_loss: 0.06272631\n",
      "Step = 134 train_loss: 0.06443744 val_loss: 0.06266517\n",
      "Step = 135 train_loss: 0.06445893 val_loss: 0.062580496\n",
      "Step = 136 train_loss: 0.06402497 val_loss: 0.06246227\n",
      "Step = 137 train_loss: 0.06415988 val_loss: 0.062367264\n",
      "Step = 138 train_loss: 0.06442283 val_loss: 0.062287547\n",
      "Step = 139 train_loss: 0.064294636 val_loss: 0.062250238\n",
      "Step = 140 train_loss: 0.06393911 val_loss: 0.062267803\n",
      "Step = 141 train_loss: 0.06414592 val_loss: 0.062279213\n",
      "Step = 142 train_loss: 0.06431286 val_loss: 0.062280927\n",
      "Step = 143 train_loss: 0.06388079 val_loss: 0.062258266\n",
      "Step = 144 train_loss: 0.064175464 val_loss: 0.06221618\n",
      "Step = 145 train_loss: 0.063520424 val_loss: 0.06213985\n",
      "Step = 146 train_loss: 0.064422324 val_loss: 0.062035944\n",
      "Step = 147 train_loss: 0.06414526 val_loss: 0.06191041\n",
      "Step = 148 train_loss: 0.06386471 val_loss: 0.061778367\n",
      "Step = 149 train_loss: 0.06345416 val_loss: 0.061638482\n",
      "Step = 150 train_loss: 0.06392039 val_loss: 0.06154646\n",
      "Step = 151 train_loss: 0.06376529 val_loss: 0.061476078\n",
      "Step = 152 train_loss: 0.06343224 val_loss: 0.06140371\n",
      "Step = 153 train_loss: 0.06380595 val_loss: 0.06132949\n",
      "Step = 154 train_loss: 0.06309877 val_loss: 0.061254457\n",
      "Step = 155 train_loss: 0.063187525 val_loss: 0.0611494\n",
      "Step = 156 train_loss: 0.06316055 val_loss: 0.061034434\n",
      "Step = 157 train_loss: 0.06228095 val_loss: 0.060930613\n",
      "Step = 158 train_loss: 0.062987074 val_loss: 0.06082521\n",
      "Step = 159 train_loss: 0.06309817 val_loss: 0.060721118\n",
      "Step = 160 train_loss: 0.063629046 val_loss: 0.0606139\n",
      "Step = 161 train_loss: 0.062484164 val_loss: 0.060526755\n",
      "Step = 162 train_loss: 0.06283669 val_loss: 0.060450356\n",
      "Step = 163 train_loss: 0.061776653 val_loss: 0.060354345\n",
      "Step = 164 train_loss: 0.062377207 val_loss: 0.060252182\n",
      "Step = 165 train_loss: 0.061937656 val_loss: 0.060126375\n",
      "Step = 166 train_loss: 0.062028375 val_loss: 0.05997122\n",
      "Step = 167 train_loss: 0.06224313 val_loss: 0.05983953\n",
      "Step = 168 train_loss: 0.061886713 val_loss: 0.059739236\n",
      "Step = 169 train_loss: 0.06180728 val_loss: 0.05967542\n",
      "Step = 170 train_loss: 0.062153812 val_loss: 0.059606\n",
      "Step = 171 train_loss: 0.06141526 val_loss: 0.05949742\n",
      "Step = 172 train_loss: 0.06150206 val_loss: 0.059382908\n",
      "Step = 173 train_loss: 0.061104897 val_loss: 0.059267335\n",
      "Step = 174 train_loss: 0.060729664 val_loss: 0.05916098\n",
      "Step = 175 train_loss: 0.06152433 val_loss: 0.059061028\n",
      "Step = 176 train_loss: 0.061406307 val_loss: 0.05892474\n",
      "Step = 177 train_loss: 0.060928874 val_loss: 0.058764413\n",
      "Step = 178 train_loss: 0.06126904 val_loss: 0.05858693\n",
      "Step = 179 train_loss: 0.060672183 val_loss: 0.058467574\n",
      "Step = 180 train_loss: 0.05986774 val_loss: 0.05837069\n",
      "Step = 181 train_loss: 0.05994902 val_loss: 0.058294162\n",
      "Step = 182 train_loss: 0.06072649 val_loss: 0.058208197\n",
      "Step = 183 train_loss: 0.06056301 val_loss: 0.058121588\n",
      "Step = 184 train_loss: 0.060452256 val_loss: 0.058047377\n",
      "Step = 185 train_loss: 0.060179092 val_loss: 0.057967797\n",
      "Step = 186 train_loss: 0.059900817 val_loss: 0.057854135\n",
      "Step = 187 train_loss: 0.05959522 val_loss: 0.057731472\n",
      "Step = 188 train_loss: 0.05912117 val_loss: 0.057573278\n",
      "Step = 189 train_loss: 0.060336616 val_loss: 0.057448417\n",
      "Step = 190 train_loss: 0.059749328 val_loss: 0.05731778\n",
      "Step = 191 train_loss: 0.05938319 val_loss: 0.057222087\n",
      "Step = 192 train_loss: 0.05996365 val_loss: 0.05711797\n",
      "Step = 193 train_loss: 0.05984707 val_loss: 0.05702998\n",
      "Step = 194 train_loss: 0.05946848 val_loss: 0.056910235\n",
      "Step = 195 train_loss: 0.05932863 val_loss: 0.056796882\n",
      "Step = 196 train_loss: 0.05815285 val_loss: 0.056732915\n",
      "Step = 197 train_loss: 0.057772525 val_loss: 0.056666292\n",
      "Step = 198 train_loss: 0.05909369 val_loss: 0.056542072\n",
      "Step = 199 train_loss: 0.058897752 val_loss: 0.056414574\n",
      "Step = 200 train_loss: 0.0581002 val_loss: 0.056287464\n",
      "Step = 201 train_loss: 0.058278486 val_loss: 0.05619023\n",
      "Step = 202 train_loss: 0.058702808 val_loss: 0.056097407\n",
      "Step = 203 train_loss: 0.058329456 val_loss: 0.05591966\n",
      "Step = 204 train_loss: 0.057803113 val_loss: 0.05575493\n",
      "Step = 205 train_loss: 0.05714384 val_loss: 0.05555779\n",
      "Step = 206 train_loss: 0.057481032 val_loss: 0.055361986\n",
      "Step = 207 train_loss: 0.057307977 val_loss: 0.05520135\n",
      "Step = 208 train_loss: 0.057430223 val_loss: 0.055095736\n",
      "Step = 209 train_loss: 0.057522424 val_loss: 0.055010855\n",
      "Step = 210 train_loss: 0.057221778 val_loss: 0.054835778\n",
      "Step = 211 train_loss: 0.05634387 val_loss: 0.05466067\n",
      "Step = 212 train_loss: 0.057423417 val_loss: 0.054450657\n",
      "Step = 213 train_loss: 0.05620192 val_loss: 0.054303356\n",
      "Step = 214 train_loss: 0.05629262 val_loss: 0.054091692\n",
      "Step = 215 train_loss: 0.056200854 val_loss: 0.053803742\n",
      "Step = 216 train_loss: 0.056516685 val_loss: 0.05354498\n",
      "Step = 217 train_loss: 0.056081194 val_loss: 0.053369988\n",
      "Step = 218 train_loss: 0.05634737 val_loss: 0.053284626\n",
      "Step = 219 train_loss: 0.054764815 val_loss: 0.053201895\n",
      "Step = 220 train_loss: 0.05605046 val_loss: 0.05312699\n",
      "Step = 221 train_loss: 0.055318065 val_loss: 0.053092632\n",
      "Step = 222 train_loss: 0.054338176 val_loss: 0.052953918\n",
      "Step = 223 train_loss: 0.055227652 val_loss: 0.052893545\n",
      "Step = 224 train_loss: 0.055027563 val_loss: 0.05279797\n",
      "Step = 225 train_loss: 0.054244027 val_loss: 0.05265551\n",
      "Step = 226 train_loss: 0.05540858 val_loss: 0.052445587\n",
      "Step = 227 train_loss: 0.054655153 val_loss: 0.052221693\n",
      "Step = 228 train_loss: 0.053818442 val_loss: 0.05202199\n",
      "Step = 229 train_loss: 0.053816285 val_loss: 0.05171756\n",
      "Step = 230 train_loss: 0.0533847 val_loss: 0.051448647\n",
      "Step = 231 train_loss: 0.053652007 val_loss: 0.05114719\n",
      "Step = 232 train_loss: 0.053345628 val_loss: 0.050966647\n",
      "Step = 233 train_loss: 0.053946618 val_loss: 0.05076149\n",
      "Step = 234 train_loss: 0.053120997 val_loss: 0.05057201\n",
      "Step = 235 train_loss: 0.05160199 val_loss: 0.050354727\n",
      "Step = 236 train_loss: 0.05315743 val_loss: 0.050161723\n",
      "Step = 237 train_loss: 0.051781468 val_loss: 0.050023776\n",
      "Step = 238 train_loss: 0.052115545 val_loss: 0.049783077\n",
      "Step = 239 train_loss: 0.052266195 val_loss: 0.04960074\n",
      "Step = 240 train_loss: 0.051645026 val_loss: 0.049540166\n",
      "Step = 241 train_loss: 0.051325627 val_loss: 0.049505617\n",
      "Step = 242 train_loss: 0.051602703 val_loss: 0.049424764\n",
      "Step = 243 train_loss: 0.05118498 val_loss: 0.0492468\n",
      "Step = 244 train_loss: 0.051250238 val_loss: 0.04899521\n",
      "Step = 245 train_loss: 0.050577126 val_loss: 0.048600152\n",
      "Step = 246 train_loss: 0.05053338 val_loss: 0.048202652\n",
      "Step = 247 train_loss: 0.049563203 val_loss: 0.047941606\n",
      "Step = 248 train_loss: 0.05127484 val_loss: 0.047913678\n",
      "Step = 249 train_loss: 0.049799185 val_loss: 0.047833037\n",
      "Step = 250 train_loss: 0.049743883 val_loss: 0.04780104\n",
      "Step = 251 train_loss: 0.04964796 val_loss: 0.04775448\n",
      "Step = 252 train_loss: 0.049549017 val_loss: 0.047630172\n",
      "Step = 253 train_loss: 0.048100445 val_loss: 0.04754598\n",
      "Step = 254 train_loss: 0.048241727 val_loss: 0.047537547\n",
      "Step = 255 train_loss: 0.05123139 val_loss: 0.047249023\n",
      "Step = 256 train_loss: 0.048969228 val_loss: 0.0470146\n",
      "Step = 257 train_loss: 0.048651833 val_loss: 0.046847217\n",
      "Step = 258 train_loss: 0.047599617 val_loss: 0.046634287\n",
      "Step = 259 train_loss: 0.047981985 val_loss: 0.046369925\n",
      "Step = 260 train_loss: 0.047768906 val_loss: 0.04612146\n",
      "Step = 261 train_loss: 0.044888675 val_loss: 0.045763306\n",
      "Step = 262 train_loss: 0.045802098 val_loss: 0.045296185\n",
      "Step = 263 train_loss: 0.046864238 val_loss: 0.04490171\n",
      "Step = 264 train_loss: 0.046575587 val_loss: 0.04455075\n",
      "Step = 265 train_loss: 0.046300724 val_loss: 0.04432305\n",
      "Step = 266 train_loss: 0.04523862 val_loss: 0.044270437\n",
      "Step = 267 train_loss: 0.04505343 val_loss: 0.04403012\n",
      "Step = 268 train_loss: 0.043625314 val_loss: 0.043730408\n",
      "Step = 269 train_loss: 0.04374452 val_loss: 0.043479163\n",
      "Step = 270 train_loss: 0.044395037 val_loss: 0.04318215\n",
      "Step = 271 train_loss: 0.044356696 val_loss: 0.04286407\n",
      "Step = 272 train_loss: 0.04354229 val_loss: 0.042519517\n",
      "Step = 273 train_loss: 0.045018025 val_loss: 0.04223243\n",
      "Step = 274 train_loss: 0.044456545 val_loss: 0.04186149\n",
      "Step = 275 train_loss: 0.044134732 val_loss: 0.041498333\n",
      "Step = 276 train_loss: 0.043704953 val_loss: 0.04105318\n",
      "Step = 277 train_loss: 0.043025732 val_loss: 0.040497467\n",
      "Step = 278 train_loss: 0.04094148 val_loss: 0.040000483\n",
      "Step = 279 train_loss: 0.04077462 val_loss: 0.039603096\n",
      "Step = 280 train_loss: 0.04204187 val_loss: 0.039059367\n",
      "Step = 281 train_loss: 0.04208573 val_loss: 0.038638677\n",
      "Step = 282 train_loss: 0.041023888 val_loss: 0.038344804\n",
      "Step = 283 train_loss: 0.041446462 val_loss: 0.038111005\n",
      "Step = 284 train_loss: 0.0411222 val_loss: 0.0377671\n",
      "Step = 285 train_loss: 0.040384334 val_loss: 0.03751342\n",
      "Step = 286 train_loss: 0.039013546 val_loss: 0.037283212\n",
      "Step = 287 train_loss: 0.039968356 val_loss: 0.036962513\n",
      "Step = 288 train_loss: 0.040826462 val_loss: 0.036684778\n",
      "Step = 289 train_loss: 0.03866499 val_loss: 0.036460638\n",
      "Step = 290 train_loss: 0.03892472 val_loss: 0.03625705\n",
      "Step = 291 train_loss: 0.038851365 val_loss: 0.035969306\n",
      "Step = 292 train_loss: 0.04073745 val_loss: 0.035764854\n",
      "Step = 293 train_loss: 0.03980778 val_loss: 0.0355593\n",
      "Step = 294 train_loss: 0.0384717 val_loss: 0.03522497\n",
      "Step = 295 train_loss: 0.037474178 val_loss: 0.034931272\n",
      "Step = 296 train_loss: 0.03670001 val_loss: 0.0346108\n",
      "Step = 297 train_loss: 0.03769149 val_loss: 0.0342862\n",
      "Step = 298 train_loss: 0.036664966 val_loss: 0.033931103\n",
      "Step = 299 train_loss: 0.03445532 val_loss: 0.03359269\n",
      "Step = 300 train_loss: 0.035823844 val_loss: 0.03321685\n",
      "Step = 301 train_loss: 0.03791531 val_loss: 0.032709435\n",
      "Step = 302 train_loss: 0.036767457 val_loss: 0.03226522\n",
      "Step = 303 train_loss: 0.03440523 val_loss: 0.03171491\n",
      "Step = 304 train_loss: 0.033936616 val_loss: 0.031370353\n",
      "Step = 305 train_loss: 0.034931 val_loss: 0.031169644\n",
      "Step = 306 train_loss: 0.034863867 val_loss: 0.03107621\n",
      "Step = 307 train_loss: 0.034129594 val_loss: 0.031096706\n",
      "Step = 308 train_loss: 0.035305228 val_loss: 0.031298842\n",
      "Step = 309 train_loss: 0.03409974 val_loss: 0.031482678\n",
      "Step = 310 train_loss: 0.03442881 val_loss: 0.031564493\n",
      "Step = 311 train_loss: 0.03251257 val_loss: 0.03150465\n",
      "Step = 312 train_loss: 0.032727584 val_loss: 0.031264838\n",
      "Step = 313 train_loss: 0.03446522 val_loss: 0.03087577\n",
      "Step = 314 train_loss: 0.034950946 val_loss: 0.030285722\n",
      "Step = 315 train_loss: 0.03276199 val_loss: 0.029440952\n",
      "Step = 316 train_loss: 0.031781144 val_loss: 0.028759526\n",
      "Step = 317 train_loss: 0.032845594 val_loss: 0.02835361\n",
      "Step = 318 train_loss: 0.03212549 val_loss: 0.027952654\n",
      "Step = 319 train_loss: 0.0328167 val_loss: 0.027693002\n",
      "Step = 320 train_loss: 0.03151148 val_loss: 0.027632676\n",
      "Step = 321 train_loss: 0.03198894 val_loss: 0.027881783\n",
      "Step = 322 train_loss: 0.031263903 val_loss: 0.02836555\n",
      "Step = 323 train_loss: 0.033268753 val_loss: 0.02882105\n",
      "Step = 324 train_loss: 0.03089708 val_loss: 0.029285705\n",
      "Step = 325 train_loss: 0.031123336 val_loss: 0.029414494\n",
      "Step = 326 train_loss: 0.029064387 val_loss: 0.029530903\n",
      "Step = 327 train_loss: 0.030519733 val_loss: 0.029467221\n",
      "Step = 328 train_loss: 0.03247083 val_loss: 0.029270513\n",
      "Step = 329 train_loss: 0.0306738 val_loss: 0.02895878\n",
      "Step = 330 train_loss: 0.03039213 val_loss: 0.028738156\n",
      "Step = 331 train_loss: 0.030197106 val_loss: 0.028470693\n",
      "Step = 332 train_loss: 0.032166626 val_loss: 0.028085414\n",
      "Step = 333 train_loss: 0.029496767 val_loss: 0.027599677\n",
      "Step = 334 train_loss: 0.029556172 val_loss: 0.027098944\n",
      "Step = 335 train_loss: 0.028813528 val_loss: 0.026509613\n",
      "Step = 336 train_loss: 0.028584838 val_loss: 0.025821902\n",
      "Step = 337 train_loss: 0.028430982 val_loss: 0.025278376\n",
      "Step = 338 train_loss: 0.02989659 val_loss: 0.024787612\n",
      "Step = 339 train_loss: 0.031079996 val_loss: 0.024492221\n",
      "Step = 340 train_loss: 0.028784249 val_loss: 0.024320306\n",
      "Step = 341 train_loss: 0.029272538 val_loss: 0.024336532\n",
      "Step = 342 train_loss: 0.029279413 val_loss: 0.024390453\n",
      "Step = 343 train_loss: 0.029900823 val_loss: 0.024450224\n",
      "Step = 344 train_loss: 0.027136149 val_loss: 0.024319312\n",
      "Step = 345 train_loss: 0.02753497 val_loss: 0.0239869\n",
      "Step = 346 train_loss: 0.029628925 val_loss: 0.023587856\n",
      "Step = 347 train_loss: 0.02903048 val_loss: 0.023179086\n",
      "Step = 348 train_loss: 0.027195306 val_loss: 0.023027467\n",
      "Step = 349 train_loss: 0.027895395 val_loss: 0.023026263\n",
      "Step = 350 train_loss: 0.027424524 val_loss: 0.022934208\n",
      "Step = 351 train_loss: 0.029591518 val_loss: 0.022875305\n",
      "Step = 352 train_loss: 0.030291079 val_loss: 0.022870887\n",
      "Step = 353 train_loss: 0.029283086 val_loss: 0.022881588\n",
      "Step = 354 train_loss: 0.027554095 val_loss: 0.023016121\n",
      "Step = 355 train_loss: 0.029993635 val_loss: 0.023260722\n",
      "Step = 356 train_loss: 0.028219564 val_loss: 0.023483735\n",
      "Step = 357 train_loss: 0.031309932 val_loss: 0.023665993\n",
      "Step = 358 train_loss: 0.028828392 val_loss: 0.02366677\n",
      "Step = 359 train_loss: 0.02936223 val_loss: 0.023483133\n",
      "Step = 360 train_loss: 0.02804471 val_loss: 0.023281517\n",
      "Step = 361 train_loss: 0.028970672 val_loss: 0.02299948\n",
      "Step = 362 train_loss: 0.02904531 val_loss: 0.022762546\n",
      "Step = 363 train_loss: 0.02800509 val_loss: 0.022473834\n",
      "Step = 364 train_loss: 0.029377308 val_loss: 0.021997828\n",
      "Step = 365 train_loss: 0.027610026 val_loss: 0.021654546\n",
      "Step = 366 train_loss: 0.028838139 val_loss: 0.021297755\n",
      "Step = 367 train_loss: 0.029236915 val_loss: 0.021181356\n",
      "Step = 368 train_loss: 0.028058302 val_loss: 0.021126049\n",
      "Step = 369 train_loss: 0.031225469 val_loss: 0.021280589\n",
      "Step = 370 train_loss: 0.028460754 val_loss: 0.02142766\n",
      "Step = 371 train_loss: 0.029034752 val_loss: 0.021472756\n",
      "Step = 372 train_loss: 0.026895713 val_loss: 0.021107815\n",
      "Step = 373 train_loss: 0.027791573 val_loss: 0.02075949\n",
      "Step = 374 train_loss: 0.026305834 val_loss: 0.020736566\n",
      "Step = 375 train_loss: 0.025732556 val_loss: 0.020808008\n",
      "Step = 376 train_loss: 0.027623408 val_loss: 0.020912983\n",
      "Step = 377 train_loss: 0.029566268 val_loss: 0.02108969\n",
      "Step = 378 train_loss: 0.027695207 val_loss: 0.021708002\n",
      "Step = 379 train_loss: 0.02911947 val_loss: 0.0228282\n",
      "Step = 380 train_loss: 0.02764369 val_loss: 0.023380019\n",
      "Step = 381 train_loss: 0.029078472 val_loss: 0.023548035\n",
      "Step = 382 train_loss: 0.026505621 val_loss: 0.023037829\n",
      "Step = 383 train_loss: 0.026725786 val_loss: 0.021794071\n",
      "Step = 384 train_loss: 0.025942935 val_loss: 0.020680787\n",
      "Step = 385 train_loss: 0.028887697 val_loss: 0.020114265\n",
      "Step = 386 train_loss: 0.028174916 val_loss: 0.020155532\n",
      "Step = 387 train_loss: 0.025879677 val_loss: 0.020127682\n",
      "Step = 388 train_loss: 0.02835437 val_loss: 0.020608762\n",
      "Step = 389 train_loss: 0.027006913 val_loss: 0.023372095\n",
      "Step = 390 train_loss: 0.026073553 val_loss: 0.024787147\n",
      "Step = 391 train_loss: 0.026922066 val_loss: 0.024420965\n",
      "Step = 392 train_loss: 0.026875466 val_loss: 0.02376317\n",
      "Step = 393 train_loss: 0.028410655 val_loss: 0.022022575\n",
      "Step = 394 train_loss: 0.027365355 val_loss: 0.020590417\n",
      "Step = 395 train_loss: 0.029145466 val_loss: 0.020844657\n",
      "Step = 396 train_loss: 0.02799648 val_loss: 0.021795362\n",
      "Step = 397 train_loss: 0.02764551 val_loss: 0.024495296\n",
      "Step = 398 train_loss: 0.027558446 val_loss: 0.027225498\n",
      "Step = 399 train_loss: 0.027340429 val_loss: 0.027522279\n",
      "Step = 400 train_loss: 0.027879648 val_loss: 0.026951127\n",
      "Step = 401 train_loss: 0.027365595 val_loss: 0.025509207\n",
      "Step = 402 train_loss: 0.027662078 val_loss: 0.023583109\n",
      "Step = 403 train_loss: 0.025662728 val_loss: 0.021210149\n",
      "Step = 404 train_loss: 0.026497176 val_loss: 0.019887915\n",
      "Step = 405 train_loss: 0.024917917 val_loss: 0.021122083\n",
      "Step = 406 train_loss: 0.025975952 val_loss: 0.023245903\n",
      "Step = 407 train_loss: 0.027380476 val_loss: 0.024413222\n",
      "Step = 408 train_loss: 0.024688588 val_loss: 0.025212398\n",
      "Step = 409 train_loss: 0.026203036 val_loss: 0.024751872\n",
      "Step = 410 train_loss: 0.027365472 val_loss: 0.024649356\n",
      "Step = 411 train_loss: 0.02591295 val_loss: 0.025605088\n",
      "Step = 412 train_loss: 0.026447069 val_loss: 0.02780859\n",
      "Step = 413 train_loss: 0.024601815 val_loss: 0.027611613\n",
      "Step = 414 train_loss: 0.025126453 val_loss: 0.025870498\n",
      "Step = 415 train_loss: 0.023476165 val_loss: 0.023860252\n",
      "Step = 416 train_loss: 0.024900775 val_loss: 0.026370432\n",
      "Step = 417 train_loss: 0.024985075 val_loss: 0.027827963\n",
      "Step = 418 train_loss: 0.0233029 val_loss: 0.028921304\n",
      "Step = 419 train_loss: 0.023328105 val_loss: 0.027731713\n",
      "Step = 420 train_loss: 0.026938872 val_loss: 0.019774284\n",
      "Step = 421 train_loss: 0.028440148 val_loss: 0.020283042\n",
      "Step = 422 train_loss: 0.026786773 val_loss: 0.029119542\n",
      "Step = 423 train_loss: 0.022451706 val_loss: 0.033401694\n",
      "Step = 424 train_loss: 0.026134096 val_loss: 0.031864267\n",
      "Step = 425 train_loss: 0.028250733 val_loss: 0.023805937\n",
      "Step = 426 train_loss: 0.02456711 val_loss: 0.019084604\n",
      "Step = 427 train_loss: 0.026935484 val_loss: 0.024769992\n",
      "Step = 428 train_loss: 0.024431018 val_loss: 0.032867823\n",
      "Step = 429 train_loss: 0.0228961 val_loss: 0.034207534\n",
      "Step = 430 train_loss: 0.024705859 val_loss: 0.030064931\n",
      "Step = 431 train_loss: 0.023873089 val_loss: 0.020805132\n",
      "Step = 432 train_loss: 0.024251921 val_loss: 0.017773354\n",
      "Step = 433 train_loss: 0.025990576 val_loss: 0.022996359\n",
      "Step = 434 train_loss: 0.024145117 val_loss: 0.03369534\n",
      "Step = 435 train_loss: 0.02644888 val_loss: 0.0351596\n",
      "Step = 436 train_loss: 0.025530891 val_loss: 0.030978493\n",
      "Step = 437 train_loss: 0.023345785 val_loss: 0.021571236\n",
      "Step = 438 train_loss: 0.024003427 val_loss: 0.015798414\n",
      "Step = 439 train_loss: 0.026577748 val_loss: 0.01585787\n",
      "Step = 440 train_loss: 0.027330628 val_loss: 0.022521013\n",
      "Step = 441 train_loss: 0.021724435 val_loss: 0.02892917\n",
      "Step = 442 train_loss: 0.021023354 val_loss: 0.029113375\n",
      "Step = 443 train_loss: 0.024232915 val_loss: 0.026492087\n",
      "Step = 444 train_loss: 0.022951461 val_loss: 0.02613373\n",
      "Step = 445 train_loss: 0.022063129 val_loss: 0.016618665\n",
      "Step = 446 train_loss: 0.024601381 val_loss: 0.015247114\n",
      "Step = 447 train_loss: 0.02254182 val_loss: 0.015448739\n",
      "Step = 448 train_loss: 0.023464484 val_loss: 0.01650408\n",
      "Step = 449 train_loss: 0.023643805 val_loss: 0.02695737\n",
      "Step = 450 train_loss: 0.023548353 val_loss: 0.0316657\n",
      "Step = 451 train_loss: 0.023311911 val_loss: 0.028326012\n",
      "Step = 452 train_loss: 0.022366993 val_loss: 0.020222053\n",
      "Step = 453 train_loss: 0.02364393 val_loss: 0.016380437\n",
      "Step = 454 train_loss: 0.01768625 val_loss: 0.015160594\n",
      "Step = 455 train_loss: 0.020657055 val_loss: 0.02054246\n",
      "Step = 456 train_loss: 0.020966679 val_loss: 0.021862382\n",
      "Step = 457 train_loss: 0.023532271 val_loss: 0.02231481\n",
      "Step = 458 train_loss: 0.020896556 val_loss: 0.021745613\n",
      "Step = 459 train_loss: 0.021845568 val_loss: 0.020891543\n",
      "Step = 460 train_loss: 0.02039472 val_loss: 0.021766946\n",
      "Step = 461 train_loss: 0.022981474 val_loss: 0.022766765\n",
      "Step = 462 train_loss: 0.022053966 val_loss: 0.022880867\n",
      "Step = 463 train_loss: 0.021925379 val_loss: 0.01849884\n",
      "Step = 464 train_loss: 0.021069027 val_loss: 0.013801401\n",
      "Step = 465 train_loss: 0.020692708 val_loss: 0.016292011\n",
      "Step = 466 train_loss: 0.020020654 val_loss: 0.018729448\n",
      "Step = 467 train_loss: 0.022395264 val_loss: 0.01997727\n",
      "Step = 468 train_loss: 0.020727038 val_loss: 0.019869255\n",
      "Step = 469 train_loss: 0.020429607 val_loss: 0.019042691\n",
      "Step = 470 train_loss: 0.019647608 val_loss: 0.018323394\n",
      "Step = 471 train_loss: 0.020064516 val_loss: 0.017683543\n",
      "Step = 472 train_loss: 0.021274557 val_loss: 0.01756524\n",
      "Step = 473 train_loss: 0.019768612 val_loss: 0.01641356\n",
      "Step = 474 train_loss: 0.019806575 val_loss: 0.015360098\n",
      "Step = 475 train_loss: 0.019971492 val_loss: 0.0132405395\n",
      "Step = 476 train_loss: 0.018613508 val_loss: 0.012534607\n",
      "Step = 477 train_loss: 0.01819663 val_loss: 0.012312461\n",
      "Step = 478 train_loss: 0.018419988 val_loss: 0.012024652\n",
      "Step = 479 train_loss: 0.018964434 val_loss: 0.01680574\n",
      "Step = 480 train_loss: 0.018602068 val_loss: 0.025621548\n",
      "Step = 481 train_loss: 0.019492794 val_loss: 0.03319007\n",
      "Step = 482 train_loss: 0.018812489 val_loss: 0.03774444\n",
      "Step = 483 train_loss: 0.018777987 val_loss: 0.039346207\n",
      "Step = 484 train_loss: 0.01968205 val_loss: 0.030236684\n",
      "Step = 485 train_loss: 0.021056835 val_loss: 0.012534634\n",
      "Step = 486 train_loss: 0.018571775 val_loss: 0.015657231\n",
      "Step = 487 train_loss: 0.018499656 val_loss: 0.014963593\n",
      "Step = 488 train_loss: 0.021225264 val_loss: 0.018220136\n",
      "Step = 489 train_loss: 0.0184453 val_loss: 0.038988788\n",
      "Step = 490 train_loss: 0.018005015 val_loss: 0.04403134\n",
      "Step = 491 train_loss: 0.021382857 val_loss: 0.04689762\n",
      "Step = 492 train_loss: 0.019133596 val_loss: 0.045836497\n",
      "Step = 493 train_loss: 0.019576188 val_loss: 0.03267475\n",
      "Step = 494 train_loss: 0.018722642 val_loss: 0.012221419\n",
      "Step = 495 train_loss: 0.01759879 val_loss: 0.01563431\n",
      "Step = 496 train_loss: 0.020891376 val_loss: 0.01581605\n",
      "Step = 497 train_loss: 0.020069804 val_loss: 0.012934229\n",
      "Step = 498 train_loss: 0.019030955 val_loss: 0.023066688\n",
      "Step = 499 train_loss: 0.02369075 val_loss: 0.044425167\n",
      "Step = 500 train_loss: 0.01748836 val_loss: 0.04297755\n",
      "Step = 501 train_loss: 0.014463202 val_loss: 0.023105055\n",
      "Step = 502 train_loss: 0.018308323 val_loss: 0.017605674\n",
      "Step = 503 train_loss: 0.022230186 val_loss: 0.018142894\n",
      "Step = 504 train_loss: 0.01846966 val_loss: 0.01931874\n",
      "Step = 505 train_loss: 0.018844854 val_loss: 0.02096232\n",
      "Step = 506 train_loss: 0.01836138 val_loss: 0.018455759\n",
      "Step = 507 train_loss: 0.018407198 val_loss: 0.015538594\n",
      "Step = 508 train_loss: 0.017706059 val_loss: 0.016593343\n",
      "Step = 509 train_loss: 0.016325114 val_loss: 0.016883094\n",
      "Step = 510 train_loss: 0.017794933 val_loss: 0.013506645\n",
      "Step = 511 train_loss: 0.018997462 val_loss: 0.014984225\n",
      "Step = 512 train_loss: 0.016807185 val_loss: 0.01710649\n",
      "Step = 513 train_loss: 0.019381395 val_loss: 0.011006933\n",
      "Step = 514 train_loss: 0.017408285 val_loss: 0.02855051\n",
      "Step = 515 train_loss: 0.017445927 val_loss: 0.037163083\n",
      "Step = 516 train_loss: 0.017859861 val_loss: 0.029128747\n",
      "Step = 517 train_loss: 0.016582612 val_loss: 0.011225053\n",
      "Step = 518 train_loss: 0.016584625 val_loss: 0.014784854\n",
      "Step = 519 train_loss: 0.015884353 val_loss: 0.011363156\n",
      "Step = 520 train_loss: 0.018511994 val_loss: 0.02005731\n",
      "Step = 521 train_loss: 0.016254723 val_loss: 0.03723819\n",
      "Step = 522 train_loss: 0.017896174 val_loss: 0.041473374\n",
      "Step = 523 train_loss: 0.01730767 val_loss: 0.034536686\n",
      "Step = 524 train_loss: 0.0189339 val_loss: 0.015188473\n",
      "Step = 525 train_loss: 0.016464973 val_loss: 0.011030244\n",
      "Step = 526 train_loss: 0.017113244 val_loss: 0.011022221\n",
      "Step = 527 train_loss: 0.017745774 val_loss: 0.025489632\n",
      "Step = 528 train_loss: 0.015597837 val_loss: 0.044189505\n",
      "Step = 529 train_loss: 0.015393008 val_loss: 0.049168672\n",
      "Step = 530 train_loss: 0.01779929 val_loss: 0.050232574\n",
      "Step = 531 train_loss: 0.02145357 val_loss: 0.045963135\n",
      "Step = 532 train_loss: 0.022332452 val_loss: 0.014147946\n",
      "Step = 533 train_loss: 0.015117354 val_loss: 0.017195877\n",
      "Step = 534 train_loss: 0.017885149 val_loss: 0.015650181\n",
      "Step = 535 train_loss: 0.017631566 val_loss: 0.010817926\n",
      "Step = 536 train_loss: 0.014663919 val_loss: 0.042714607\n",
      "Step = 537 train_loss: 0.014054543 val_loss: 0.049886975\n",
      "Step = 538 train_loss: 0.017182268 val_loss: 0.048933767\n",
      "Step = 539 train_loss: 0.017448299 val_loss: 0.03494191\n",
      "Step = 540 train_loss: 0.016196456 val_loss: 0.011675882\n",
      "Step = 541 train_loss: 0.015938932 val_loss: 0.011910563\n",
      "Step = 542 train_loss: 0.017977886 val_loss: 0.009236249\n",
      "Step = 543 train_loss: 0.015042326 val_loss: 0.034644693\n",
      "Step = 544 train_loss: 0.015422116 val_loss: 0.046243984\n",
      "Step = 545 train_loss: 0.014911698 val_loss: 0.047181737\n",
      "Step = 546 train_loss: 0.01447394 val_loss: 0.04464013\n",
      "Step = 547 train_loss: 0.013752173 val_loss: 0.0350913\n",
      "Step = 548 train_loss: 0.017139968 val_loss: 0.02559613\n",
      "Step = 549 train_loss: 0.017620739 val_loss: 0.012937424\n",
      "Step = 550 train_loss: 0.016610313 val_loss: 0.012751204\n",
      "Step = 551 train_loss: 0.014787129 val_loss: 0.028448187\n",
      "Step = 552 train_loss: 0.01387097 val_loss: 0.037831567\n",
      "Step = 553 train_loss: 0.014626725 val_loss: 0.042954344\n",
      "Step = 554 train_loss: 0.015185603 val_loss: 0.046102148\n",
      "Step = 555 train_loss: 0.014674317 val_loss: 0.04216143\n",
      "Step = 556 train_loss: 0.015698813 val_loss: 0.029893314\n",
      "Step = 557 train_loss: 0.01374662 val_loss: 0.020242926\n",
      "Step = 558 train_loss: 0.014084427 val_loss: 0.0139262965\n",
      "Step = 559 train_loss: 0.015105957 val_loss: 0.025681019\n",
      "Step = 560 train_loss: 0.01593109 val_loss: 0.03763327\n",
      "Step = 561 train_loss: 0.014971475 val_loss: 0.04163742\n",
      "Step = 562 train_loss: 0.015496915 val_loss: 0.041909512\n",
      "Step = 563 train_loss: 0.015041141 val_loss: 0.03187879\n",
      "Step = 564 train_loss: 0.013005664 val_loss: 0.019856246\n",
      "Step = 565 train_loss: 0.0132131465 val_loss: 0.012183457\n",
      "Step = 566 train_loss: 0.016292468 val_loss: 0.013536946\n",
      "Step = 567 train_loss: 0.015223032 val_loss: 0.034858577\n",
      "Step = 568 train_loss: 0.013007911 val_loss: 0.045793444\n",
      "Step = 569 train_loss: 0.014071127 val_loss: 0.048535988\n",
      "Step = 570 train_loss: 0.01460508 val_loss: 0.04724947\n",
      "Step = 571 train_loss: 0.015911117 val_loss: 0.030225312\n",
      "Step = 572 train_loss: 0.013679314 val_loss: 0.009020211\n",
      "Step = 573 train_loss: 0.01562556 val_loss: 0.015153737\n",
      "Step = 574 train_loss: 0.014549816 val_loss: 0.030556781\n",
      "Step = 575 train_loss: 0.013725627 val_loss: 0.040232103\n",
      "Step = 576 train_loss: 0.014260849 val_loss: 0.042321727\n",
      "Step = 577 train_loss: 0.015941797 val_loss: 0.025658395\n",
      "Step = 578 train_loss: 0.012226585 val_loss: 0.009816175\n",
      "Step = 579 train_loss: 0.015506536 val_loss: 0.013864373\n",
      "Step = 580 train_loss: 0.013258483 val_loss: 0.023525206\n",
      "Step = 581 train_loss: 0.013241664 val_loss: 0.02937519\n",
      "Step = 582 train_loss: 0.013130939 val_loss: 0.031465884\n",
      "Step = 583 train_loss: 0.014660542 val_loss: 0.030652273\n",
      "Step = 584 train_loss: 0.013093244 val_loss: 0.031214947\n",
      "Step = 585 train_loss: 0.012464992 val_loss: 0.036828764\n",
      "Step = 586 train_loss: 0.013706308 val_loss: 0.03491498\n",
      "Step = 587 train_loss: 0.013006021 val_loss: 0.027256228\n",
      "Step = 588 train_loss: 0.016286446 val_loss: 0.014295873\n",
      "Step = 589 train_loss: 0.012439933 val_loss: 0.009554052\n",
      "Step = 590 train_loss: 0.01650504 val_loss: 0.031956904\n",
      "Step = 591 train_loss: 0.0123756835 val_loss: 0.04633802\n",
      "Step = 592 train_loss: 0.014392222 val_loss: 0.047143545\n",
      "Step = 593 train_loss: 0.01572789 val_loss: 0.043495554\n",
      "Step = 594 train_loss: 0.014532046 val_loss: 0.03831145\n",
      "Step = 595 train_loss: 0.013641674 val_loss: 0.032146916\n",
      "Step = 596 train_loss: 0.013861904 val_loss: 0.026317125\n",
      "Step = 597 train_loss: 0.014057656 val_loss: 0.022862542\n",
      "Step = 598 train_loss: 0.014061419 val_loss: 0.023906754\n",
      "Step = 599 train_loss: 0.015216449 val_loss: 0.027820747\n",
      "Step = 600 train_loss: 0.012832178 val_loss: 0.03067428\n",
      "Step = 601 train_loss: 0.0142851425 val_loss: 0.029972205\n",
      "Step = 602 train_loss: 0.012535959 val_loss: 0.026933774\n",
      "Step = 603 train_loss: 0.011848632 val_loss: 0.024374649\n",
      "Step = 604 train_loss: 0.013281137 val_loss: 0.022645514\n",
      "Step = 605 train_loss: 0.014942982 val_loss: 0.022284226\n",
      "Step = 606 train_loss: 0.013132738 val_loss: 0.018584136\n",
      "Step = 607 train_loss: 0.013749606 val_loss: 0.013277489\n",
      "Step = 608 train_loss: 0.012223138 val_loss: 0.01219716\n",
      "Step = 609 train_loss: 0.015712505 val_loss: 0.03158949\n",
      "Step = 610 train_loss: 0.01299215 val_loss: 0.04980798\n",
      "Step = 611 train_loss: 0.01327161 val_loss: 0.05449569\n",
      "Step = 612 train_loss: 0.013103886 val_loss: 0.05523156\n",
      "Step = 613 train_loss: 0.01214325 val_loss: 0.054071248\n",
      "Step = 614 train_loss: 0.012606687 val_loss: 0.051003154\n",
      "Step = 615 train_loss: 0.0136191845 val_loss: 0.046787363\n",
      "Step = 616 train_loss: 0.0134253865 val_loss: 0.035303794\n",
      "Step = 617 train_loss: 0.013660595 val_loss: 0.020637594\n",
      "Step = 618 train_loss: 0.013874167 val_loss: 0.008968848\n",
      "Step = 619 train_loss: 0.013728865 val_loss: 0.008313279\n",
      "Step = 620 train_loss: 0.011822454 val_loss: 0.0100687025\n",
      "Step = 621 train_loss: 0.013596365 val_loss: 0.01456821\n",
      "Step = 622 train_loss: 0.013464525 val_loss: 0.023592647\n",
      "Step = 623 train_loss: 0.014488242 val_loss: 0.023235952\n",
      "Step = 624 train_loss: 0.013680235 val_loss: 0.034761477\n",
      "Step = 625 train_loss: 0.0143430345 val_loss: 0.05036428\n",
      "Step = 626 train_loss: 0.011815845 val_loss: 0.05255619\n",
      "Step = 627 train_loss: 0.013185197 val_loss: 0.052344367\n",
      "Step = 628 train_loss: 0.014202055 val_loss: 0.048533916\n",
      "Step = 629 train_loss: 0.012570224 val_loss: 0.042072523\n",
      "Step = 630 train_loss: 0.011462019 val_loss: 0.033778608\n",
      "Step = 631 train_loss: 0.011491781 val_loss: 0.029028622\n",
      "Step = 632 train_loss: 0.013602235 val_loss: 0.02916376\n",
      "Step = 633 train_loss: 0.0120618185 val_loss: 0.021871302\n",
      "Step = 634 train_loss: 0.012928373 val_loss: 0.018133119\n",
      "Step = 635 train_loss: 0.011954169 val_loss: 0.015077257\n",
      "Step = 636 train_loss: 0.013731844 val_loss: 0.016978065\n",
      "Step = 637 train_loss: 0.012845477 val_loss: 0.018903796\n",
      "Step = 638 train_loss: 0.01403825 val_loss: 0.01539892\n",
      "Step = 639 train_loss: 0.013643182 val_loss: 0.010500348\n",
      "Step = 640 train_loss: 0.014772871 val_loss: 0.011833981\n",
      "Step = 641 train_loss: 0.012851604 val_loss: 0.01263088\n",
      "Step = 642 train_loss: 0.012990026 val_loss: 0.01223615\n",
      "Step = 643 train_loss: 0.014562925 val_loss: 0.01731339\n",
      "Step = 644 train_loss: 0.0143365925 val_loss: 0.03801831\n",
      "Step = 645 train_loss: 0.017353985 val_loss: 0.056231022\n",
      "Step = 646 train_loss: 0.017810434 val_loss: 0.058905333\n",
      "Step = 647 train_loss: 0.013995324 val_loss: 0.05601384\n",
      "Step = 648 train_loss: 0.014958307 val_loss: 0.041886337\n",
      "Step = 649 train_loss: 0.014693052 val_loss: 0.010499679\n",
      "Step = 650 train_loss: 0.012988599 val_loss: 0.026984788\n",
      "Step = 651 train_loss: 0.014425808 val_loss: 0.027589353\n",
      "Step = 652 train_loss: 0.01316531 val_loss: 0.025576504\n",
      "Step = 653 train_loss: 0.012700225 val_loss: 0.023302417\n",
      "Step = 654 train_loss: 0.012395049 val_loss: 0.021392725\n",
      "Step = 655 train_loss: 0.014811055 val_loss: 0.020150188\n",
      "Step = 656 train_loss: 0.016755693 val_loss: 0.021580692\n",
      "Step = 657 train_loss: 0.012171204 val_loss: 0.02506928\n",
      "Step = 658 train_loss: 0.011287058 val_loss: 0.026341476\n",
      "Step = 659 train_loss: 0.012462299 val_loss: 0.02505233\n",
      "Step = 660 train_loss: 0.012645638 val_loss: 0.017891504\n",
      "Step = 661 train_loss: 0.011900371 val_loss: 0.011355021\n",
      "Step = 662 train_loss: 0.013389996 val_loss: 0.00947796\n",
      "Step = 663 train_loss: 0.01295204 val_loss: 0.0093233995\n",
      "Step = 664 train_loss: 0.012615988 val_loss: 0.008070981\n",
      "Step = 665 train_loss: 0.016301554 val_loss: 0.008671831\n",
      "Step = 666 train_loss: 0.011877726 val_loss: 0.008921135\n",
      "Step = 667 train_loss: 0.012143759 val_loss: 0.010140758\n",
      "Step = 668 train_loss: 0.014120575 val_loss: 0.00774978\n",
      "Step = 669 train_loss: 0.013530503 val_loss: 0.020778216\n",
      "Step = 670 train_loss: 0.012109672 val_loss: 0.036001038\n",
      "Step = 671 train_loss: 0.014160707 val_loss: 0.0354788\n",
      "Step = 672 train_loss: 0.014021308 val_loss: 0.020118745\n",
      "Step = 673 train_loss: 0.011618054 val_loss: 0.00963138\n",
      "Step = 674 train_loss: 0.0134492805 val_loss: 0.008535708\n",
      "Step = 675 train_loss: 0.014285734 val_loss: 0.010993013\n",
      "Step = 676 train_loss: 0.012684563 val_loss: 0.008055242\n",
      "Step = 677 train_loss: 0.011328925 val_loss: 0.0127909575\n",
      "Step = 678 train_loss: 0.01195248 val_loss: 0.027065583\n",
      "Step = 679 train_loss: 0.013189991 val_loss: 0.026642794\n",
      "Step = 680 train_loss: 0.012299688 val_loss: 0.012091198\n",
      "Step = 681 train_loss: 0.012961602 val_loss: 0.009050684\n",
      "Step = 682 train_loss: 0.012252276 val_loss: 0.007792643\n",
      "Step = 683 train_loss: 0.0137819145 val_loss: 0.010796242\n",
      "Step = 684 train_loss: 0.011581461 val_loss: 0.014425097\n",
      "Step = 685 train_loss: 0.013533708 val_loss: 0.015564862\n",
      "Step = 686 train_loss: 0.012503422 val_loss: 0.014223028\n",
      "Step = 687 train_loss: 0.012958477 val_loss: 0.015626026\n",
      "Step = 688 train_loss: 0.013867665 val_loss: 0.01159925\n",
      "Step = 689 train_loss: 0.013040647 val_loss: 0.009257523\n",
      "Step = 690 train_loss: 0.012794853 val_loss: 0.009076924\n",
      "Step = 691 train_loss: 0.014908956 val_loss: 0.008233144\n",
      "Step = 692 train_loss: 0.011667032 val_loss: 0.0106814755\n",
      "Step = 693 train_loss: 0.013402434 val_loss: 0.021040523\n",
      "Step = 694 train_loss: 0.013058862 val_loss: 0.022809133\n",
      "Step = 695 train_loss: 0.011846917 val_loss: 0.021768361\n",
      "Step = 696 train_loss: 0.012820287 val_loss: 0.01986294\n",
      "Step = 697 train_loss: 0.012315035 val_loss: 0.01794375\n",
      "Step = 698 train_loss: 0.011279209 val_loss: 0.023729127\n",
      "Step = 699 train_loss: 0.014596015 val_loss: 0.027903058\n",
      "Step = 700 train_loss: 0.011304955 val_loss: 0.03300686\n",
      "Step = 701 train_loss: 0.011742591 val_loss: 0.03683957\n",
      "Step = 702 train_loss: 0.013771926 val_loss: 0.0296944\n",
      "Step = 703 train_loss: 0.011641948 val_loss: 0.023401206\n",
      "Step = 704 train_loss: 0.013170733 val_loss: 0.014898181\n",
      "Step = 705 train_loss: 0.011315765 val_loss: 0.015824415\n",
      "Step = 706 train_loss: 0.01225319 val_loss: 0.011753328\n",
      "Step = 707 train_loss: 0.013448029 val_loss: 0.010442162\n",
      "Step = 708 train_loss: 0.013753937 val_loss: 0.024463592\n",
      "Step = 709 train_loss: 0.0115419235 val_loss: 0.037481867\n",
      "Step = 710 train_loss: 0.013772279 val_loss: 0.043245837\n",
      "Step = 711 train_loss: 0.012449772 val_loss: 0.0456697\n",
      "Step = 712 train_loss: 0.012058436 val_loss: 0.04042333\n",
      "Step = 713 train_loss: 0.014214743 val_loss: 0.023625849\n",
      "Step = 714 train_loss: 0.012254651 val_loss: 0.009234599\n",
      "Step = 715 train_loss: 0.011295384 val_loss: 0.008460179\n",
      "Step = 716 train_loss: 0.010837936 val_loss: 0.009482456\n",
      "Step = 717 train_loss: 0.014230315 val_loss: 0.008516933\n",
      "Step = 718 train_loss: 0.011249802 val_loss: 0.013477989\n",
      "Step = 719 train_loss: 0.01195759 val_loss: 0.03020213\n",
      "Step = 720 train_loss: 0.010861978 val_loss: 0.043080445\n",
      "Step = 721 train_loss: 0.011818634 val_loss: 0.048814733\n",
      "Step = 722 train_loss: 0.012805965 val_loss: 0.051413734\n",
      "Step = 723 train_loss: 0.013591496 val_loss: 0.0432884\n",
      "Step = 724 train_loss: 0.015148877 val_loss: 0.018663196\n",
      "Step = 725 train_loss: 0.013567225 val_loss: 0.010911039\n",
      "Step = 726 train_loss: 0.013033604 val_loss: 0.013895068\n",
      "Step = 727 train_loss: 0.013323318 val_loss: 0.0076607363\n",
      "Step = 728 train_loss: 0.012306275 val_loss: 0.029858641\n",
      "Step = 729 train_loss: 0.011619055 val_loss: 0.04915957\n",
      "Step = 730 train_loss: 0.014154861 val_loss: 0.050979596\n",
      "Step = 731 train_loss: 0.012375743 val_loss: 0.046657093\n",
      "Step = 732 train_loss: 0.014236375 val_loss: 0.027350213\n",
      "Step = 733 train_loss: 0.011338515 val_loss: 0.009535811\n",
      "Step = 734 train_loss: 0.012287805 val_loss: 0.010916265\n",
      "Step = 735 train_loss: 0.013965767 val_loss: 0.0104653025\n",
      "Step = 736 train_loss: 0.013419068 val_loss: 0.011331066\n",
      "Step = 737 train_loss: 0.012405901 val_loss: 0.028259557\n",
      "Step = 738 train_loss: 0.012111912 val_loss: 0.03560186\n",
      "Step = 739 train_loss: 0.011838467 val_loss: 0.033101782\n",
      "Step = 740 train_loss: 0.011724273 val_loss: 0.028618295\n",
      "Step = 741 train_loss: 0.011604167 val_loss: 0.02003092\n",
      "Step = 742 train_loss: 0.013711729 val_loss: 0.0094853705\n",
      "Step = 743 train_loss: 0.011717904 val_loss: 0.008614531\n",
      "Step = 744 train_loss: 0.012838971 val_loss: 0.008014792\n",
      "Step = 745 train_loss: 0.013352219 val_loss: 0.00914024\n",
      "Step = 746 train_loss: 0.014654398 val_loss: 0.0099739935\n",
      "Step = 747 train_loss: 0.012309849 val_loss: 0.025874045\n",
      "Step = 748 train_loss: 0.013204471 val_loss: 0.039340593\n",
      "Step = 749 train_loss: 0.011668244 val_loss: 0.043566763\n",
      "Step = 750 train_loss: 0.011731962 val_loss: 0.044935282\n",
      "Step = 751 train_loss: 0.012414397 val_loss: 0.03808206\n",
      "Step = 752 train_loss: 0.014318622 val_loss: 0.025909511\n",
      "Step = 753 train_loss: 0.012501443 val_loss: 0.011020602\n",
      "Step = 754 train_loss: 0.012791804 val_loss: 0.007937872\n",
      "Step = 755 train_loss: 0.012043835 val_loss: 0.008519345\n",
      "Step = 756 train_loss: 0.012527953 val_loss: 0.008005387\n",
      "Step = 757 train_loss: 0.012138267 val_loss: 0.010348988\n",
      "Step = 758 train_loss: 0.012566526 val_loss: 0.01924079\n",
      "Step = 759 train_loss: 0.010581412 val_loss: 0.022677524\n",
      "Step = 760 train_loss: 0.01217281 val_loss: 0.0164707\n",
      "Step = 761 train_loss: 0.011935415 val_loss: 0.016615774\n",
      "Step = 762 train_loss: 0.011618318 val_loss: 0.013069573\n",
      "Step = 763 train_loss: 0.011394786 val_loss: 0.00895294\n",
      "Step = 764 train_loss: 0.0114844 val_loss: 0.010116713\n",
      "Step = 765 train_loss: 0.011983558 val_loss: 0.015875783\n",
      "Step = 766 train_loss: 0.010672551 val_loss: 0.014463938\n",
      "Step = 767 train_loss: 0.01225902 val_loss: 0.016359987\n",
      "Step = 768 train_loss: 0.01149671 val_loss: 0.01417251\n",
      "Step = 769 train_loss: 0.012378312 val_loss: 0.01145589\n",
      "Step = 770 train_loss: 0.011804419 val_loss: 0.009543829\n",
      "Step = 771 train_loss: 0.011841576 val_loss: 0.012935897\n",
      "Step = 772 train_loss: 0.0122481575 val_loss: 0.01211939\n",
      "Step = 773 train_loss: 0.011367824 val_loss: 0.009363041\n",
      "Step = 774 train_loss: 0.011339595 val_loss: 0.014096702\n",
      "Step = 775 train_loss: 0.01265645 val_loss: 0.025309606\n",
      "Step = 776 train_loss: 0.013074277 val_loss: 0.034601826\n",
      "Step = 777 train_loss: 0.012192903 val_loss: 0.03531459\n",
      "Step = 778 train_loss: 0.014173702 val_loss: 0.027107675\n",
      "Step = 779 train_loss: 0.011649898 val_loss: 0.015281219\n",
      "Step = 780 train_loss: 0.011282478 val_loss: 0.019253667\n",
      "Step = 781 train_loss: 0.01242326 val_loss: 0.029525092\n",
      "Step = 782 train_loss: 0.0113757495 val_loss: 0.032614384\n",
      "Step = 783 train_loss: 0.011592146 val_loss: 0.037250873\n",
      "Step = 784 train_loss: 0.011511569 val_loss: 0.041208576\n",
      "Step = 785 train_loss: 0.012116086 val_loss: 0.03889799\n",
      "Step = 786 train_loss: 0.010428954 val_loss: 0.03187693\n",
      "Step = 787 train_loss: 0.011707673 val_loss: 0.020587604\n",
      "Step = 788 train_loss: 0.011070932 val_loss: 0.009562451\n",
      "Step = 789 train_loss: 0.011239083 val_loss: 0.008043668\n",
      "Step = 790 train_loss: 0.012924504 val_loss: 0.017237607\n",
      "Step = 791 train_loss: 0.012610374 val_loss: 0.043889567\n",
      "Step = 792 train_loss: 0.014011631 val_loss: 0.055242423\n",
      "Step = 793 train_loss: 0.011967108 val_loss: 0.057338435\n",
      "Step = 794 train_loss: 0.0127700055 val_loss: 0.052550837\n",
      "Step = 795 train_loss: 0.010601516 val_loss: 0.0416415\n",
      "Step = 796 train_loss: 0.012012505 val_loss: 0.035376366\n",
      "Step = 797 train_loss: 0.011263096 val_loss: 0.02721956\n",
      "Step = 798 train_loss: 0.010816377 val_loss: 0.021739617\n",
      "Step = 799 train_loss: 0.012647032 val_loss: 0.020453237\n",
      "Step = 800 train_loss: 0.012935986 val_loss: 0.03374441\n",
      "Step = 801 train_loss: 0.011061231 val_loss: 0.041280225\n",
      "Step = 802 train_loss: 0.012802137 val_loss: 0.04286937\n",
      "Step = 803 train_loss: 0.011260972 val_loss: 0.032681838\n",
      "Step = 804 train_loss: 0.0109671885 val_loss: 0.024117777\n",
      "Step = 805 train_loss: 0.01136196 val_loss: 0.012826419\n",
      "Step = 806 train_loss: 0.011915171 val_loss: 0.0074804104\n",
      "Step = 807 train_loss: 0.012041224 val_loss: 0.0074890344\n",
      "Step = 808 train_loss: 0.011475682 val_loss: 0.0076829153\n",
      "Step = 809 train_loss: 0.01229682 val_loss: 0.010105191\n",
      "Step = 810 train_loss: 0.012437116 val_loss: 0.018502887\n",
      "Step = 811 train_loss: 0.0128674945 val_loss: 0.03561075\n",
      "Step = 812 train_loss: 0.013080622 val_loss: 0.044459414\n",
      "Step = 813 train_loss: 0.013185559 val_loss: 0.041532874\n",
      "Step = 814 train_loss: 0.014321764 val_loss: 0.031231642\n",
      "Step = 815 train_loss: 0.012107388 val_loss: 0.01248327\n",
      "Step = 816 train_loss: 0.011114352 val_loss: 0.008112536\n",
      "Step = 817 train_loss: 0.013620769 val_loss: 0.010666785\n",
      "Step = 818 train_loss: 0.010987253 val_loss: 0.01174884\n",
      "Step = 819 train_loss: 0.0107472325 val_loss: 0.021812852\n",
      "Step = 820 train_loss: 0.010388941 val_loss: 0.035309263\n",
      "Step = 821 train_loss: 0.010736181 val_loss: 0.043452375\n",
      "Step = 822 train_loss: 0.012643531 val_loss: 0.04662701\n",
      "Step = 823 train_loss: 0.011386693 val_loss: 0.050400373\n",
      "Step = 824 train_loss: 0.012818278 val_loss: 0.048023496\n",
      "Step = 825 train_loss: 0.012340157 val_loss: 0.04016551\n",
      "Step = 826 train_loss: 0.011988065 val_loss: 0.02843284\n",
      "Step = 827 train_loss: 0.012084609 val_loss: 0.025024703\n",
      "Step = 828 train_loss: 0.011193017 val_loss: 0.022130288\n",
      "Step = 829 train_loss: 0.013124449 val_loss: 0.022258379\n",
      "Step = 830 train_loss: 0.0122030275 val_loss: 0.031090224\n",
      "Step = 831 train_loss: 0.0115259355 val_loss: 0.033694696\n",
      "Step = 832 train_loss: 0.011311527 val_loss: 0.036788072\n",
      "Step = 833 train_loss: 0.012651177 val_loss: 0.03686199\n",
      "Step = 834 train_loss: 0.01181298 val_loss: 0.03051735\n",
      "Step = 835 train_loss: 0.011296072 val_loss: 0.029574575\n",
      "Step = 836 train_loss: 0.013172207 val_loss: 0.01573922\n",
      "Step = 837 train_loss: 0.011149803 val_loss: 0.010413096\n",
      "Step = 838 train_loss: 0.012231434 val_loss: 0.009974246\n",
      "Step = 839 train_loss: 0.011530661 val_loss: 0.009800289\n",
      "Step = 840 train_loss: 0.011605381 val_loss: 0.0198775\n",
      "Step = 841 train_loss: 0.011629871 val_loss: 0.030253073\n",
      "Step = 842 train_loss: 0.011232917 val_loss: 0.031400263\n",
      "Step = 843 train_loss: 0.010841439 val_loss: 0.02603257\n",
      "Step = 844 train_loss: 0.012016133 val_loss: 0.012338413\n",
      "Step = 845 train_loss: 0.011222316 val_loss: 0.007685225\n",
      "Step = 846 train_loss: 0.011052157 val_loss: 0.0080517875\n",
      "Step = 847 train_loss: 0.012683237 val_loss: 0.009760395\n",
      "Step = 848 train_loss: 0.011174722 val_loss: 0.022315752\n",
      "Step = 849 train_loss: 0.011580652 val_loss: 0.038338896\n",
      "Step = 850 train_loss: 0.01161981 val_loss: 0.04492886\n",
      "Step = 851 train_loss: 0.012653528 val_loss: 0.044518862\n",
      "Step = 852 train_loss: 0.012043351 val_loss: 0.030130055\n",
      "Step = 853 train_loss: 0.01289121 val_loss: 0.01402859\n",
      "Step = 854 train_loss: 0.011573796 val_loss: 0.0077557075\n",
      "Step = 855 train_loss: 0.012620687 val_loss: 0.007590776\n",
      "Step = 856 train_loss: 0.011397679 val_loss: 0.0077412534\n",
      "Step = 857 train_loss: 0.011450212 val_loss: 0.010513634\n",
      "Step = 858 train_loss: 0.010171006 val_loss: 0.020460952\n",
      "Step = 859 train_loss: 0.0125083495 val_loss: 0.028177567\n",
      "Step = 860 train_loss: 0.0114726145 val_loss: 0.0283324\n",
      "Step = 861 train_loss: 0.01275975 val_loss: 0.02301062\n",
      "Step = 862 train_loss: 0.011545359 val_loss: 0.02135721\n",
      "Step = 863 train_loss: 0.011528233 val_loss: 0.02181089\n",
      "Step = 864 train_loss: 0.010897202 val_loss: 0.02324604\n",
      "Step = 865 train_loss: 0.011348967 val_loss: 0.0313494\n",
      "Step = 866 train_loss: 0.012213401 val_loss: 0.035172448\n",
      "Step = 867 train_loss: 0.012176314 val_loss: 0.02934232\n",
      "Step = 868 train_loss: 0.011130745 val_loss: 0.021560652\n",
      "Step = 869 train_loss: 0.010923141 val_loss: 0.015955357\n",
      "Step = 870 train_loss: 0.010814958 val_loss: 0.013223094\n",
      "Step = 871 train_loss: 0.011429708 val_loss: 0.00868607\n",
      "Step = 872 train_loss: 0.010733912 val_loss: 0.008758267\n",
      "Step = 873 train_loss: 0.012136982 val_loss: 0.011075086\n",
      "Step = 874 train_loss: 0.011453489 val_loss: 0.022922585\n",
      "Step = 875 train_loss: 0.010726999 val_loss: 0.032606866\n",
      "Step = 876 train_loss: 0.012328231 val_loss: 0.029290514\n",
      "Step = 877 train_loss: 0.0112022115 val_loss: 0.031038586\n",
      "Step = 878 train_loss: 0.011084039 val_loss: 0.03530605\n",
      "Step = 879 train_loss: 0.011419121 val_loss: 0.044816885\n",
      "Step = 880 train_loss: 0.012117712 val_loss: 0.045586918\n",
      "Step = 881 train_loss: 0.011089498 val_loss: 0.040281396\n",
      "Step = 882 train_loss: 0.010291512 val_loss: 0.036029704\n",
      "Step = 883 train_loss: 0.009809677 val_loss: 0.029785788\n",
      "Step = 884 train_loss: 0.01041177 val_loss: 0.02812601\n",
      "Step = 885 train_loss: 0.011903657 val_loss: 0.025354551\n",
      "Step = 886 train_loss: 0.011603013 val_loss: 0.024400478\n",
      "Step = 887 train_loss: 0.0098846 val_loss: 0.017114045\n",
      "Step = 888 train_loss: 0.011361309 val_loss: 0.014546373\n",
      "Step = 889 train_loss: 0.011188022 val_loss: 0.01192443\n",
      "Step = 890 train_loss: 0.010673391 val_loss: 0.015316654\n",
      "Step = 891 train_loss: 0.011300242 val_loss: 0.021768944\n",
      "Step = 892 train_loss: 0.013441334 val_loss: 0.02038951\n",
      "Step = 893 train_loss: 0.011504605 val_loss: 0.0121176755\n",
      "Step = 894 train_loss: 0.010709599 val_loss: 0.010216526\n",
      "Step = 895 train_loss: 0.010799946 val_loss: 0.01088905\n",
      "Step = 896 train_loss: 0.011557573 val_loss: 0.019188015\n",
      "Step = 897 train_loss: 0.011915288 val_loss: 0.029516349\n",
      "Step = 898 train_loss: 0.0106337555 val_loss: 0.037389815\n",
      "Step = 899 train_loss: 0.010764796 val_loss: 0.04611516\n",
      "Step = 900 train_loss: 0.01140045 val_loss: 0.04672775\n",
      "Step = 901 train_loss: 0.012263598 val_loss: 0.044069793\n",
      "Step = 902 train_loss: 0.01046985 val_loss: 0.03707964\n",
      "Step = 903 train_loss: 0.009724566 val_loss: 0.028132575\n",
      "Step = 904 train_loss: 0.0107676685 val_loss: 0.015488635\n",
      "Step = 905 train_loss: 0.011144123 val_loss: 0.0090039475\n",
      "Step = 906 train_loss: 0.011505764 val_loss: 0.0073455186\n",
      "Step = 907 train_loss: 0.010713162 val_loss: 0.008018972\n",
      "Step = 908 train_loss: 0.011501974 val_loss: 0.01256724\n",
      "Step = 909 train_loss: 0.010259688 val_loss: 0.018508444\n",
      "Step = 910 train_loss: 0.011004196 val_loss: 0.021584282\n",
      "Step = 911 train_loss: 0.011487944 val_loss: 0.017251415\n",
      "Step = 912 train_loss: 0.010408982 val_loss: 0.016994333\n",
      "Step = 913 train_loss: 0.012541432 val_loss: 0.0129411435\n",
      "Step = 914 train_loss: 0.009900451 val_loss: 0.008796139\n",
      "Step = 915 train_loss: 0.010539825 val_loss: 0.010132638\n",
      "Step = 916 train_loss: 0.011721153 val_loss: 0.016281018\n",
      "Step = 917 train_loss: 0.010923231 val_loss: 0.017625114\n",
      "Step = 918 train_loss: 0.01341481 val_loss: 0.023664571\n",
      "Step = 919 train_loss: 0.011549554 val_loss: 0.027155144\n",
      "Step = 920 train_loss: 0.011084691 val_loss: 0.029400697\n",
      "Step = 921 train_loss: 0.011536927 val_loss: 0.032617945\n",
      "Step = 922 train_loss: 0.010575101 val_loss: 0.026600024\n",
      "Step = 923 train_loss: 0.011690412 val_loss: 0.017009143\n",
      "Step = 924 train_loss: 0.009602049 val_loss: 0.010061019\n",
      "Step = 925 train_loss: 0.010861654 val_loss: 0.0077407965\n",
      "Step = 926 train_loss: 0.01113402 val_loss: 0.0073437085\n",
      "Step = 927 train_loss: 0.01125372 val_loss: 0.008167658\n",
      "Step = 928 train_loss: 0.011413111 val_loss: 0.02078813\n",
      "Step = 929 train_loss: 0.011734137 val_loss: 0.03606486\n",
      "Step = 930 train_loss: 0.01042584 val_loss: 0.045732774\n",
      "Step = 931 train_loss: 0.010193578 val_loss: 0.050426327\n",
      "Step = 932 train_loss: 0.009868907 val_loss: 0.050878525\n",
      "Step = 933 train_loss: 0.011120283 val_loss: 0.0474247\n",
      "Step = 934 train_loss: 0.011445447 val_loss: 0.040231608\n",
      "Step = 935 train_loss: 0.010576253 val_loss: 0.03390033\n",
      "Step = 936 train_loss: 0.01088478 val_loss: 0.026518356\n",
      "Step = 937 train_loss: 0.01270571 val_loss: 0.014825266\n",
      "Step = 938 train_loss: 0.010505334 val_loss: 0.010082202\n",
      "Step = 939 train_loss: 0.011322498 val_loss: 0.007482669\n",
      "Step = 940 train_loss: 0.011790598 val_loss: 0.007518319\n",
      "Step = 941 train_loss: 0.0111472905 val_loss: 0.0074896375\n",
      "Step = 942 train_loss: 0.00993199 val_loss: 0.0071243835\n",
      "Step = 943 train_loss: 0.012967451 val_loss: 0.007165926\n",
      "Step = 944 train_loss: 0.012153591 val_loss: 0.007762222\n",
      "Step = 945 train_loss: 0.010608623 val_loss: 0.009056058\n",
      "Step = 946 train_loss: 0.011902185 val_loss: 0.013399034\n",
      "Step = 947 train_loss: 0.014538729 val_loss: 0.0075983745\n",
      "Step = 948 train_loss: 0.011301247 val_loss: 0.0389788\n",
      "Step = 949 train_loss: 0.011815753 val_loss: 0.059491362\n",
      "Step = 950 train_loss: 0.011794437 val_loss: 0.060484007\n",
      "Step = 951 train_loss: 0.011254262 val_loss: 0.057434514\n",
      "Step = 952 train_loss: 0.011418493 val_loss: 0.04931543\n",
      "Step = 953 train_loss: 0.010434018 val_loss: 0.03400366\n",
      "Step = 954 train_loss: 0.010105547 val_loss: 0.017369201\n",
      "Step = 955 train_loss: 0.011154006 val_loss: 0.013256313\n",
      "Step = 956 train_loss: 0.011344201 val_loss: 0.01666849\n",
      "Step = 957 train_loss: 0.0108411545 val_loss: 0.030534822\n",
      "Step = 958 train_loss: 0.010319863 val_loss: 0.034699522\n",
      "Step = 959 train_loss: 0.010275817 val_loss: 0.04067997\n",
      "Step = 960 train_loss: 0.01076155 val_loss: 0.040675767\n",
      "Step = 961 train_loss: 0.009881108 val_loss: 0.039290782\n",
      "Step = 962 train_loss: 0.012109978 val_loss: 0.022665879\n",
      "Step = 963 train_loss: 0.011087438 val_loss: 0.013325795\n",
      "Step = 964 train_loss: 0.010230714 val_loss: 0.010040121\n",
      "Step = 965 train_loss: 0.010316332 val_loss: 0.0072744316\n",
      "Step = 966 train_loss: 0.010456864 val_loss: 0.00781876\n",
      "Step = 967 train_loss: 0.010366337 val_loss: 0.011060948\n",
      "Step = 968 train_loss: 0.010724572 val_loss: 0.010912299\n",
      "Step = 969 train_loss: 0.011100078 val_loss: 0.009848626\n",
      "Step = 970 train_loss: 0.010534366 val_loss: 0.0074711386\n",
      "Step = 971 train_loss: 0.0098702265 val_loss: 0.008326736\n",
      "Step = 972 train_loss: 0.011092963 val_loss: 0.007867869\n",
      "Step = 973 train_loss: 0.010427043 val_loss: 0.010066031\n",
      "Step = 974 train_loss: 0.011043403 val_loss: 0.017675575\n",
      "Step = 975 train_loss: 0.011006289 val_loss: 0.028241387\n",
      "Step = 976 train_loss: 0.01103119 val_loss: 0.02918955\n",
      "Step = 977 train_loss: 0.012028583 val_loss: 0.010200529\n",
      "Step = 978 train_loss: 0.012036405 val_loss: 0.017339306\n",
      "Step = 979 train_loss: 0.011370695 val_loss: 0.023788804\n",
      "Step = 980 train_loss: 0.010867788 val_loss: 0.02374538\n",
      "Step = 981 train_loss: 0.011383862 val_loss: 0.022331705\n",
      "Step = 982 train_loss: 0.012109862 val_loss: 0.02034813\n",
      "Step = 983 train_loss: 0.014295885 val_loss: 0.018311014\n",
      "Step = 984 train_loss: 0.012176564 val_loss: 0.015932186\n",
      "Step = 985 train_loss: 0.010983123 val_loss: 0.014104799\n",
      "Step = 986 train_loss: 0.010274293 val_loss: 0.013173536\n",
      "Step = 987 train_loss: 0.011566953 val_loss: 0.014253423\n",
      "Step = 988 train_loss: 0.010619893 val_loss: 0.015739579\n",
      "Step = 989 train_loss: 0.011290098 val_loss: 0.015180932\n",
      "Step = 990 train_loss: 0.009965091 val_loss: 0.012907424\n",
      "Step = 991 train_loss: 0.010732747 val_loss: 0.008920655\n",
      "Step = 992 train_loss: 0.011626815 val_loss: 0.0088026\n",
      "Step = 993 train_loss: 0.012016066 val_loss: 0.010466878\n",
      "Step = 994 train_loss: 0.010576675 val_loss: 0.007549369\n",
      "Step = 995 train_loss: 0.011327168 val_loss: 0.010219326\n",
      "Step = 996 train_loss: 0.009603648 val_loss: 0.017190143\n",
      "Step = 997 train_loss: 0.011419107 val_loss: 0.017144257\n",
      "Step = 998 train_loss: 0.012260867 val_loss: 0.007977737\n",
      "Step = 999 train_loss: 0.011737417 val_loss: 0.0191493\n",
      "942\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "# summary(net,(1,100,1000))\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.00001)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "val_losses=[]\n",
    "\n",
    "# 用于保存模型的最大数量\n",
    "max_models_to_keep = 10\n",
    "saved_models = []\n",
    "\n",
    "for t in range(1000):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 保存模型\n",
    "    model_path = 'Target_model/net_parameters'+str(t)+'.pkl'\n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, model_path)\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print('Step = %d' % t, 'train_loss:', train_loss.data.numpy(), 'val_loss:', val_loss.data.numpy())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    # 添加当前模型及其验证损失\n",
    "    saved_models.append((val_loss.item(), model_path))\n",
    "\n",
    "    # 保持模型数量不超过max_models_to_keep\n",
    "    if len(saved_models) > max_models_to_keep:\n",
    "        # 找到验证损失最大的一组模型并删除\n",
    "        saved_models.sort(key=lambda x: x[0])  # 排序，根据损失\n",
    "        os.remove(saved_models.pop()[1])  # 删除损失最大的模型\n",
    "\n",
    "# 'saved_models' 中现在只包含验证损失最小的前十个模型\n",
    "'''选择损失最小的模型'''\n",
    "best_index = val_losses.index(np.min(val_losses))\n",
    "print(best_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_14204\\399239800.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_14204\\399239800.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''复制验证损失最小的模型到最佳模型文件夹'''\n",
    "if 1:\n",
    "    shutil.copyfile('Target_model/net_parameters'+str(best_index)+'.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "\n",
    "'''重新加载最佳模型''' \n",
    "model = Net()\n",
    "a = model.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['model_state_dict'])\n",
    "optimizer.load_state_dict(torch.load('Best_target_model/net_parameters.pkl')['optimizer_state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.14422457\n",
      "mpe_val: 0.14114408\n",
      "mpe_a: 0.20557648100235199\n",
      "mpe_b: 0.17040967580563987\n",
      "rmse_train: 220.38603\n",
      "rmse_val: 188.5166\n",
      "rmse_a: 248.54937275654996\n",
      "rmse_b: 245.42702581419186\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.6651217341423035\n",
      "r2_val: 0.6585894227027893\n",
      "r2_a: 0.5952010329940542\n",
      "r2_b: 0.2590036137604501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
