{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型效果\n",
    "\n",
    "r2_train: 0.9062628746032715\n",
    "r2_val: 0.8613404631614685\n",
    "r2_a: 0.9024388593844427\n",
    "r2_b: 0.5164209698966113\n",
    "\n",
    "对比（非lstm架构）\n",
    "r2_train: 0.9947811961174011\n",
    "r2_val: 0.9749422669410706\n",
    "r2_a: 0.9464100454957816\n",
    "r2_b: 0.6836000724611211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "'''Load data'''\n",
    "temp_Qd = np.load('Data_processed/Qd_100.npy',allow_pickle=True).item()\n",
    "temp_life = np.load('Data_processed/cycle_life.npy',allow_pickle=True).item()\n",
    "all_capacity = np.load('Data_processed/all_capacity.npy',allow_pickle=True).item()\n",
    "temp_Qd_all = []\n",
    "temp_life_all = []\n",
    "all_capacity_all = []\n",
    "\n",
    "for key in temp_Qd.keys():\n",
    "    temp_life_all.append(temp_life[key])\n",
    "    all_capacity_all.append(all_capacity[key])\n",
    "    temp_Qd_list = []\n",
    "    for item in temp_Qd[key]:\n",
    "        temp_Qd_list.append(item)\n",
    "    temp_Qd_arr = np.asarray(temp_Qd_list)\n",
    "    temp_Qd_all.append(temp_Qd_arr)\n",
    "all_Qd_arr = np.asarray(temp_Qd_all)\n",
    "cycle_life_arr = np.asarray(temp_life_all)    \n",
    "\n",
    "\n",
    "'''Divide the dataset as the original paper stated'''\n",
    "test_ind = np.hstack((np.arange(0,(41+43),2),83))\n",
    "train_ind = np.arange(1,(41+43-1),2)\n",
    "secondary_test_ind = np.arange(124-40,124)\n",
    "\n",
    "all_keys = list(temp_Qd.keys())\n",
    "train_keys = [all_keys[inx] for inx in train_ind]\n",
    "test_keys = [all_keys[inx] for inx in test_ind]\n",
    "secondary_test_keys = [all_keys[inx] for inx in secondary_test_ind]\n",
    "\n",
    "cycle_life_arr=np.asarray(cycle_life_arr).reshape(-1,1)\n",
    "max_label=np.max(cycle_life_arr)\n",
    "cycle_life_arr=cycle_life_arr/max_label\n",
    "\n",
    "\n",
    "train_Qds = np.asarray(all_Qd_arr)[train_ind]\n",
    "train_cycle_lifes = np.asarray(cycle_life_arr)[train_ind]\n",
    "\n",
    "test_Qd_a = np.asarray(all_Qd_arr)[test_ind]\n",
    "test_cycle_life_a = np.asarray(cycle_life_arr)[test_ind]\n",
    "\n",
    "test_Qd_b = np.asarray(all_Qd_arr)[secondary_test_ind]\n",
    "test_cycle_life_b = np.asarray(cycle_life_arr)[secondary_test_ind]\n",
    "\n",
    "train_Qd, _, train_cycle_life, _ = train_test_split(train_Qds, train_cycle_lifes, test_size=0.36, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "# 修改模型需要改层数，回传部分，以及lstm部分\n",
    "'''Define the CNN model''' \n",
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40)),\n",
    "            torch.nn.BatchNorm2d(8),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(8, 16, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(16, 32, kernel_size=(2, 3), stride=1),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(32, 64, kernel_size=(2, 2), stride=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2),\n",
    "            \n",
    "            torch.nn.Conv2d(64, 64, kernel_size=(1, 2), stride=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.LeakyReLU(inplace=True),\n",
    "            torch.nn.Dropout(p=0.2)\n",
    "        )\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(64, 32, num_layers=2,bidirectional=True, batch_first=True)\n",
    "        # self.fc1 = torch.nn.Linear(7616, 100)\n",
    "        # self.drop_layer6 = torch.nn.Dropout(p=0.2)\n",
    "        # self.fc2 = torch.nn.Linear(100, 1)   \n",
    "        self.fc = torch.nn.Linear(7616, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            x = self.layers(x) \n",
    "            # x = x.view(x.size(0), -1)\n",
    "            x = to_3d(x)\n",
    "            x, _ = self.lstm(x)\n",
    "            # x = torch.relu(self.fc1(x))\n",
    "            # x = self.drop_layer6(x)                     \n",
    "            x = torch.sigmoid(self.fc(x.reshape(x.size(0), -1)))\n",
    "      \n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during forward pass: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(50, 50), stride=(5, 40))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "    (5): Conv2d(8, 16, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Dropout(p=0.2, inplace=False)\n",
      "    (9): Conv2d(16, 32, kernel_size=(2, 3), stride=(1, 1))\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (12): Dropout(p=0.2, inplace=False)\n",
      "    (13): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (16): Dropout(p=0.2, inplace=False)\n",
      "    (17): Conv2d(64, 64, kernel_size=(1, 2), stride=(1, 1))\n",
      "    (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (20): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (lstm): LSTM(64, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=7616, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()     # define the network\n",
    "print(net)      # net architecture\n",
    "# summary(net,(1,100,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step = 0 train_loss: 0.070691824 val_loss: 0.044454128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_31996\\118431579.py:96: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  history = pd.concat([history, new_entry], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step = 1 train_loss: 0.046006076 val_loss: 0.030603923\n",
      "Step = 2 train_loss: 0.033373766 val_loss: 0.023165636\n",
      "Step = 3 train_loss: 0.029005758 val_loss: 0.020901404\n",
      "Step = 4 train_loss: 0.032131094 val_loss: 0.021211052\n",
      "Step = 5 train_loss: 0.035249494 val_loss: 0.021735586\n",
      "Step = 6 train_loss: 0.03768881 val_loss: 0.021738261\n",
      "Step = 7 train_loss: 0.035304394 val_loss: 0.021341935\n",
      "Step = 8 train_loss: 0.034094613 val_loss: 0.02092751\n",
      "Step = 9 train_loss: 0.031727962 val_loss: 0.02081569\n",
      "Step = 10 train_loss: 0.030163355 val_loss: 0.021218896\n",
      "Step = 11 train_loss: 0.028640162 val_loss: 0.022227895\n",
      "Step = 12 train_loss: 0.028624406 val_loss: 0.023706933\n",
      "Step = 13 train_loss: 0.028633088 val_loss: 0.025321934\n",
      "Step = 14 train_loss: 0.029222984 val_loss: 0.026555622\n",
      "Step = 15 train_loss: 0.030506648 val_loss: 0.027091103\n",
      "Step = 16 train_loss: 0.031036887 val_loss: 0.026816465\n",
      "Step = 17 train_loss: 0.030321112 val_loss: 0.025793873\n",
      "Step = 18 train_loss: 0.02963471 val_loss: 0.024324829\n",
      "Step = 19 train_loss: 0.029212508 val_loss: 0.022830164\n",
      "Step = 20 train_loss: 0.027347924 val_loss: 0.021573845\n",
      "Step = 21 train_loss: 0.02733034 val_loss: 0.020436408\n",
      "Step = 22 train_loss: 0.02661136 val_loss: 0.0206212\n",
      "Step = 23 train_loss: 0.028062655 val_loss: 0.021823635\n",
      "Step = 24 train_loss: 0.027850261 val_loss: 0.023203922\n",
      "Step = 25 train_loss: 0.02685125 val_loss: 0.024066072\n",
      "Step = 26 train_loss: 0.02881679 val_loss: 0.024535578\n",
      "Step = 27 train_loss: 0.02831958 val_loss: 0.024899896\n",
      "Step = 28 train_loss: 0.026732774 val_loss: 0.025239907\n",
      "Step = 29 train_loss: 0.024325933 val_loss: 0.025315529\n",
      "Step = 30 train_loss: 0.028473554 val_loss: 0.024563935\n",
      "Step = 31 train_loss: 0.029030887 val_loss: 0.023757283\n",
      "Step = 32 train_loss: 0.028908668 val_loss: 0.023014883\n",
      "Step = 33 train_loss: 0.028424393 val_loss: 0.022385316\n",
      "Step = 34 train_loss: 0.028254954 val_loss: 0.021964295\n",
      "Step = 35 train_loss: 0.028210249 val_loss: 0.021654757\n",
      "Step = 36 train_loss: 0.02918738 val_loss: 0.021413542\n",
      "Step = 37 train_loss: 0.029601919 val_loss: 0.02112284\n",
      "Step = 38 train_loss: 0.028868394 val_loss: 0.021163056\n",
      "Step = 39 train_loss: 0.02732438 val_loss: 0.02142114\n",
      "Step = 40 train_loss: 0.027942441 val_loss: 0.021957427\n",
      "Step = 41 train_loss: 0.028055964 val_loss: 0.022727914\n",
      "Step = 42 train_loss: 0.025979538 val_loss: 0.023894114\n",
      "Step = 43 train_loss: 0.024237147 val_loss: 0.025723249\n",
      "Step = 44 train_loss: 0.02830663 val_loss: 0.02803519\n",
      "Step = 45 train_loss: 0.027968021 val_loss: 0.030076515\n",
      "Step = 46 train_loss: 0.028153466 val_loss: 0.031802665\n",
      "Step = 47 train_loss: 0.029770186 val_loss: 0.03299517\n",
      "Step = 48 train_loss: 0.028312733 val_loss: 0.033851556\n",
      "Step = 49 train_loss: 0.027729383 val_loss: 0.03456054\n",
      "Step = 50 train_loss: 0.027027376 val_loss: 0.034967527\n",
      "Step = 51 train_loss: 0.026049707 val_loss: 0.03532062\n",
      "Step = 52 train_loss: 0.02541634 val_loss: 0.035614766\n",
      "Step = 53 train_loss: 0.026551835 val_loss: 0.035513394\n",
      "Step = 54 train_loss: 0.025039371 val_loss: 0.035751957\n",
      "Step = 55 train_loss: 0.024616597 val_loss: 0.037144907\n",
      "Step = 56 train_loss: 0.025006859 val_loss: 0.03910426\n",
      "Step = 57 train_loss: 0.023645727 val_loss: 0.04118153\n",
      "Step = 58 train_loss: 0.022924243 val_loss: 0.043564558\n",
      "Step = 59 train_loss: 0.021230774 val_loss: 0.046409532\n",
      "Step = 60 train_loss: 0.02057025 val_loss: 0.049253464\n",
      "Step = 61 train_loss: 0.013870744 val_loss: 0.05251835\n",
      "Step = 62 train_loss: 0.008385625 val_loss: 0.05516689\n",
      "Step = 63 train_loss: 0.033435557 val_loss: 0.053495508\n",
      "Step = 64 train_loss: 0.03379225 val_loss: 0.051198464\n",
      "Step = 65 train_loss: 0.03296673 val_loss: 0.04838972\n",
      "Step = 66 train_loss: 0.033677164 val_loss: 0.0454881\n",
      "Step = 67 train_loss: 0.03144712 val_loss: 0.042506978\n",
      "Step = 68 train_loss: 0.029892363 val_loss: 0.03964913\n",
      "Step = 69 train_loss: 0.031304866 val_loss: 0.037009027\n",
      "Step = 70 train_loss: 0.028265893 val_loss: 0.03462249\n",
      "Step = 71 train_loss: 0.027732253 val_loss: 0.032579664\n",
      "Step = 72 train_loss: 0.028429352 val_loss: 0.030851908\n",
      "Step = 73 train_loss: 0.029569622 val_loss: 0.029392492\n",
      "Step = 74 train_loss: 0.029442528 val_loss: 0.028286308\n",
      "Step = 75 train_loss: 0.026519336 val_loss: 0.027481142\n",
      "Step = 76 train_loss: 0.027595213 val_loss: 0.02685774\n",
      "Step = 77 train_loss: 0.026660316 val_loss: 0.026409814\n",
      "Step = 78 train_loss: 0.028276425 val_loss: 0.026006192\n",
      "Step = 79 train_loss: 0.029377349 val_loss: 0.02575237\n",
      "Step = 80 train_loss: 0.028957834 val_loss: 0.025590327\n",
      "Step = 81 train_loss: 0.029293796 val_loss: 0.02552056\n",
      "Step = 82 train_loss: 0.029263837 val_loss: 0.025411794\n",
      "Step = 83 train_loss: 0.029003028 val_loss: 0.025367888\n",
      "Step = 84 train_loss: 0.027425123 val_loss: 0.025345502\n",
      "Step = 85 train_loss: 0.030072251 val_loss: 0.025315626\n",
      "Step = 86 train_loss: 0.030311652 val_loss: 0.025332628\n",
      "Step = 87 train_loss: 0.026729386 val_loss: 0.025308251\n",
      "Step = 88 train_loss: 0.028411498 val_loss: 0.02529251\n",
      "Step = 89 train_loss: 0.031487297 val_loss: 0.02527687\n",
      "Step = 90 train_loss: 0.028115656 val_loss: 0.025301805\n",
      "Step = 91 train_loss: 0.028702347 val_loss: 0.025269888\n",
      "Step = 92 train_loss: 0.028060053 val_loss: 0.025274675\n",
      "Step = 93 train_loss: 0.02583176 val_loss: 0.025285896\n",
      "Step = 94 train_loss: 0.029670952 val_loss: 0.025244085\n",
      "Step = 95 train_loss: 0.028415404 val_loss: 0.025195688\n",
      "Step = 96 train_loss: 0.029275883 val_loss: 0.025188105\n",
      "Step = 97 train_loss: 0.02849439 val_loss: 0.025193717\n",
      "Step = 98 train_loss: 0.027644772 val_loss: 0.02514046\n",
      "Step = 99 train_loss: 0.027742287 val_loss: 0.025073221\n",
      "Step = 100 train_loss: 0.029348643 val_loss: 0.024995502\n",
      "Step = 101 train_loss: 0.027799634 val_loss: 0.0248865\n",
      "Step = 102 train_loss: 0.02889055 val_loss: 0.024809185\n",
      "Step = 103 train_loss: 0.029188124 val_loss: 0.024751736\n",
      "Step = 104 train_loss: 0.029650778 val_loss: 0.024738448\n",
      "Step = 105 train_loss: 0.028912378 val_loss: 0.02466296\n",
      "Step = 106 train_loss: 0.02664915 val_loss: 0.024629427\n",
      "Step = 107 train_loss: 0.028438596 val_loss: 0.024611648\n",
      "Step = 108 train_loss: 0.030184956 val_loss: 0.024564696\n",
      "Step = 109 train_loss: 0.028973164 val_loss: 0.024612162\n",
      "Step = 110 train_loss: 0.02657694 val_loss: 0.024650166\n",
      "Step = 111 train_loss: 0.029271271 val_loss: 0.024682272\n",
      "Step = 112 train_loss: 0.029216431 val_loss: 0.024686174\n",
      "Step = 113 train_loss: 0.02830777 val_loss: 0.024764305\n",
      "Step = 114 train_loss: 0.02921171 val_loss: 0.024870453\n",
      "Step = 115 train_loss: 0.028850334 val_loss: 0.024988806\n",
      "Step = 116 train_loss: 0.030969711 val_loss: 0.025061011\n",
      "Step = 117 train_loss: 0.028022679 val_loss: 0.025149578\n",
      "Step = 118 train_loss: 0.026828803 val_loss: 0.025268689\n",
      "Step = 119 train_loss: 0.026860962 val_loss: 0.025419831\n",
      "Step = 120 train_loss: 0.026207682 val_loss: 0.025530031\n",
      "Step = 121 train_loss: 0.027736535 val_loss: 0.02567452\n",
      "Step = 122 train_loss: 0.028733274 val_loss: 0.025781361\n",
      "Step = 123 train_loss: 0.028855909 val_loss: 0.025780698\n",
      "Step = 124 train_loss: 0.027338525 val_loss: 0.025716476\n",
      "Step = 125 train_loss: 0.02798595 val_loss: 0.025629776\n",
      "Step = 126 train_loss: 0.029131118 val_loss: 0.025448412\n",
      "Step = 127 train_loss: 0.029147567 val_loss: 0.025175318\n",
      "Step = 128 train_loss: 0.027922781 val_loss: 0.024721157\n",
      "Step = 129 train_loss: 0.027632993 val_loss: 0.023933366\n",
      "Step = 130 train_loss: 0.02705992 val_loss: 0.022509333\n",
      "Step = 131 train_loss: 0.028302886 val_loss: 0.017613474\n",
      "Step = 132 train_loss: 0.028256793 val_loss: 0.025962237\n",
      "Step = 133 train_loss: 0.026201036 val_loss: 0.16765586\n",
      "Step = 134 train_loss: 0.02760367 val_loss: 0.27186614\n",
      "Step = 135 train_loss: 0.024867993 val_loss: 0.30505738\n",
      "Step = 136 train_loss: 0.018550113 val_loss: 0.32435697\n",
      "Step = 137 train_loss: 0.013498343 val_loss: 0.3329504\n",
      "Step = 138 train_loss: 0.015442365 val_loss: 0.33714023\n",
      "Step = 139 train_loss: 0.018669542 val_loss: 0.3389265\n",
      "Step = 140 train_loss: 0.020129431 val_loss: 0.33897886\n",
      "Step = 141 train_loss: 0.016254948 val_loss: 0.3364314\n",
      "Step = 142 train_loss: 0.011048345 val_loss: 0.3265096\n",
      "Step = 143 train_loss: 0.01447478 val_loss: 0.24438986\n",
      "Step = 144 train_loss: 0.0066682217 val_loss: 0.061026964\n",
      "Step = 145 train_loss: 0.015954562 val_loss: 0.30893785\n",
      "Step = 146 train_loss: 0.009057866 val_loss: 0.37222147\n",
      "Step = 147 train_loss: 0.015392653 val_loss: 0.3882806\n",
      "Step = 148 train_loss: 0.017943231 val_loss: 0.39837056\n",
      "Step = 149 train_loss: 0.011390707 val_loss: 0.409166\n",
      "Step = 150 train_loss: 0.013398599 val_loss: 0.4204939\n",
      "Step = 151 train_loss: 0.01551108 val_loss: 0.43131262\n",
      "Step = 152 train_loss: 0.017936155 val_loss: 0.44066462\n",
      "Step = 153 train_loss: 0.018363552 val_loss: 0.44790143\n",
      "Step = 154 train_loss: 0.014885441 val_loss: 0.4538871\n",
      "Step = 155 train_loss: 0.01814999 val_loss: 0.4582593\n",
      "Step = 156 train_loss: 0.0133019285 val_loss: 0.4616152\n",
      "Step = 157 train_loss: 0.010430879 val_loss: 0.46448308\n",
      "Step = 158 train_loss: 0.012349026 val_loss: 0.46499318\n",
      "Step = 159 train_loss: 0.013760456 val_loss: 0.4649206\n",
      "Step = 160 train_loss: 0.012977364 val_loss: 0.46650413\n",
      "Step = 161 train_loss: 0.012852858 val_loss: 0.46922943\n",
      "Step = 162 train_loss: 0.016890168 val_loss: 0.47219262\n",
      "Step = 163 train_loss: 0.012337427 val_loss: 0.47575268\n",
      "Step = 164 train_loss: 0.01783482 val_loss: 0.47963157\n",
      "Step = 165 train_loss: 0.02135661 val_loss: 0.48234293\n",
      "Step = 166 train_loss: 0.013564777 val_loss: 0.48495117\n",
      "Step = 167 train_loss: 0.020964699 val_loss: 0.4861012\n",
      "Step = 168 train_loss: 0.026113454 val_loss: 0.48576662\n",
      "Step = 169 train_loss: 0.011428491 val_loss: 0.48516688\n",
      "Step = 170 train_loss: 0.013249369 val_loss: 0.4847005\n",
      "Step = 171 train_loss: 0.012941837 val_loss: 0.48462743\n",
      "Step = 172 train_loss: 0.017658316 val_loss: 0.48441628\n",
      "Step = 173 train_loss: 0.015653787 val_loss: 0.48362005\n",
      "Step = 174 train_loss: 0.009942928 val_loss: 0.4839496\n",
      "Step = 175 train_loss: 0.013234432 val_loss: 0.48424196\n",
      "Step = 176 train_loss: 0.013699247 val_loss: 0.4832528\n",
      "Step = 177 train_loss: 0.013202086 val_loss: 0.48137257\n",
      "Step = 178 train_loss: 0.013597454 val_loss: 0.47809908\n",
      "Step = 179 train_loss: 0.011707848 val_loss: 0.47008198\n",
      "Step = 180 train_loss: 0.018163268 val_loss: 0.4613654\n",
      "Step = 181 train_loss: 0.014702452 val_loss: 0.45717713\n",
      "Step = 182 train_loss: 0.019285837 val_loss: 0.4556574\n",
      "Step = 183 train_loss: 0.01607203 val_loss: 0.45636892\n",
      "Step = 184 train_loss: 0.023149926 val_loss: 0.4562624\n",
      "Step = 185 train_loss: 0.019425405 val_loss: 0.45287034\n",
      "Step = 186 train_loss: 0.014406134 val_loss: 0.4426919\n",
      "Step = 187 train_loss: 0.021102296 val_loss: 0.42409468\n",
      "Step = 188 train_loss: 0.009195009 val_loss: 0.39185613\n",
      "Step = 189 train_loss: 0.012996707 val_loss: 0.28568187\n",
      "Step = 190 train_loss: 0.0059634144 val_loss: 0.14454523\n",
      "Step = 191 train_loss: 0.00828606 val_loss: 0.08667893\n",
      "Step = 192 train_loss: 0.007919726 val_loss: 0.09267342\n",
      "Step = 193 train_loss: 0.020371497 val_loss: 0.17647208\n",
      "Step = 194 train_loss: 0.020423174 val_loss: 0.22581676\n",
      "Step = 195 train_loss: 0.015989685 val_loss: 0.22613633\n",
      "Step = 196 train_loss: 0.018039024 val_loss: 0.20923692\n",
      "Step = 197 train_loss: 0.016301312 val_loss: 0.1827116\n",
      "Step = 198 train_loss: 0.017479898 val_loss: 0.15062168\n",
      "Step = 199 train_loss: 0.019540753 val_loss: 0.114318185\n",
      "Step = 200 train_loss: 0.019774478 val_loss: 0.07987878\n",
      "Step = 201 train_loss: 0.013439622 val_loss: 0.05198632\n",
      "Step = 202 train_loss: 0.01152441 val_loss: 0.03264057\n",
      "Step = 203 train_loss: 0.021072071 val_loss: 0.020901717\n",
      "Step = 204 train_loss: 0.013809898 val_loss: 0.016656138\n",
      "Step = 205 train_loss: 0.0162534 val_loss: 0.01743381\n",
      "Step = 206 train_loss: 0.01304324 val_loss: 0.019960037\n",
      "Step = 207 train_loss: 0.012502081 val_loss: 0.022505563\n",
      "Step = 208 train_loss: 0.012726628 val_loss: 0.023822961\n",
      "Step = 209 train_loss: 0.014535286 val_loss: 0.01739258\n",
      "Step = 210 train_loss: 0.013312231 val_loss: 0.029477\n",
      "Step = 211 train_loss: 0.010992234 val_loss: 0.07392742\n",
      "Step = 212 train_loss: 0.009705562 val_loss: 0.33030307\n",
      "Step = 213 train_loss: 0.011392257 val_loss: 0.35097498\n",
      "Step = 214 train_loss: 0.015506202 val_loss: 0.33479345\n",
      "Step = 215 train_loss: 0.010568542 val_loss: 0.3244842\n",
      "Step = 216 train_loss: 0.009926071 val_loss: 0.31744695\n",
      "Step = 217 train_loss: 0.0079189185 val_loss: 0.2980679\n",
      "Step = 218 train_loss: 0.0105578 val_loss: 0.27445924\n",
      "Step = 219 train_loss: 0.01342923 val_loss: 0.24448064\n",
      "Step = 220 train_loss: 0.006713813 val_loss: 0.2160533\n",
      "Step = 221 train_loss: 0.008875711 val_loss: 0.18724683\n",
      "Step = 222 train_loss: 0.0098355245 val_loss: 0.15531349\n",
      "Step = 223 train_loss: 0.010697454 val_loss: 0.118616305\n",
      "Step = 224 train_loss: 0.01382622 val_loss: 0.079501905\n",
      "Step = 225 train_loss: 0.012339234 val_loss: 0.04555455\n",
      "Step = 226 train_loss: 0.013788561 val_loss: 0.022156134\n",
      "Step = 227 train_loss: 0.008662036 val_loss: 0.014563648\n",
      "Step = 228 train_loss: 0.006071098 val_loss: 0.014782924\n",
      "Step = 229 train_loss: 0.005195963 val_loss: 0.02883081\n",
      "Step = 230 train_loss: 0.00477628 val_loss: 0.038423777\n",
      "Step = 231 train_loss: 0.0048765047 val_loss: 0.043296646\n",
      "Step = 232 train_loss: 0.004651447 val_loss: 0.04547023\n",
      "Step = 233 train_loss: 0.0067753014 val_loss: 0.046117224\n",
      "Step = 234 train_loss: 0.0043999767 val_loss: 0.046266504\n",
      "Step = 235 train_loss: 0.009963875 val_loss: 0.046366986\n",
      "Step = 236 train_loss: 0.004400609 val_loss: 0.04749866\n",
      "Step = 237 train_loss: 0.0034683729 val_loss: 0.047863666\n",
      "Step = 238 train_loss: 0.00530142 val_loss: 0.04729081\n",
      "Step = 239 train_loss: 0.0037393833 val_loss: 0.0455694\n",
      "Step = 240 train_loss: 0.004270178 val_loss: 0.043169692\n",
      "Step = 241 train_loss: 0.005360363 val_loss: 0.0410826\n",
      "Step = 242 train_loss: 0.004238452 val_loss: 0.0391568\n",
      "Step = 243 train_loss: 0.008363803 val_loss: 0.03678462\n",
      "Step = 244 train_loss: 0.0038126723 val_loss: 0.03632182\n",
      "Step = 245 train_loss: 0.004430863 val_loss: 0.031564135\n",
      "Step = 246 train_loss: 0.0057064253 val_loss: 0.030774994\n",
      "Step = 247 train_loss: 0.0066335765 val_loss: 0.03562004\n",
      "Step = 248 train_loss: 0.0053872326 val_loss: 0.03596781\n",
      "Step = 249 train_loss: 0.004602263 val_loss: 0.036817934\n",
      "Step = 250 train_loss: 0.003647865 val_loss: 0.03821969\n",
      "Step = 251 train_loss: 0.0039700624 val_loss: 0.03950097\n",
      "Step = 252 train_loss: 0.006318343 val_loss: 0.040825587\n",
      "Step = 253 train_loss: 0.0032237696 val_loss: 0.029366616\n",
      "Step = 254 train_loss: 0.0026647472 val_loss: 0.09225189\n",
      "Step = 255 train_loss: 0.0057569346 val_loss: 0.16749777\n",
      "Step = 256 train_loss: 0.0054377215 val_loss: 0.06681912\n",
      "Step = 257 train_loss: 0.0055893087 val_loss: 0.0067993402\n",
      "Step = 258 train_loss: 0.004593032 val_loss: 0.04743503\n",
      "Step = 259 train_loss: 0.0045665824 val_loss: 0.04874248\n",
      "Step = 260 train_loss: 0.0037970007 val_loss: 0.04876665\n",
      "Step = 261 train_loss: 0.0038724556 val_loss: 0.04893359\n",
      "Step = 262 train_loss: 0.0046194643 val_loss: 0.048816875\n",
      "Step = 263 train_loss: 0.004866659 val_loss: 0.048476834\n",
      "Step = 264 train_loss: 0.0025072382 val_loss: 0.048510477\n",
      "Step = 265 train_loss: 0.0038260343 val_loss: 0.048173115\n",
      "Step = 266 train_loss: 0.0033126688 val_loss: 0.04759187\n",
      "Step = 267 train_loss: 0.0046877973 val_loss: 0.044165622\n",
      "Step = 268 train_loss: 0.0026883243 val_loss: 0.03775881\n",
      "Step = 269 train_loss: 0.0035774969 val_loss: 0.043124307\n",
      "Step = 270 train_loss: 0.003403727 val_loss: 0.04543231\n",
      "Step = 271 train_loss: 0.002732483 val_loss: 0.045575082\n",
      "Step = 272 train_loss: 0.004652712 val_loss: 0.044768814\n",
      "Step = 273 train_loss: 0.0017546939 val_loss: 0.04403318\n",
      "Step = 274 train_loss: 0.0031183134 val_loss: 0.043595325\n",
      "Step = 275 train_loss: 0.0030157252 val_loss: 0.04316793\n",
      "Step = 276 train_loss: 0.004345241 val_loss: 0.04287106\n",
      "Step = 277 train_loss: 0.0018610033 val_loss: 0.04305158\n",
      "Step = 278 train_loss: 0.0021197468 val_loss: 0.042906374\n",
      "Step = 279 train_loss: 0.0025776534 val_loss: 0.039807737\n",
      "Step = 280 train_loss: 0.003670354 val_loss: 0.022425868\n",
      "Step = 281 train_loss: 0.0042480924 val_loss: 0.015395976\n",
      "Step = 282 train_loss: 0.0039166952 val_loss: 0.013960271\n",
      "Step = 283 train_loss: 0.0031002709 val_loss: 0.014938699\n",
      "Step = 284 train_loss: 0.0031585882 val_loss: 0.017151173\n",
      "Step = 285 train_loss: 0.0025475402 val_loss: 0.02060026\n",
      "Step = 286 train_loss: 0.003946567 val_loss: 0.029125502\n",
      "Step = 287 train_loss: 0.0021721309 val_loss: 0.03527795\n",
      "Step = 288 train_loss: 0.0044974675 val_loss: 0.037968896\n",
      "Step = 289 train_loss: 0.002380052 val_loss: 0.03917655\n",
      "Step = 290 train_loss: 0.0022766455 val_loss: 0.039463606\n",
      "Step = 291 train_loss: 0.0039394046 val_loss: 0.039307315\n",
      "Step = 292 train_loss: 0.0027839052 val_loss: 0.039083064\n",
      "Step = 293 train_loss: 0.0019011634 val_loss: 0.038999632\n",
      "Step = 294 train_loss: 0.0036530495 val_loss: 0.03861103\n",
      "Step = 295 train_loss: 0.003121372 val_loss: 0.037959605\n",
      "Step = 296 train_loss: 0.004137088 val_loss: 0.037170574\n",
      "Step = 297 train_loss: 0.0027663382 val_loss: 0.03638534\n",
      "Step = 298 train_loss: 0.0036493652 val_loss: 0.03544943\n",
      "Step = 299 train_loss: 0.0031772233 val_loss: 0.034343924\n",
      "Step = 300 train_loss: 0.0033581068 val_loss: 0.033213392\n",
      "Step = 301 train_loss: 0.002198949 val_loss: 0.032131016\n",
      "Step = 302 train_loss: 0.0012763906 val_loss: 0.031190572\n",
      "Step = 303 train_loss: 0.0051173675 val_loss: 0.030396717\n",
      "Step = 304 train_loss: 0.0031492433 val_loss: 0.029924428\n",
      "Step = 305 train_loss: 0.0023226738 val_loss: 0.029993357\n",
      "Step = 306 train_loss: 0.0038671112 val_loss: 0.03115319\n",
      "Step = 307 train_loss: 0.0022498043 val_loss: 0.032239806\n",
      "Step = 308 train_loss: 0.0030527955 val_loss: 0.033570614\n",
      "Step = 309 train_loss: 0.002816238 val_loss: 0.034206145\n",
      "Step = 310 train_loss: 0.0020103538 val_loss: 0.03451817\n",
      "Step = 311 train_loss: 0.00243587 val_loss: 0.036124077\n",
      "Step = 312 train_loss: 0.0038792992 val_loss: 0.036729988\n",
      "Step = 313 train_loss: 0.002178101 val_loss: 0.037581224\n",
      "Step = 314 train_loss: 0.0045246286 val_loss: 0.03729139\n",
      "Step = 315 train_loss: 0.0022429447 val_loss: 0.039050758\n",
      "Step = 316 train_loss: 0.0025845491 val_loss: 0.039858554\n",
      "Step = 317 train_loss: 0.0037312147 val_loss: 0.038793184\n",
      "Step = 318 train_loss: 0.00315938 val_loss: 0.03541978\n",
      "Step = 319 train_loss: 0.002203576 val_loss: 0.031755976\n",
      "Step = 320 train_loss: 0.0033834213 val_loss: 0.025715083\n",
      "Step = 321 train_loss: 0.0034288566 val_loss: 0.016633498\n",
      "Step = 322 train_loss: 0.0038237034 val_loss: 0.012108365\n",
      "Step = 323 train_loss: 0.0036287834 val_loss: 0.01788374\n",
      "Step = 324 train_loss: 0.002613235 val_loss: 0.01742474\n",
      "Step = 325 train_loss: 0.00284194 val_loss: 0.018025585\n",
      "Step = 326 train_loss: 0.002949853 val_loss: 0.011570349\n",
      "Step = 327 train_loss: 0.0024724288 val_loss: 0.010277701\n",
      "Step = 328 train_loss: 0.003115652 val_loss: 0.011775547\n",
      "Step = 329 train_loss: 0.0021940225 val_loss: 0.009176119\n",
      "Step = 330 train_loss: 0.0060953097 val_loss: 0.027800776\n",
      "Step = 331 train_loss: 0.002714412 val_loss: 0.036697667\n",
      "Step = 332 train_loss: 0.0023024457 val_loss: 0.03905959\n",
      "Step = 333 train_loss: 0.005021486 val_loss: 0.039729178\n",
      "Step = 334 train_loss: 0.0029804907 val_loss: 0.03923753\n",
      "Step = 335 train_loss: 0.0033070808 val_loss: 0.039355617\n",
      "Step = 336 train_loss: 0.0022822374 val_loss: 0.039434023\n",
      "Step = 337 train_loss: 0.0020295256 val_loss: 0.03561951\n",
      "Step = 338 train_loss: 0.002987489 val_loss: 0.023502918\n",
      "Step = 339 train_loss: 0.0029119796 val_loss: 0.01371363\n",
      "Step = 340 train_loss: 0.002640952 val_loss: 0.008144776\n",
      "Step = 341 train_loss: 0.0023483175 val_loss: 0.0063741584\n",
      "Step = 342 train_loss: 0.0029018295 val_loss: 0.024493929\n",
      "Step = 343 train_loss: 0.0028935126 val_loss: 0.03880421\n",
      "Step = 344 train_loss: 0.0017567148 val_loss: 0.0421862\n",
      "Step = 345 train_loss: 0.0024055413 val_loss: 0.042511217\n",
      "Step = 346 train_loss: 0.0030874682 val_loss: 0.04975141\n",
      "Step = 347 train_loss: 0.0030633693 val_loss: 0.33076003\n",
      "Step = 348 train_loss: 0.003956946 val_loss: 0.29197636\n",
      "Step = 349 train_loss: 0.0050108153 val_loss: 0.025725685\n",
      "Step = 350 train_loss: 0.002392482 val_loss: 0.045354784\n",
      "Step = 351 train_loss: 0.0037642515 val_loss: 0.045716263\n",
      "Step = 352 train_loss: 0.0044767847 val_loss: 0.1002082\n",
      "Step = 353 train_loss: 0.00363458 val_loss: 0.27197117\n",
      "Step = 354 train_loss: 0.003967661 val_loss: 0.23393568\n",
      "Step = 355 train_loss: 0.003467647 val_loss: 0.0053648725\n",
      "Step = 356 train_loss: 0.004000139 val_loss: 0.046593115\n",
      "Step = 357 train_loss: 0.0024489611 val_loss: 0.046625704\n",
      "Step = 358 train_loss: 0.004306778 val_loss: 0.04635896\n",
      "Step = 359 train_loss: 0.0023483548 val_loss: 0.12387784\n",
      "Step = 360 train_loss: 0.0028315948 val_loss: 0.38597605\n",
      "Step = 361 train_loss: 0.0039153015 val_loss: 0.1695718\n",
      "Step = 362 train_loss: 0.005978559 val_loss: 0.04745046\n",
      "Step = 363 train_loss: 0.002529086 val_loss: 0.047805373\n",
      "Step = 364 train_loss: 0.002910094 val_loss: 0.047825463\n",
      "Step = 365 train_loss: 0.0019648476 val_loss: 0.011520542\n",
      "Step = 366 train_loss: 0.003007955 val_loss: 0.3719642\n",
      "Step = 367 train_loss: 0.004952214 val_loss: 0.30331185\n",
      "Step = 368 train_loss: 0.0030827387 val_loss: 0.009445788\n",
      "Step = 369 train_loss: 0.0048225545 val_loss: 0.05010806\n",
      "Step = 370 train_loss: 0.0024414966 val_loss: 0.050446928\n",
      "Step = 371 train_loss: 0.0042534852 val_loss: 0.0496635\n",
      "Step = 372 train_loss: 0.017255446 val_loss: 0.05035452\n",
      "Step = 373 train_loss: 0.019357515 val_loss: 0.05182493\n",
      "Step = 374 train_loss: 0.016253678 val_loss: 0.055062324\n",
      "Step = 375 train_loss: 0.0230243 val_loss: 0.05851492\n",
      "Step = 376 train_loss: 0.017365372 val_loss: 0.062204607\n",
      "Step = 377 train_loss: 0.017845022 val_loss: 0.0659343\n",
      "Step = 378 train_loss: 0.021968653 val_loss: 0.06885611\n",
      "Step = 379 train_loss: 0.016990695 val_loss: 0.072281025\n",
      "Step = 380 train_loss: 0.016890999 val_loss: 0.076217435\n",
      "Step = 381 train_loss: 0.015590738 val_loss: 0.08079059\n",
      "Step = 382 train_loss: 0.01365817 val_loss: 0.0856938\n",
      "Step = 383 train_loss: 0.013622167 val_loss: 0.090351015\n",
      "Step = 384 train_loss: 0.014858966 val_loss: 0.094342925\n",
      "Step = 385 train_loss: 0.015115793 val_loss: 0.09717741\n",
      "Step = 386 train_loss: 0.016780209 val_loss: 0.0987871\n",
      "Step = 387 train_loss: 0.020499408 val_loss: 0.09950453\n",
      "Step = 388 train_loss: 0.027126454 val_loss: 0.0992266\n",
      "Step = 389 train_loss: 0.012748757 val_loss: 0.09854538\n",
      "Step = 390 train_loss: 0.010261456 val_loss: 0.09751583\n",
      "Step = 391 train_loss: 0.008523673 val_loss: 0.096292734\n",
      "Step = 392 train_loss: 0.0152574675 val_loss: 0.094491266\n",
      "Step = 393 train_loss: 0.009988638 val_loss: 0.0921063\n",
      "Step = 394 train_loss: 0.013794984 val_loss: 0.08933568\n",
      "Step = 395 train_loss: 0.009195655 val_loss: 0.086754546\n",
      "Step = 396 train_loss: 0.011515057 val_loss: 0.08358994\n",
      "Step = 397 train_loss: 0.012106736 val_loss: 0.0808663\n",
      "Step = 398 train_loss: 0.014586217 val_loss: 0.07765165\n",
      "Step = 399 train_loss: 0.012753518 val_loss: 0.074862145\n",
      "Step = 400 train_loss: 0.0042388565 val_loss: 0.071532525\n",
      "Step = 401 train_loss: 0.008312755 val_loss: 0.06379941\n",
      "Step = 402 train_loss: 0.00785972 val_loss: 0.060035184\n",
      "Step = 403 train_loss: 0.0053041484 val_loss: 0.056481685\n",
      "Step = 404 train_loss: 0.014695477 val_loss: 0.049153958\n",
      "Step = 405 train_loss: 0.00937458 val_loss: 0.051082052\n",
      "Step = 406 train_loss: 0.005379615 val_loss: 0.050750017\n",
      "Step = 407 train_loss: 0.006411708 val_loss: 0.050693706\n",
      "Step = 408 train_loss: 0.0140026165 val_loss: 0.04869227\n",
      "Step = 409 train_loss: 0.0148554705 val_loss: 0.048982628\n",
      "Step = 410 train_loss: 0.01605738 val_loss: 0.050671026\n",
      "Step = 411 train_loss: 0.009989787 val_loss: 0.052227996\n",
      "Step = 412 train_loss: 0.0101779485 val_loss: 0.40120408\n",
      "Step = 413 train_loss: 0.017617932 val_loss: 0.46317765\n",
      "Step = 414 train_loss: 0.01215846 val_loss: 0.47309595\n",
      "Step = 415 train_loss: 0.010421782 val_loss: 0.48073086\n",
      "Step = 416 train_loss: 0.00971392 val_loss: 0.48570848\n",
      "Step = 417 train_loss: 0.0137540465 val_loss: 0.48795518\n",
      "Step = 418 train_loss: 0.01206282 val_loss: 0.48888806\n",
      "Step = 419 train_loss: 0.013008842 val_loss: 0.48923725\n",
      "Step = 420 train_loss: 0.014816097 val_loss: 0.48962557\n",
      "Step = 421 train_loss: 0.014048801 val_loss: 0.48992428\n",
      "Step = 422 train_loss: 0.015722891 val_loss: 0.48905006\n",
      "Step = 423 train_loss: 0.007852767 val_loss: 0.4859483\n",
      "Step = 424 train_loss: 0.011277253 val_loss: 0.45154995\n",
      "Step = 425 train_loss: 0.029050741 val_loss: 0.4836075\n",
      "Step = 426 train_loss: 0.01042764 val_loss: 0.48505467\n",
      "Step = 427 train_loss: 0.013538871 val_loss: 0.4830882\n",
      "Step = 428 train_loss: 0.0099406885 val_loss: 0.48202273\n",
      "Step = 429 train_loss: 0.010394125 val_loss: 0.48173633\n",
      "Step = 430 train_loss: 0.007371672 val_loss: 0.48170444\n",
      "Step = 431 train_loss: 0.008191502 val_loss: 0.48232633\n",
      "Step = 432 train_loss: 0.007391163 val_loss: 0.48316303\n",
      "Step = 433 train_loss: 0.0061753783 val_loss: 0.48468623\n",
      "Step = 434 train_loss: 0.0051110066 val_loss: 0.48613265\n",
      "Step = 435 train_loss: 0.004817804 val_loss: 0.48701394\n",
      "Step = 436 train_loss: 0.0046098176 val_loss: 0.48557922\n",
      "Step = 437 train_loss: 0.004556142 val_loss: 0.4807909\n",
      "Step = 438 train_loss: 0.0032605552 val_loss: 0.47545224\n",
      "Step = 439 train_loss: 0.004390596 val_loss: 0.47883126\n",
      "Step = 440 train_loss: 0.007258198 val_loss: 0.4783476\n",
      "Step = 441 train_loss: 0.004944931 val_loss: 0.47898534\n",
      "Step = 442 train_loss: 0.0060366127 val_loss: 0.4799171\n",
      "Step = 443 train_loss: 0.004842525 val_loss: 0.4809678\n",
      "Step = 444 train_loss: 0.0062946533 val_loss: 0.48094374\n",
      "Step = 445 train_loss: 0.007097771 val_loss: 0.47793478\n",
      "Step = 446 train_loss: 0.005060884 val_loss: 0.457474\n",
      "Step = 447 train_loss: 0.0030415037 val_loss: 0.30666572\n",
      "Step = 448 train_loss: 0.0045559835 val_loss: 0.032836836\n",
      "Step = 449 train_loss: 0.002709416 val_loss: 0.05260194\n",
      "Step = 450 train_loss: 0.00296344 val_loss: 0.052696604\n",
      "Step = 451 train_loss: 0.00365145 val_loss: 0.054032866\n",
      "Step = 452 train_loss: 0.0031172438 val_loss: 0.009752405\n",
      "Step = 453 train_loss: 0.004147754 val_loss: 0.3866664\n",
      "Step = 454 train_loss: 0.0051553496 val_loss: 0.44335365\n",
      "Step = 455 train_loss: 0.004907179 val_loss: 0.4430839\n",
      "Step = 456 train_loss: 0.010615677 val_loss: 0.34281385\n",
      "Step = 457 train_loss: 0.004076289 val_loss: 0.042510655\n",
      "Step = 458 train_loss: 0.0020143148 val_loss: 0.02510561\n",
      "Step = 459 train_loss: 0.0027413666 val_loss: 0.050148092\n",
      "Step = 460 train_loss: 0.0027804514 val_loss: 0.04957013\n",
      "Step = 461 train_loss: 0.0023269 val_loss: 0.04812447\n",
      "Step = 462 train_loss: 0.0031197323 val_loss: 0.046609033\n",
      "Step = 463 train_loss: 0.003150813 val_loss: 0.045606002\n",
      "Step = 464 train_loss: 0.0028942626 val_loss: 0.044678558\n",
      "Step = 465 train_loss: 0.0029892172 val_loss: 0.04374242\n",
      "Step = 466 train_loss: 0.0021180406 val_loss: 0.0429476\n",
      "Step = 467 train_loss: 0.0027011358 val_loss: 0.04220039\n",
      "Step = 468 train_loss: 0.0019885118 val_loss: 0.041665766\n",
      "Step = 469 train_loss: 0.0022197517 val_loss: 0.041235372\n",
      "Step = 470 train_loss: 0.0021002025 val_loss: 0.04095612\n",
      "Step = 471 train_loss: 0.0032565454 val_loss: 0.040598165\n",
      "Step = 472 train_loss: 0.0018902725 val_loss: 0.04030895\n",
      "Step = 473 train_loss: 0.0023402325 val_loss: 0.040372517\n",
      "Step = 474 train_loss: 0.0019178069 val_loss: 0.04036835\n",
      "Step = 475 train_loss: 0.0023843548 val_loss: 0.040219404\n",
      "Step = 476 train_loss: 0.0019064724 val_loss: 0.04038511\n",
      "Step = 477 train_loss: 0.0016012379 val_loss: 0.04108021\n",
      "Step = 478 train_loss: 0.0016802355 val_loss: 0.041560642\n",
      "Step = 479 train_loss: 0.002763326 val_loss: 0.041368384\n",
      "Step = 480 train_loss: 0.001971802 val_loss: 0.03995908\n",
      "Step = 481 train_loss: 0.002841469 val_loss: 0.036479846\n",
      "Step = 482 train_loss: 0.001660886 val_loss: 0.035391174\n",
      "Step = 483 train_loss: 0.002092428 val_loss: 0.038014073\n",
      "Step = 484 train_loss: 0.0016333254 val_loss: 0.039117698\n",
      "Step = 485 train_loss: 0.0016457071 val_loss: 0.04001068\n",
      "Step = 486 train_loss: 0.0027866126 val_loss: 0.04077783\n",
      "Step = 487 train_loss: 0.003180679 val_loss: 0.040874567\n",
      "Step = 488 train_loss: 0.0020979634 val_loss: 0.040898994\n",
      "Step = 489 train_loss: 0.0035581212 val_loss: 0.04092402\n",
      "Step = 490 train_loss: 0.0023206982 val_loss: 0.04098383\n",
      "Step = 491 train_loss: 0.0025922565 val_loss: 0.04100693\n",
      "Step = 492 train_loss: 0.0020496333 val_loss: 0.04100282\n",
      "Step = 493 train_loss: 0.0023531846 val_loss: 0.041021436\n",
      "Step = 494 train_loss: 0.0023215564 val_loss: 0.040956225\n",
      "Step = 495 train_loss: 0.0014520137 val_loss: 0.04086481\n",
      "Step = 496 train_loss: 0.0014814205 val_loss: 0.040642697\n",
      "Step = 497 train_loss: 0.002554811 val_loss: 0.040343933\n",
      "Step = 498 train_loss: 0.0015069804 val_loss: 0.03988199\n",
      "Step = 499 train_loss: 0.0017402805 val_loss: 0.039221518\n",
      "Step = 500 train_loss: 0.0024690116 val_loss: 0.03840807\n",
      "Step = 501 train_loss: 0.0011168937 val_loss: 0.037484482\n",
      "Step = 502 train_loss: 0.002317743 val_loss: 0.036637235\n",
      "Step = 503 train_loss: 0.0025233454 val_loss: 0.03577914\n",
      "Step = 504 train_loss: 0.0025664829 val_loss: 0.03522998\n",
      "Step = 505 train_loss: 0.0034541865 val_loss: 0.035898652\n",
      "Step = 506 train_loss: 0.0022145438 val_loss: 0.03692258\n",
      "Step = 507 train_loss: 0.0021860932 val_loss: 0.037380084\n",
      "Step = 508 train_loss: 0.0028234327 val_loss: 0.03718832\n",
      "Step = 509 train_loss: 0.0026198316 val_loss: 0.035806302\n",
      "Step = 510 train_loss: 0.002291463 val_loss: 0.031579368\n",
      "Step = 511 train_loss: 0.00205278 val_loss: 0.023772862\n",
      "Step = 512 train_loss: 0.0016436448 val_loss: 0.019182604\n",
      "Step = 513 train_loss: 0.0022211205 val_loss: 0.013827962\n",
      "Step = 514 train_loss: 0.0028202385 val_loss: 0.012970681\n",
      "Step = 515 train_loss: 0.0020950534 val_loss: 0.00994074\n",
      "Step = 516 train_loss: 0.0014525234 val_loss: 0.00920989\n",
      "Step = 517 train_loss: 0.002448695 val_loss: 0.0097768465\n",
      "Step = 518 train_loss: 0.0013522429 val_loss: 0.010787368\n",
      "Step = 519 train_loss: 0.0018849309 val_loss: 0.009870868\n",
      "Step = 520 train_loss: 0.003404132 val_loss: 0.011076531\n",
      "Step = 521 train_loss: 0.0026060725 val_loss: 0.011762058\n",
      "Step = 522 train_loss: 0.002716335 val_loss: 0.011536372\n",
      "Step = 523 train_loss: 0.0034290028 val_loss: 0.011718234\n",
      "Step = 524 train_loss: 0.0034232223 val_loss: 0.011730954\n",
      "Step = 525 train_loss: 0.0021327774 val_loss: 0.024976932\n",
      "Step = 526 train_loss: 0.002716704 val_loss: 0.031173322\n",
      "Step = 527 train_loss: 0.0018157195 val_loss: 0.02360439\n",
      "Step = 528 train_loss: 0.0020003947 val_loss: 0.017248495\n",
      "Step = 529 train_loss: 0.0035727636 val_loss: 0.010061745\n",
      "Step = 530 train_loss: 0.0024235824 val_loss: 0.018095214\n",
      "Step = 531 train_loss: 0.0020688104 val_loss: 0.03970031\n",
      "Step = 532 train_loss: 0.0022138841 val_loss: 0.036954172\n",
      "Step = 533 train_loss: 0.0031711592 val_loss: 0.035359457\n",
      "Step = 534 train_loss: 0.0026452406 val_loss: 0.036107957\n",
      "Step = 535 train_loss: 0.0018868322 val_loss: 0.14390837\n",
      "Step = 536 train_loss: 0.0026107563 val_loss: 0.46006492\n",
      "Step = 537 train_loss: 0.0027023344 val_loss: 0.47634804\n",
      "Step = 538 train_loss: 0.0026494733 val_loss: 0.48070434\n",
      "Step = 539 train_loss: 0.003169006 val_loss: 0.4819788\n",
      "Step = 540 train_loss: 0.0033283134 val_loss: 0.48278272\n",
      "Step = 541 train_loss: 0.0029191896 val_loss: 0.48324254\n",
      "Step = 542 train_loss: 0.0057730544 val_loss: 0.48203966\n",
      "Step = 543 train_loss: 0.004412514 val_loss: 0.46636915\n",
      "Step = 544 train_loss: 0.006760127 val_loss: 0.3835012\n",
      "Step = 545 train_loss: 0.0027749487 val_loss: 0.013880067\n",
      "Step = 546 train_loss: 0.00382048 val_loss: 0.05180396\n",
      "Step = 547 train_loss: 0.0029249047 val_loss: 0.049066894\n",
      "Step = 548 train_loss: 0.0024054353 val_loss: 0.048219435\n",
      "Step = 549 train_loss: 0.002849898 val_loss: 0.046673067\n",
      "Step = 550 train_loss: 0.0025058542 val_loss: 0.046221994\n",
      "Step = 551 train_loss: 0.004044772 val_loss: 0.047208376\n",
      "Step = 552 train_loss: 0.0023488156 val_loss: 0.007729688\n",
      "Step = 553 train_loss: 0.0013626579 val_loss: 0.4520098\n",
      "Step = 554 train_loss: 0.0020222792 val_loss: 0.47921854\n",
      "Step = 555 train_loss: 0.0038993421 val_loss: 0.48567215\n",
      "Step = 556 train_loss: 0.0037851646 val_loss: 0.48319176\n",
      "Step = 557 train_loss: 0.0027895535 val_loss: 0.4699708\n",
      "Step = 558 train_loss: 0.0028406007 val_loss: 0.44498584\n",
      "Step = 559 train_loss: 0.0021806352 val_loss: 0.22834815\n",
      "Step = 560 train_loss: 0.0028464892 val_loss: 0.013336045\n",
      "Step = 561 train_loss: 0.0019916461 val_loss: 0.04708663\n",
      "Step = 562 train_loss: 0.003519848 val_loss: 0.042370502\n",
      "Step = 563 train_loss: 0.0023659056 val_loss: 0.04012938\n",
      "Step = 564 train_loss: 0.0033907634 val_loss: 0.0347415\n",
      "Step = 565 train_loss: 0.0038895349 val_loss: 0.029195733\n",
      "Step = 566 train_loss: 0.0017585271 val_loss: 0.027033675\n",
      "Step = 567 train_loss: 0.0017372516 val_loss: 0.02701677\n",
      "Step = 568 train_loss: 0.0024671818 val_loss: 0.030644039\n",
      "Step = 569 train_loss: 0.0029326852 val_loss: 0.032656476\n",
      "Step = 570 train_loss: 0.0028038432 val_loss: 0.0353054\n",
      "Step = 571 train_loss: 0.0019790935 val_loss: 0.037668288\n",
      "Step = 572 train_loss: 0.0036305161 val_loss: 0.041207414\n",
      "Step = 573 train_loss: 0.002601852 val_loss: 0.45075202\n",
      "Step = 574 train_loss: 0.0024545107 val_loss: 0.48585933\n",
      "Step = 575 train_loss: 0.0032463716 val_loss: 0.48683828\n",
      "Step = 576 train_loss: 0.003237959 val_loss: 0.4870692\n",
      "Step = 577 train_loss: 0.004351656 val_loss: 0.4891244\n",
      "Step = 578 train_loss: 0.005934551 val_loss: 0.4924188\n",
      "Step = 579 train_loss: 0.0037131454 val_loss: 0.49405953\n",
      "Step = 580 train_loss: 0.0050912295 val_loss: 0.47505248\n",
      "Step = 581 train_loss: 0.006206182 val_loss: 0.1278703\n",
      "Step = 582 train_loss: 0.0019739235 val_loss: 0.008750843\n",
      "Step = 583 train_loss: 0.002957943 val_loss: 0.009975786\n",
      "Step = 584 train_loss: 0.003948109 val_loss: 0.009994248\n",
      "Step = 585 train_loss: 0.0048969183 val_loss: 0.012454642\n",
      "Step = 586 train_loss: 0.0042720046 val_loss: 0.02650003\n",
      "Step = 587 train_loss: 0.007095442 val_loss: 0.0121419355\n",
      "Step = 588 train_loss: 0.007003464 val_loss: 0.02422912\n",
      "Step = 589 train_loss: 0.006022819 val_loss: 0.043193\n",
      "Step = 590 train_loss: 0.005515615 val_loss: 0.046892297\n",
      "Step = 591 train_loss: 0.0060391803 val_loss: 0.048000857\n",
      "Step = 592 train_loss: 0.004899777 val_loss: 0.049239043\n",
      "Step = 593 train_loss: 0.004480898 val_loss: 0.04728195\n",
      "Step = 594 train_loss: 0.0031605007 val_loss: 0.01301513\n",
      "Step = 595 train_loss: 0.0034265008 val_loss: 0.43507102\n",
      "Step = 596 train_loss: 0.004032374 val_loss: 0.46154088\n",
      "Step = 597 train_loss: 0.0051747314 val_loss: 0.46128926\n",
      "Step = 598 train_loss: 0.002358664 val_loss: 0.4585801\n",
      "Step = 599 train_loss: 0.0031127841 val_loss: 0.1810606\n",
      "Best model at index: 355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fisher Man\\AppData\\Local\\Temp\\ipykernel_31996\\118431579.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('Best_target_model/net_parameters.pkl')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "'''Data loader for Pytorch'''\n",
    "input_train = torch.FloatTensor(train_Qd)\n",
    "input_train = torch.unsqueeze(input_train, 1)\n",
    "train_labels = torch.FloatTensor(train_cycle_life)\n",
    "\n",
    "input_val = torch.FloatTensor(train_Qds)\n",
    "input_val = torch.unsqueeze(input_val, 1)\n",
    "val_labels = torch.FloatTensor(train_cycle_lifes)\n",
    "\n",
    "input_test_a = torch.FloatTensor(test_Qd_a)\n",
    "input_test_a = torch.unsqueeze(input_test_a, 1)\n",
    "test_labels_a = torch.FloatTensor(test_cycle_life_a)\n",
    "\n",
    "input_test_b = torch.FloatTensor(test_Qd_b)\n",
    "input_test_b = torch.unsqueeze(input_test_b, 1)\n",
    "test_labels_b = torch.FloatTensor(test_cycle_life_b)\n",
    "\n",
    "\n",
    "\n",
    "seed = 17\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(seed)    # reproducible    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
    "loss_func = torch.nn.MSELoss() \n",
    "\n",
    "# # 动态调整学习率\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.8 ** (epoch // 40))\n",
    "\n",
    "# # 将学习率调度器修改为指数下降\n",
    "# initial_lr = 0.0001  # 初始学习率\n",
    "# decay_rate = 0.96  # 每个epoch衰减的比例\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: decay_rate ** epoch)\n",
    "\n",
    "# 假设你已经定义了模型和优化器\n",
    "# 当验证损失不下降时，减少学习率\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# 创建历史记录DataFrame\n",
    "history = pd.DataFrame(columns=['index', 'train_loss', 'val_loss'])\n",
    "val_losses = []\n",
    "max_models_to_keep = 10\n",
    "saved_models=[]\n",
    "import os\n",
    "\n",
    "for t in range(600):\n",
    "    net.train()\n",
    "    train_prediction = net(input_train)\n",
    "    train_loss = loss_func(train_prediction, train_labels)\n",
    "   \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    # 保存模型\n",
    "    model_path = 'Target_model/net_parameters'+str(t)+'.pkl'\n",
    "    torch.save({\n",
    "        'epoch': t,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()}, model_path)\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_prediction = net(input_val)\n",
    "        val_loss = loss_func(val_prediction, val_labels)\n",
    "    print('Step = %d' % t, 'train_loss:', train_loss.data.numpy(), 'val_loss:', val_loss.data.numpy())\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    # 添加当前模型及其验证损失\n",
    "    saved_models.append((val_loss.item(), model_path))\n",
    "\n",
    "    # 保持模型数量不超过max_models_to_keep\n",
    "    if len(saved_models) > max_models_to_keep:\n",
    "        # 找到验证损失最大的一组模型并删除\n",
    "        saved_models.sort(key=lambda x: x[0])  # 排序，根据损失\n",
    "        os.remove(saved_models.pop()[1])  # 删除损失最大的模型\n",
    "    # 加入训练历史记录\n",
    "    # 创建新的一行数据\n",
    "    new_entry = pd.DataFrame({'index': [t], 'train_loss': [train_loss.item()], 'val_loss': [val_loss.item()]})\n",
    "    \n",
    "    # 使用concat进行数据合并\n",
    "    history = pd.concat([history, new_entry], ignore_index=True)\n",
    "\n",
    "'''find the best model'''\n",
    "best_index = val_losses.index(np.min(val_losses))\n",
    "print(f'Best model at index: {best_index}')\n",
    "\n",
    "'''copy the best model from model file to the model_test file'''\n",
    "shutil.copyfile(f'Target_model/net_parameters{best_index}.pkl', 'Best_target_model/net_parameters.pkl')\n",
    "\n",
    "'''Reload the best model''' \n",
    "model = Net()\n",
    "checkpoint = torch.load('Best_target_model/net_parameters.pkl')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpe_train: 0.1665813\n",
      "mpe_val: 0.1746908\n",
      "mpe_a: 0.1842235470657678\n",
      "mpe_b: 0.20359400337320183\n",
      "rmse_train: 177.14629\n",
      "rmse_val: 163.70825\n",
      "rmse_a: 164.96116526780366\n",
      "rmse_b: 266.60696727580097\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "'''Prediction using the best model'''\n",
    "predict_labels_a=[]\n",
    "predict_labels_b=[]\n",
    "predict_labels_train=[]\n",
    "predict_labels_val = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict_labels_a.append(model(input_test_a).data.numpy())\n",
    "    predict_labels_b.append(model(input_test_b).data.numpy())\n",
    "    \n",
    "    predict_labels_train.append(model(input_train).data.numpy())   \n",
    "    predict_labels_val.append(model(input_val).data.numpy())      \n",
    "\n",
    "\n",
    "actual_label_arr_train=np.round(np.asarray(train_labels*max_label).reshape(-1,1))  \n",
    "predict_label_arr_train=np.round(np.asarray(predict_labels_train).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_val=np.round(np.asarray(val_labels*max_label).reshape(-1,1))   \n",
    "predict_label_arr_val=np.round(np.asarray(predict_labels_val).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_a=np.round(np.asarray(test_cycle_life_a*max_label).reshape(-1,1))   \n",
    "predict_label_arr_a=np.round(np.asarray(predict_labels_a).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "actual_label_arr_b=np.round(np.asarray(test_cycle_life_b * max_label).reshape(-1,1))   \n",
    "predict_label_arr_b=np.round(np.asarray(predict_labels_b).reshape(-1,1)*max_label)\n",
    "\n",
    "\n",
    "'''Plot the predicted cycle lives'''\n",
    "plt.scatter(actual_label_arr_train, predict_label_arr_train,s=50,c='k')\n",
    "plt.scatter(actual_label_arr_val, predict_label_arr_val,s=50,c='b')\n",
    "plt.scatter(actual_label_arr_a, predict_label_arr_a,s=50,c='r')\n",
    "plt.scatter(actual_label_arr_b, predict_label_arr_b,s=50,c='g')\n",
    "plt.plot([100,2500],[100,2500])\n",
    "\n",
    "\n",
    "'''Evaluation metrics'''\n",
    "mpe_a=np.mean(np.abs(predict_label_arr_a-actual_label_arr_a)/actual_label_arr_a)\n",
    "mpe_b=np.mean(np.abs(predict_label_arr_b-actual_label_arr_b)/actual_label_arr_b)\n",
    "mpe_train = np.mean(np.abs(predict_label_arr_train-actual_label_arr_train)/actual_label_arr_train)\n",
    "mpe_val = np.mean(np.abs(predict_label_arr_val-actual_label_arr_val)/actual_label_arr_val)\n",
    "\n",
    "\n",
    "rmse_a=np.sqrt(np.mean((predict_label_arr_a-actual_label_arr_a)**2))\n",
    "rmse_b=np.sqrt(np.mean((predict_label_arr_b-actual_label_arr_b)**2))\n",
    "rmse_train=np.sqrt(np.mean((predict_label_arr_train-actual_label_arr_train)**2))\n",
    "rmse_val=np.sqrt(np.mean((predict_label_arr_val-actual_label_arr_val)**2))\n",
    "\n",
    "\n",
    "print ('mpe_train:',mpe_train)\n",
    "print ('mpe_val:',mpe_val)\n",
    "print ('mpe_a:',mpe_a)\n",
    "print ('mpe_b:',mpe_b)\n",
    "\n",
    "print ('rmse_train:', rmse_train)\n",
    "print ('rmse_val:', rmse_val)\n",
    "print ('rmse_a:', rmse_a)\n",
    "print ('rmse_b:', rmse_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_train: 0.7836369872093201\n",
      "r2_val: 0.7425346374511719\n",
      "r2_a: 0.8216892674870501\n",
      "r2_b: 0.12559162121117318\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_a = r2_score(actual_label_arr_a, predict_label_arr_a)\n",
    "r2_b = r2_score(actual_label_arr_b, predict_label_arr_b)\n",
    "r2_train = r2_score(actual_label_arr_train, predict_label_arr_train)\n",
    "r2_val = r2_score(actual_label_arr_val, predict_label_arr_val)\n",
    "\n",
    "print('r2_train:', r2_train)\n",
    "print('r2_val:', r2_val)\n",
    "print('r2_a:', r2_a)\n",
    "print('r2_b:', r2_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
